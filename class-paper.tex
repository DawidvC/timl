%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath,amsfonts,amscd,amssymb,proof,MnSymbol}
\usepackage{mathpartir}
\usepackage{turnstile}% http://ctan.org/pkg/turnstile
\usepackage{adjustbox}% http://ctan.org/pkg/adjustbox
\usepackage{color}

\renewcommand{\makehor}[4]
  {\ifthenelse{\equal{#1}{n}}{\hspace{#3}}{}
   \ifthenelse{\equal{#1}{s}}{\rule[-0.5#2]{#3}{#2}}{}
   \ifthenelse{\equal{#1}{d}}{\setlength{\lengthvar}{#2}
     \addtolength{\lengthvar}{0.5#4}
     \rule[-\lengthvar]{#3}{#2}
     \hspace{-#3}
     \rule[0.5#4]{#3}{#2}}{}
   \ifthenelse{\equal{#1}{t}}{\setlength{\lengthvar}{1.5#2}
     \addtolength{\lengthvar}{#4}
     \rule[-\lengthvar]{#3}{#2}
     \hspace{-#3}
     \rule[-0.5#2]{#3}{#2}
     \hspace{-#3}
     \setlength{\lengthvar}{0.5#2}
     \addtolength{\lengthvar}{#4}
     \rule[\lengthvar]{#3}{#2}}{}
   \ifthenelse{\equal{#1}{w}}{% New wavy $\sim$ definition
     \setbox0=\hbox{$\sim$}%
     \raisebox{-.6ex}{\hspace*{-.05ex}\adjustbox{width=#3,height=\height}{\clipbox{0.75 0 0 0}{\usebox0}}}}{}
  }

\newcommand{\thide}[1]{\left \{ #1 \right \}}
\newcommand{\typing}[4]{\turnstile{s}{s}{#4}{#3}{n}#1:#2}
\newcommand{\kinding}[2]{\turnstile{s}{s}{}{}{n}#1:#2}
\newcommand{\teq}[2]{#1\equiv#2}
\newcommand{\arrow}[4]{#1\xrightarrow[#3]{#2}#4}
\newcommand{\symlet}{\mathsf{let\;}}
\newcommand{\symin}{\mathsf{\;in\;}}
\newcommand{\symletrec}{\mathsf{letrec\;}}
\newcommand{\symand}{\mathsf{\;and\;}}
\newcommand{\symmatch}{\mathsf{match}}
\newcommand{\FV}{\mathsf{FV}}
\newcommand{\symwith}{\mathsf{\;with\;}}
\newcommand{\syminl}{\mathsf{inl}}
\newcommand{\syminr}{\mathsf{inr}}
\newcommand{\symmax}{\mathsf{max}}
\newcommand{\symSinl}{\mathsf{Sinl\;}}
\newcommand{\symSinr}{\mathsf{Sinr\;}}
\newcommand{\symfold}{\mathsf{fold\;}}
\newcommand{\symSfold}{\mathsf{Sfold}}
\newcommand{\symunfold}{\mathsf{unfold\;}}
\newcommand{\symSunfold}{\mathsf{Sunfold\;}}
\newcommand{\symhide}{\mathsf{hide\;}}
\newcommand{\symShide}{\mathsf{Shide}}
\newcommand{\symunhide}{\mathsf{unhide\;}}
\newcommand{\symSunhide}{\mathsf{Sunhide\;}}
\newcommand{\leO}{\preceq}
\newcommand{\sympair}{\mathsf{pair}}
\newcommand{\symtt}{\mathsf{tt}}
\newcommand{\symunit}{\mathsf{unit}}
\newcommand{\symlist}{\mathsf{list}}
\newcommand{\symnil}{\mathsf{nil}}
\newcommand{\symcons}{\mathsf{cons}}
\newcommand{\symfix}{\mathsf{fix}}
\newcommand{\symbool}{\mathsf{bool}}
\newcommand{\symtrue}{\mathsf{true}}
\newcommand{\symfalse}{\mathsf{false}}
\newcommand{\symmerge}{\mathsf{merge}}

%% \newcommand{\intro}[2]{#2^#1}
\newcommand{\intro}[2]{(#1 : #2)}
%% \newcommand{\intro}[2]{(#2 \mathsf{\;size\;} #1)}
%% \newcommand{\intro}[2]{\{#2 \mathsf{\;|\;} #1\}}

\newcommand{\symsum}{\mathsf{sum}}
\newcommand{\symfst}{\mathsf{fst}}
\newcommand{\symsnd}{\mathsf{snd}}
\newcommand{\symif}{\mathsf{if\;}}
\newcommand{\symthen}{\mathsf{\;then\;}}
\newcommand{\symelse}{\mathsf{\;else\;}}
\newcommand{\symSbool}{\mathsf{Sbool}}
\newcommand{\symuf}{\mathsf{uf}}
\newcommand{\symuh}{\mathsf{uh}}
\newcommand{\syml}{\mathsf{l}}
\newcommand{\symr}{\mathsf{r}}
\newcommand{\symf}{\mathsf{f}}
\newcommand{\syms}{\mathsf{s}}
\newcommand{\symmsort}{\mathsf{msort}}
\newcommand{\symSstat}{\mathsf{Sstat}}
\newcommand{\symsplit}{\mathsf{split}}
\newcommand{\symprod}{\mathsf{prod}}
\newcommand{\symStt}{\mathsf{Stt}}
\newcommand{\symSpair}{\mathsf{Spair}}
\newcommand{\symSlr}{\mathsf{Slr}}
\newcommand{\defeq}{\triangleq}
\newcommand{\symwork}{\mathsf{w}}
\newcommand{\symspan}{\mathsf{s}}

\newcommand{\logo}{\lambda^{\forall,\omega,\mu}_\mathrm{c}}
\newcommand{\Sstats}[1]{\left \langle #1 \right \rangle}
\newtheorem{thm}{Theorem}
\newcommand{\optional}[1]{\lfloor #1 \rfloor}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

%% \titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{$\logo$ : Complexity Recursive Types}
%% \subtitle{Subtitle Text, if any}

\authorinfo{Peng Wang}
           {MIT CSAIL}
           {wangpeng@csail.mit.edu}

\maketitle

\begin{abstract}
We present the first time-complexity type system with recursive types. Our type system lets functional programmers specify time-complexity bounds of functions, and the type system checks and guarantees such bounds. This language is a milestone towards the ``great synthesis'' of complexity analysis and programming languages, for it supports universal polymorphism, type-level operators and isomorphic recursive types, the combined power of which enables generic algebraic data types, the watershed of practical usefulness. Potential beneficiaries of our language includes everyday functional programmers and computer theoriests, using it both as a practical bug-preventing tool and as a formal logic to prove algorithm complexities.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore, 
% you may leave them out
%% \terms
%% term1, term2

\keywords
type system; complexity; recursive types

\section{Introduction}

One major obstacle of the adaptation of functional programming is the lack of a clear intuition of functional program's time complexity. A C programmer can easily read off the big-O complexity of a for-loop, but even experienced functional programmers are sometimes muddled about the time complexity of a piece of functional code. The confusion comes from the lack of knowledge of the compiler's inner-working, as stated by the scoff ``you can hide an elephant under a beta reduction''. Two examples of the mythical objects that compilers manage internally are closures and algebraic data values. When one does not know how closures are created, passed, and used, he or she will rightfully wonder the time cost of creating a lambda abstraction, passing a function, and applying a function (beta reduction). For values of algebraic data types such as list, the immutability of these values (one can only construct new values, not in-place modify them) often gives a C programmer the wrong impression that such values are copied and passed-by-value all the time and that passing a list as argument will take O(n) time because of the copying. The reality could not be further from this false impression, since immutability means almost everything can be safely implemented by pointers and references.

But requiring every programmer to fully understand the inner-working of functional-language compilers is infeasible and unreasonable. One should be able to reason about programs in the language in which she actually writes, not in the target language of the compiler. It is the compiler's responsibility to preserve all properties of interest from the source program to the target program. So to demythify the complexity of function programming, two pieces of work need to be done: (1) we need a way to let the programmers easily reason about the complexity of functional programs in the source language level, and (2) we need a compiler that preserves the complexity of the source program downto the target program. This paper deals with the first part, with some assumptions about the second part. 

One usually reasons about functional programs using its operational semantics. The closest thing related to time cost in operational semantics is the number of reduction steps (in some counting metric) in small-step operational semantics, with some evaluation strategy. The design choices are (1) which evaluation strategy to use and (2) in which metric to do the counting. For (1), we choose strict call-by-value evaluation strategy against call-by-name strategy, because we think call-by-value fits people's intuition of evaluation order better, and it makes our assumption about the compiler more plausible. Call-by-name evaluation strategy has the drawback that since it can postpone computations and executing them in a sudden burst, a reduction step in the evaluation strategy could take an arbitrary amount of time on a real machine.

For design choice (2), there are work~\cite{blelloch2013cache} that employ ingenious cost models in which the counting is done, and we do believe that we need well-designed cost models to account for various machine characteristics when we want to have a more precise analysis of the time cost. But for this work, we choose the simplest cost model - the number of reduction steps in a standard strict call-by-value evaluation strategy. The justification for this choice is that (a) our top-level theorem will give any well-typed program a big-O bound on the number of reduction steps, which allows for a constant factor to be the room of imprecision, and (b) we make the assumption that {\bf any reduction step in a standard strict call-by-value evaluation strategy can be implemented by the compiler as a constant-time computation (excluding garbage collection)}. When we later make clear our definition of ``constant'' and ``non-constant'', we will come back to this assumption and try to persuade the reader that this is a reasonable assumption. Combining (a) and (b), we will have a end-to-end theorem of the big-O bound on the actual running time (defined in a well-accepted machine model such as the Random Access Machine (RAM)), independent of our choice of functional cost model, which can be treated as a intermediate proof step.

Using such a cost model, programmers can do the counting in their heads and reason about their programs' complexity informally. However, we want to allow programmers to formally express their complexity constraints in a type system, which can formally guarantee the complexity via type-checking. Type systems are for preventing bugs, and we think that calling a function of a wrong complexity order, such as calling an O(n) function when an O(1) one is intended, is really a bug, not a ``performance issue''. Complexity, or at least complexity order, should be a constituent of functional correctness.

We designed such a type system for the calculus $\lambda^{\forall,\omega\,\mu}$ (that is, lambda calculus extended with impredicative (first-class) universal polymorphism, type-level operators, and (isomorphic) recursive types). Our main contribution is {\bf the first complexity type system with recursive types}. We believe this is an important milestone because the combiningg power of $\lambda^\forall$, $\lambda^\omega$, and $\lambda^\mu$ gives birth to generic algebraic data types such as lists and trees, which is the minimal requirement for a functional language to become practically ``useful''.

We imagine two groups of potential users for such a language and type system. The first group is everyday programmers, for whom this type system can be incorporated into the ML toolchain and provide additional benefits besides the already very useful ML type system. To be user-friendly, we must have a type checker (which means the type system needs to be proved decidable) and reasonable degree of type inference. We will discuss this in Section \ref{section-discussion} as future work.

The second group of users are computer theorists conducting complexity proofs of algorithms. As~\cite{harper2014proposal} points out, there is an unfornate crevice between ``combinatorial'' theoriests and programming language theoriests. Members of the former still believe that the only mathematically rigid way to do complexity analysis is on the assembly level - on the Turning Machine model or Random Access Machine model; while the latter community has decades of achievements raising the abstraction level and building large systems \emph{compositionally} out of smaller components. The combinatorial theorists actually often give up rigidity by using some ``pseudo-code'' in the papers and assume that the reader can straight-forwardly ``compile'' the pseudo-code into rigid RAM code in his or her head. We hope that our language can serve as the start point when computer theoriests can express their complexity proof in a rigid whilst high-level formal language, enjoying the benefits of modern functional languages such as compositionality and abstraction. Our language is only a start point because it still lacks such important features as mutable references (and mutable arrays), which is essential for implementing matrix-based numeric algorithms; and such advanced complexity analysis as probablistic and amortized analysis, which have become mainstream for modern complexity analysis. We also lack parallelism, though in our design we intentionlly maintain the generality (we have the notion of ``work'' and ``span'') to make later extension to parallelism easier.

Section \ref{section-example} will give an illustrating example, the merge-sort, to show what the type system can express. It will also give the top-level type soundness theorem. Section \ref{section-lang} defines the language and the typing rules. Section \ref{section-proof} gives the proof of the type soundness, using Logical Step-indexed Logical Relation~\cite{dreyer2009logical}. Section \ref{section-related} discusses related work. Section \ref{section-discussion} discusses this work's strength and limitations, future work and conclusion.

\section{\label{section-example}Merge-Sort Example and Main Theorem}

The merge-sort code and the types of relevant functions are shown in Figure \ref{msort}. Function $\symmsort$ is generic for the element type of the input list and generic for the comparison function. The type of $\symmsort$ requires that the comparison function $cmp$ should compare a pair of elements within 1 unit time (the meaning of ``1 unit'' will be explained later), and guarantees that $\symmsort$ will finish within $|s|*\log(|s|)$ units of time. As seen from the example, complexities are annotated on the arrows in the type, where above the arrow is the time complexity bound of the function, while below the arrow is the size bound of the this function's result value. In order to refer to the input size, each type on the left side of an arrow will introduce a \emph{size variable}, whose scope consists of the complexity expressions above and below the arrow, and the type on the right side of the arrow. Together with currying, the complexity expressions can refer to multiple arguments' sizes for n-arity functions, as shown in the type of the $\symmerge$ function. This notation may give a superficially dependent-typed flavor, but the type system is not dependent-typed because types can only depend on size variables, not values, and size variables will only be used on the arrows.

\begin{figure}
\begin{align*}
\symmsort &\defeq \lambda A. \lambda cmp.\;\symfix\;f(xs). \\
& \hspace{.1in} \symmatch\;xs\symwith \\
& \hspace{.2in} |\; \symnil\Rightarrow xs \\
& \hspace{.2in} |\; \_::xs' \Rightarrow \symmatch\;xs'\symwith \\
& \hspace{.3in} |\; \symnil\Rightarrow xs \\
& \hspace{.3in} |\; \_ \Rightarrow \symmatch\; \symsplit\;xs \symwith \\
& \hspace{.4in} |\; (ys, zs) \Rightarrow \symmerge\;cmp\;(f\;ys)\;(f\;zs) \\
& \hspace{-0.2in} : \forall A.\;\arrow{(\arrow{A}{}{}{\arrow{A}{1}{}{\symbool}})}{}{}{\arrow{\intro{s}{\symlist\;A}}{|s|*\log(|s|)}{\Sstats{s}}{\symlist\;A}} \\
\mathsf{where} & \\
\symsplit &: \forall A.\;\arrow{\intro{s}{\symlist\;A}}{|s|/2}{\Sstats{s}/2}{\symlist\;A\times\symlist\;A} \\
\symmerge &: \forall A.\;\arrow{(\arrow{A}{}{}{\arrow{A}{1}{}{\symbool}})}{}{}{\arrow{\intro{x}{\symlist\;A}}{}{}{\arrow{\intro{y}{\symlist\;A}}{|x|+|y|}{\Sstats{x}+\Sstats{y}}{\symlist\;A}}}
\end{align*}
\caption{\label{msort}Merge-sort}
\end{figure}

The ``size'' of an input value is not merely a natural number, as one might expect, but a tree-like structure, since we aim to deal with general algebraic data types such as trees. In order to use such a size in a time complexity expression, which is merely a number, we defined some measures on a size structure. Here the $|s|$ measure corresponds to the length of a list if $s$ is the size of a list value. The definition of $|-|$ will be given in Section \ref{section-lang}. In the size expression below the arrow, $\Sstats{s}$ stands for a size that has the same measures as size $s$, but not necessarily the same structure. In this example it can be understood as ``the size of a list whose length is the same as $s$'', which is a proper description of merge-sort's result.  The notations $\Sstats{s}/2$ and $\Sstats{x}+\Sstats{y}$ means ``a list of half length'' and ``a list of combined length'' respectively, whose precise definitions are given in Section \ref{section-lang}.

A unit of time in the complexity expression is defined as one unfold-fold reduction, and the complexity expression states the number of unfold-fold reductions. All other reductions are ignored by complexity expressions. The reason for this seemingly bizarre choice is that (1) the actual number of all reductions depends on some details of the input value, which are not accounted for by its size structure in our definition; and our type soundness theorem (see below) will state that (2) the actual number of all reduction steps will be bounded by the number of unfold-fold reductions, with a constant factor. Claim (2) is true crucially because we do not have built-in fixpoint or recursive-let in our language, so the only source of unbounded execution is via some recursive-type encoding of some combinators, which all have the property that each recursive call will involve at least one unfold-fold reduction (for the encoding we use to define fixpoint, the number is exactly one). The unfold-fold count thus covers the number of recursive calls, which is essentially the only thing that changes with input size. For functional programmers who are more familiar with algebraic data types than recursive types, they can think of the count as ``the number of recursive calls plus the number of pattern-matchings on recursive data structures''. Counting this is actually easier than counting all reduction steps, since not all programmers are familiar with the whole set of evaluation rules. 

Now we give the top-level type soundness theorem of our type system. It has two parts. Theorem \ref{thm-safety} states the standard type safety, in terms of nonstuckness. Theorem \ref{thm-boundedness}, distinguishing for our type system, states the time-boundedness of any well-typed program in our language.

\begin{thm}[\label{thm-safety}Soundness w.r.t. Safety]
$$
\begin{array}{l}
\forall e,\tau. \\
\hspace{.1in} \vdash e:\tau \Rightarrow \\
\hspace{.2in} \forall e'.\; e \leadsto^* e' \Rightarrow e'\in\mathsf{Val} \;\vee\; \exists e''.\;e' \leadsto e''.
\end{array}
$$
\end{thm}

\begin{thm}[\label{thm-boundedness}Soundness w.r.t. Boundedness]
$$
\begin{array}{l}
\forall f,\tau_1,\tau_2,c,s. \\
\hspace{0.1in} \vdash f:\arrow{(x:\tau_1)}{c}{s}{\tau_2} \wedge f\in\mathsf{Val}\Rightarrow \\
\hspace{0.2in} \exists C,\xi_0. \;\forall v.\; \vdash v:\tau_1 \wedge |v| \geq \xi_0 \Rightarrow \\
\hspace{0.3in} \forall e',n.\; f\;v\leadsto^n e' \Rightarrow n\leq C\times c[|v|/x]
\end{array}
$$
\end{thm}

Theorem \ref{thm-safety}, the standard nonstuckness property, says that for any well-typed program (in an empty typing context), wherever the program runs to ($e'$), it is either successfully done (be a value) or can take one more step. That is, it won't get stuck in some erronous state where there is no legal step to progress.

Theorem \ref{thm-boundedness} says that for any well-typed value $f$ of the arrow type $\arrow{(x:\tau_1)}{c}{s}{\tau_2}$, there exists some constant factor $C$ and a threshold $\xi_0$, so that for any input value of the right type and whose size is larger than or equal to the threshold $\xi_0$, the composed expression $f\;v$ won't run for more than $C\times c[|v|/x]$ steps, where $c$ is the complexity guarantee given by the type. Combining it with Theorem \ref{thm-safety}, we can conclude that $f\;v$ will always successfully terminate within $C\times c[|v|/x]$ steps. Here a step $\leadsto$ means any reduction step in a standard strict call-by-value evaluation strategy (defined in Section \ref{section-lang}), not just the unfold-fold count used for complexity expressions in types. Note that the existentially quantified $C$ and $\xi_0$ are outside hence independent of the input value, which meets the standard definition of asymptotic complexity. The reason we need a threshold for input size and do not guarentee time bound for inputs of all sizes is that sometimes the complexity expressions do not make sense with too small sizes, as will be seen in Section \ref{section-lang}. Small inputs do not matter since in asymptotic analysis we only care about the performance behavior as the inputs grow ever larger. We do guarentee safety on inputs of all sizes, since Theorem \ref{thm-safety} does not involve thresholds.

Theorem \ref{thm-boundedness} is only one corollary from the strenghened result in the main proof, for unary functions. We can have theorems such as Theorem \ref{thm-boundedness2} for binary and n-arity functions, and all existentially quantified constant factors and thresholds will be independent of any input.

\begin{thm}[\label{thm-boundedness2}Soundness w.r.t. Boundedness (2-arity)]
$$
\begin{array}{l}
\forall f,\tau_1,\tau_2,\tau_3,c,s. \\
\hspace{0.1in} \vdash f:\arrow{(x_1:\tau_1)}{0}{0}{\arrow{(x_2:\tau_2)}{c}{s}{\tau_3}} \wedge f\in\mathsf{Val}\Rightarrow \\
\hspace{0.2in} \exists C,\xi_0^1,\xi_0^2. \;\forall v_1,v_2.\; (\forall i.\;\vdash v_i:\tau_i \wedge |v_i| \geq \xi_0^i) \Rightarrow \\
\hspace{0.3in} \forall e',n.\; f\;v_1\;v_2\leadsto^n e' \Rightarrow n\leq C\times c[|v_1|/x_1][|v_2|/x_2]
\end{array}
$$
\end{thm}

\section{\label{section-lang}Language and Type System}

\subsection{Syntax}

The syntax of the language is given in Figure \ref{syntax}. Excluding the complexity and size part, and the ``hide'' type $\thide{\tau}$, all the syntax and evaluation context are standard. We put type annotations in lambda, $\syminl$, $\syminr$ and fold to make type unique (modulo complexity annotations), and elide them when irrelevant. All type variables are restricted to kind $*$, so we do not need kind annotations in type-level binders.

\begin{figure}
$$\begin{array}{rrcl}
  \textrm{Size Subpart Idx} & i &::=& \symf \textrm{(fst)} \mid \syms \textrm{(snd)} \mid \syml \textrm{(left)} \mid \symr \textrm{(right)} \mid \symuf \textrm{(unfold)} \\
  & & & \mid \symuh \textrm{(unhide)} \\
  \textrm{Complexity Expr} & c &::=& x.\vec{i}.\symwork \textrm{(work)} \mid x.\vec{i}.\symspan \textrm{(span)} \mid 0 \mid 1 \mid c+c \mid \cdots \\
  \textrm{Sizes} & s &::=& x.\vec{i} \mid \Sstats{c_\symwork,c_\symspan} \mid \symStt \mid \symSinl s \mid \symSinr s \mid \symSlr\;s\;s \\
  & & & \mid (s,s) \mid \symSfold\;s \mid \symShide\;s \\
  \textrm{Types} & \tau &::=& \symunit \mid \tau\times\tau \mid \tau+\tau \mid X \mid \arrow{\intro{x}{\tau}}{c}{s}{\tau} \\
  & & & \mid \forall^c_s X.\tau \mid \Lambda X.\tau \mid \tau\;\tau \mid \mu X.\tau \mid \thide\tau \\
  \textrm{Expressions} & e &::=& \symtt \mid (e,e) \mid \syminl_\tau\;e \mid \syminr_\tau\;e \mid x \mid e\;e \mid \lambda x:\tau.\;e \\
  & & & \mid \symlet x := e \symin e \mid e\;\tau \mid \lambda X.e \mid \symfold\tau\;e \\
  & & & \mid \symunfold\;e \mid \symhide\;e \mid \symunhide\;e \\
  & & & \mid \symmatch\;e\symwith (x,y)\Rightarrow e \\
  & & & \mid (\symmatch\;e\symwith\syminl\;x\Rightarrow e\;|\;\syminr\;x\Rightarrow e) \\
  \textrm{Values} & v &::=& x \mid \lambda x:\tau.e \mid \lambda X.e \mid \symfold\tau\;v \mid \symtt \mid (v,v) \\
  & & & \mid \syminl\;v \mid \syminr\;v \mid \symhide v \\
  \textrm{CBV Eval Ctx} & E &::=& \Box \mid E\;e \mid v\;E \mid \symlet x := E\symin e \mid E\;\tau \\
  & & & \mid \symfold\tau\;E \mid \symunfold\tau\;E \mid \symhide\;E \mid \symunhide\;E \\
  & & & \mid (E, e) \mid (v, E) \mid \syminl\;E \mid \syminr\;E \\
  & & & \mid \symmatch\;E\symwith(x,y)\Rightarrow e \\
  & & & \mid (\symmatch\;E\symwith\syminl\;x\Rightarrow e\;|\;\syminr\;x\Rightarrow e) \\
  \textrm{Kinds} & k &::=& * \mid *\to k \\
  \textrm{Typing Ctx} & \Gamma &::=& \cdot \mid \Gamma,X \mid \Gamma, x:(\tau, \optional{s})
\end{array}$$
\caption{\label{syntax}Syntax}
\end{figure}

Now let's turn to the complexity and size part. Firstly, complexity and size annotations only appear on arrows and in $\forall$ binders (the latter can be seen as a degenerated case of the former). Each arrow type $\arrow{\intro{x}{\tau_1}}{c}{s}{\tau_2}$ introduces a size variable standing for the input size, which can be referred to in $c$, $s$ and $\tau_2$ (but not $\tau_1$). The $\forall$ binder does not introduce new size variables. The reason for $\forall$ to also have complexity annotation is that, just like $\lambda x.e$, $\lambda X.e$ is a ``delayed'' computation, meaning that it is itself a value but when been applied, its inner part may take some steps to reach another value. But in most cases, the $c$ and $s$ in $\forall^c_s$ are just 0. When $c$ or $s$ is 0, we elide it in this paper.

We first explain the definition of sizes. As mention before, the size of a value is a tree-like structure, closely mirroring the structure of a value. Comparing ``Sizes'' and ``Values'' shows that they are almost isomorphic except that sizes do not have $\lambda$-abstractions but have constructors $x.\vec{i}$, $\Sstats{c_\symwork,c_\symspan}$ and $(\symSlr\;s\;s)$. $x.\vec{i}$ is used to refer to a subpart of a size variable. Since we are doing asymptotic analysis, size variables will always be instantiated with ``large enough'' instances, so we can go as deep as we like into a size variable. The query indices for subparts are the corresponding eliminators of size constructors. $\Sstats{c_\symwork,c_\symspan}$ is used to construct a size that has no structural information, only the measures. There are two measures, ``work'' and ``span'', borrowed from the parallel computation literature. ``Work'' means the number of ``$\symSfold$'' nodes in the size; ``span'' means the height of the size seen as a tree, only considering ``$\symSfold$'' nodes. ``Span'' seems useless in a non-parallel setting, but remember we can define trees, where ``span'' is meaningful even in a non-parallel setting.

The intuition behind the definition of size is that the $\symSfold$-skeleton is the real ``variable'' thing in a size, and when we say ``how computation grows with the input size'' we actually mean ``how computation grows with the input size's $\symSfold$-skeleton''.

We need the $\Sstats{c_\symwork,c_\symspan}$ constructor because $s$ is also our specification language for a function's result size (below the arrow), and usually measures are the only thing we can specify about the result size. We need $(\symSlr\;s\;s)$ to specify the size of sum-type values when we are not sure which branch it will take. For brevity we write ``$\Sstats{s}$'' for ``$\Sstats{s.\symwork,s.\symspan}$'', ``0'' for ``$\Sstats{0,0}$'', ``$\Sstats{c_1,c_2}/2$'' for ``$\Sstats{c_1/2,c_2/2}$'' and similarly lift other operations on measures to sizes. 

Now we explain the time complexity expression. Since variables can appear in a complexity expression, we define complexity expression as an abstract algebra instead of just natural numbers. Which algebra to use (hence the expressiveness of complexity expressions) is almost an orthogonal issue of the type system design. We will elaborate the axioms we used in the type soundness proof regarding this algebra, thus any algebra satisfying these axioms will give us a sound type system. Do note that the choice of this algebra greatly influence the complexity precision and type checking/inference difficulty on concrete programs, so we will discuss this orthogonal but important issue in Section \ref{section-discussion}. Besides the algebraic operations, a complexity expression can also refer to a measure of (a subpart of) a size variable. For intuitiveness we write ``$|x|$'' for ``$x.\symwork$''

We introduced a new ``hide'' type constructor denoted as $\thide{\tau}$, its corresponding terms $(\symhide e)$ and $(\symunhide e)$, and its corresponding size constructor $(\symShide\;s)$. The motivation for this new type constructor is illustrated by this example: $f:\forall A.\;\arrow{\intro{x}{\symlist\;A}}{|x|}{}{\symlist\;A}$. When one first looks at this function one may think its complexity is linear with the input list's length. But when one instantiates it with a type, such as $(\symlist\;\symbool)$, $f$ suddenly becomes $f\;(\symlist\;\symbool):\arrow{\intro{x}{\symlist\;(\symlist\;\symbool)}}{|x|}{}{\symlist\;(\symlist\;\symbool)}$. Its complexity is no longer linear with the outer list's length! The problem is that there is not a way for a polymorphic function to express the notion of ``ignoring values of the parameter type'' in its complexity specification. Now that we have ``hide'' types and define list as $\symlist \defeq \Lambda A.\;\mu X.\;\symunit + \thide{A} \times X$, and the measure calculation ignores everything under a $\symShide$ node, both $f$'s and $(f\;(\symlist\;\symbool))$'s complexity specification mean the length of the input list.

\subsection{Typing Rules}

Figure \ref{typing} gives representative typing rules. Typing judgments have the form $\Gamma\typing{e}{\tau}{c}{s}$. $c$ stands for the time bound of $e$ executing to a value. $s$ stands for the size bound of the final value. The typing rules formalize our intuition of how to count the reduction steps. In the typing context $\Gamma$ (defined in Figure \ref{syntax}), we reuse term variables $x$ to mean both a value and its size (conflating term variable and size variable), so the typing context becomes dependent (entries in it can depend on entries before them), and type variables become interleaved with term variables. But still our type system is not dependent-typed.

In each typing entry $x:(\tau,\optional{s})$ of $\Gamma$ there is an optional size $\optional{s}$. It is needed because of a nuance in the design of rule {\sc MatchLR}. For ease of type checking, in this rule we want to propagate time information forward: calculating each branch's time complexity and max them in the end. But because we want to avoid defining max for sizes, we want to propagate size information backward: taking the size requirement in the end and check whether it is satisfied in each branch. For this reason, the size requirement $s'$ in each branch is the same as the overall size requirement (which cannot mention branch-local variable $x$), and each branch needs the fact that the branch-local variable $x$ is just an alias of some outer term and its size, hence $\optional{s}$. Type information is also propagated backward since there may be arrows (with sizes) within the type.



(To be continued)

\begin{figure}
\fbox{$\Gamma\typing{e}{\tau}{c}{s}$}
\begin{mathpar}

\inferrule* [Right=Var] 
{\Gamma(x)=(\tau, s)} 
{\Gamma\typing{x}{\tau}{0}{s}} 

\inferrule* [Right=Var2] 
{\Gamma(x)=(\tau, \bot)} 
{\Gamma\typing{x}{\tau}{0}{x}} 

\inferrule* [Right=App] 
{\Gamma\typing{e_1}{\arrow{\intro{x}{\tau_1}}{c}{s}{\tau_2}}{c_1}{s_1} \\ \Gamma\typing{e_2}{\tau_1}{c_2}{s_2} }
{\Gamma\typing{e_1\;e_2}{\tau_2}{c_1+c_2+n[s_2/x]}{s[s_2/x]}} 

\inferrule* [Right=Abs] 
{\Gamma\vdash\tau_1 : * \\ \Gamma, x:(\tau_1, \bot)\typing{e}{\tau_2}{c}{s} }
{\Gamma\typing{\lambda x : \tau_1.\;e}{\arrow{\intro{x}{\tau_1}}{c}{s}{\tau_2}}{0}{0}} 

\\

\inferrule* [Right=Tapp]
{\Gamma\typing{e}{\forall^c_s X.\tau}{c'}{1}}
{\Gamma\typing{e\;\tau_2}{\tau[\tau_2/X]}{c'+c}{s}}

\inferrule* [Right=Tabs]
{\Gamma, X\typing{e}{\tau}{c}{s}}
{\Gamma\typing{\lambda X.\;e}{\forall X^c_s.\tau}{0}{0}} 

\\

\inferrule* [Right=Fold]
{\tau \equiv \mu X. \tau_1 \\ \Gamma\typing{e}{\tau_1[\tau/X]}{c}{s}}
{\Gamma\typing{\symfold \tau \; e}{\tau}{c}{\symSfold s}}

\inferrule* [Right=Unfold]
{\Gamma\typing{e}{t}{c}{s} \\ \mathsf{IsFold}(s)=s_1 \\ \tau \equiv \mu X.\tau_1}
{\Gamma\typing{\symunfold\;e}{\tau_1[\tau/X]}{1+c}{s_1}}

\inferrule* [Right=Hide]
{\Gamma\typing{e}{\tau}{c}{s}}
{\Gamma\typing{\symhide e}{\thide{\tau}}{c}{\symShide s}} 

\inferrule* [Right=Unhide]
{\Gamma\typing{e}{\thide{\tau}}{c}{s} \\ \mathsf{IsHide}(s)=s_1}
{\Gamma\typing{\symunhide e}{\tau}{c}{s_1}}

\inferrule* [Right=Le]
{\Gamma\typing{e}{\tau}{c}{s} \\ c \leq c' \\ s \leq s'}
{\Gamma\typing{e}{\tau}{c'}{s'}} \\

\inferrule* [Right=Pair]
{\Gamma\typing{e_1}{\tau_1}{c_1}{s_1} \\ \Gamma\typing{e_2}{\tau_2}{c_2}{s_2}}
{\Gamma\typing{(e_1,e_2)}{\tau_1\times\tau_2}{c_1+c_2}{(s_1,s_2)}} 

\inferrule* [Right=MatchPair]
{\Gamma\typing{e}{\tau_1\times\tau_2}{c}{s} \\ \mathsf{IsPair}(s)=(s_1,s_2) \\ \Gamma, a:(\tau_1,s_1), b:(\tau_2,s_2)\typing{e'}{\tau}{c'}{s'}}
{\Gamma\typing{\symmatch\;e \symwith (a,b) \Rightarrow e'}{\tau[s_1/a][s_2/b]}{c + c'[s_1/a][s_2/b]}{s[s_1/a][s_2/b]}} 

\\

\inferrule* [Right=MatchLR]
{\Gamma\typing{e}{\tau_1+\tau_2}{c}{s} \\ \mathsf{IsLR}(s)=(s_1,s_2) \\ \forall i,\;\Gamma, x:(\tau_i,s_i)\typing{e_i}{\tau}{c_i}{s'} \\ x\not\in\FV(\tau)\cup\FV(s')}
{\Gamma\typing{\symmatch\; e \symwith \syminl\;x \Rightarrow e_1 \;|\; \syminr\;x \Rightarrow e_2}{\tau}{c+\symmax(c_1[s_1/x],c_2[s_2/x])}{s'}} 

\\

\inferrule* [Right=MatchL]
{\Gamma\typing{e}{\tau_1+\tau_2}{c}{\symSinl(s)} \\ \Gamma, x:(\tau_1,s)\typing{e_1}{\tau'}{c'}{s'}}
{\Gamma\typing{\symmatch\; e \symwith \syminl\;x \Rightarrow e_1 \;|\; \syminr\;x \Rightarrow e_2}{\tau'[s/x]}{c+c'[s/x]}{s'[s/x]}}

\end{mathpar}
\caption{\label{typing}Typing rules}
\end{figure}

\section{(For 6.888) Curret Status}

I have encoded all the definitions in Coq, and proved the typing of the merge-sort example. Currently I'm proving the type soundness, having designed a logical relation that will be the main proof technique.

\section{\label{section-proof}Type Soundness Proof}

\section{\label{section-related}Related Work}

\section{\label{section-discussion}Discussion and Conclusion}

\appendix

%% \acks

%% Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\bibliography{bib.bib}

%% \begin{thebibliography}{}
%% \softraggedright

%% \bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%% P. Q. Smith, and X. Y. Jones. ...reference text...

%% \end{thebibliography}


\end{document}

%                       Revision History
%                       -------- -------
%  Date         Person  Ver.    Change
%  ----         ------  ----    ------

%  2013.06.29   TU      0.1--4  comments on permission/copyright notices

