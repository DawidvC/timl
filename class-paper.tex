%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath,amsfonts,amscd,amssymb,proof,MnSymbol}
\usepackage{mathpartir}
\usepackage{turnstile}% http://ctan.org/pkg/turnstile
\usepackage{adjustbox}% http://ctan.org/pkg/adjustbox
\usepackage{color}
\usepackage{listings}
\usepackage{lstcoq}

%% \renewcommand{\makehor}[4]
%%   {\ifthenelse{\equal{#1}{n}}{\hspace{#3}}{}
%%    \ifthenelse{\equal{#1}{s}}{\rule[-0.5#2]{#3}{#2}}{}
%%    \ifthenelse{\equal{#1}{d}}{\setlength{\lengthvar}{#2}
%%      \addtolength{\lengthvar}{0.5#4}
%%      \rule[-\lengthvar]{#3}{#2}
%%      \hspace{-#3}
%%      \rule[0.5#4]{#3}{#2}}{}
%%    \ifthenelse{\equal{#1}{t}}{\setlength{\lengthvar}{1.5#2}
%%      \addtolength{\lengthvar}{#4}
%%      \rule[-\lengthvar]{#3}{#2}
%%      \hspace{-#3}
%%      \rule[-0.5#2]{#3}{#2}
%%      \hspace{-#3}
%%      \setlength{\lengthvar}{0.5#2}
%%      \addtolength{\lengthvar}{#4}
%%      \rule[\lengthvar]{#3}{#2}}{}
%%    \ifthenelse{\equal{#1}{w}}{% New wavy $\sim$ definition
%%      \setbox0=\hbox{$\sim$}%
%%      \raisebox{-.6ex}{\hspace*{-.05ex}\adjustbox{width=#3,height=\height}{\clipbox{0.75 0 0 0}{\usebox0}}}}{}
%%   }

\newcommand{\thide}[1]{\left \{ #1 \right \}}
%% \newcommand{\typing}[4]{\turnstile{s}{s}{#4}{#3}{n}#1:#2}
\newcommand{\typing}[4]{\sststile{#4}{#3}#1:#2}
\newcommand{\related}[4]{\sdtstile{#4}{#3}#1:#2}
\newcommand{\kinding}[2]{\turnstile{s}{s}{}{}{n}#1:#2}
\newcommand{\teq}[2]{#1\equiv#2}
\newcommand{\arrow}[4]{#1\xrightarrow[#3]{#2}#4}
\newcommand{\symlet}{\mathsf{let\;}}
\newcommand{\symin}{\mathsf{\;in\;}}
\newcommand{\symletrec}{\mathsf{letrec\;}}
\newcommand{\symand}{\mathsf{\;and\;}}
\newcommand{\symmatch}{\mathsf{match}}
\newcommand{\FV}{\mathsf{FV}}
\newcommand{\symwith}{\mathsf{\;with\;}}
\newcommand{\symreturn}{\mathsf{\;return\;}}
\newcommand{\syminl}{\mathsf{inl}}
\newcommand{\syminr}{\mathsf{inr}}
\newcommand{\symmax}{\mathsf{max}}
\newcommand{\symSinl}{\mathsf{Sinl\;}}
\newcommand{\symSinr}{\mathsf{Sinr\;}}
\newcommand{\symfold}{\mathsf{fold\;}}
\newcommand{\symSfold}{\mathsf{Sfold}}
\newcommand{\symunfold}{\mathsf{unfold\;}}
\newcommand{\symSunfold}{\mathsf{Sunfold\;}}
\newcommand{\symhide}{\mathsf{hide\;}}
\newcommand{\symShide}{\mathsf{Shide}}
\newcommand{\symunhide}{\mathsf{unhide\;}}
\newcommand{\symSunhide}{\mathsf{Sunhide\;}}
\newcommand{\leO}{\preceq}
\newcommand{\sympair}{\mathsf{pair}}
\newcommand{\symtt}{\mathsf{tt}}
\newcommand{\symunit}{\mathsf{unit}}
\newcommand{\symlist}{\mathsf{list}}
\newcommand{\symnil}{\mathsf{nil}}
\newcommand{\symcons}{\mathsf{cons}}
\newcommand{\symfix}{\mathsf{fix}}
\newcommand{\symbool}{\mathsf{bool}}
\newcommand{\symtrue}{\mathsf{true}}
\newcommand{\symfalse}{\mathsf{false}}
\newcommand{\symmerge}{\mathsf{merge}}
\newcommand{\relV}[1]{\mathcal{V}\lsem#1\rsem}
\newcommand{\relE}[1]{\mathcal{E}\lsem#1\rsem}
\newcommand{\relEC}[1]{\mathcal{EC}\lsem#1\rsem}
\newcommand{\later}{\triangleright}
\newcommand{\vtos}[1]{\lfloor #1 \rfloor}

%% \newcommand{\intro}[2]{#2^#1}
\newcommand{\intro}[2]{(#1 : #2)}
%% \newcommand{\intro}[2]{(#2 \mathsf{\;size\;} #1)}
%% \newcommand{\intro}[2]{\{#2 \mathsf{\;|\;} #1\}}

\newcommand{\symsum}{\mathsf{sum}}
\newcommand{\symfst}{\mathsf{fst}}
\newcommand{\symsnd}{\mathsf{snd}}
\newcommand{\symif}{\mathsf{if\;}}
\newcommand{\symthen}{\mathsf{\;then\;}}
\newcommand{\symelse}{\mathsf{\;else\;}}
\newcommand{\symSbool}{\mathsf{Sbool}}
\newcommand{\symuf}{\mathsf{uf}}
\newcommand{\symuh}{\mathsf{uh}}
\newcommand{\syml}{\mathsf{l}}
\newcommand{\symr}{\mathsf{r}}
\newcommand{\symf}{\mathsf{f}}
\newcommand{\syms}{\mathsf{s}}
\newcommand{\symmsort}{\mathsf{msort}}
\newcommand{\symSstat}{\mathsf{Sstat}}
\newcommand{\symsplit}{\mathsf{split}}
\newcommand{\symprod}{\mathsf{prod}}
\newcommand{\symStt}{\mathsf{Stt}}
\newcommand{\symSpair}{\mathsf{Spair}}
\newcommand{\symSlr}{\mathsf{Ssum}}
\newcommand{\symwork}{\mathsf{w}}
\newcommand{\symspan}{\mathsf{s}}

%% \newcommand{\defeq}{\triangleq}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\newcommand{\logo}{\lambda^{\forall,\omega,\mu}_\mathrm{c}}
\newcommand{\Sstats}[1]{\left \langle #1 \right \rangle}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newcommand{\optional}[1]{\lfloor #1 \rfloor}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

%% \titlebanner{banner above paper title}        % These are ignored unless
%% \preprintfooter{short description of paper}   % 'preprint' option specified.

\title{$\logo$ : Complexity Recursive Types}
%% \subtitle{Subtitle Text, if any}

\authorinfo{Peng Wang}
           {MIT CSAIL}
           {wangpeng@csail.mit.edu}

\maketitle

\begin{abstract}
This paper contributes the first time-complexity type system with recursive types. The type system lets functional programmers specify time-complexity bounds of functions, and the type system checks and guarantees such bounds. This language is a milestone towards the ``great synthesis'' of complexity analysis and programming languages, for it supports universal polymorphism, type-level operators and isomorphic recursive types, the combined power of which enables generic algebraic data types, the watershed of practical usefulness. Potential beneficiaries of our language includes everyday functional programmers and computer theoriests, using it both as a practical bug-preventing tool and as a formal logic to prove algorithm complexities. We formalize our design and proof in the Coq proof assistant.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore, 
% you may leave them out
%% \terms
%% term1, term2

\keywords
type system; complexity; recursive types

\section{Introduction}

One major obstacle of the adaptation of functional programming is the lack of a clear intuition of functional program's time complexity. A C programmer can easily read off the big-O complexity of a for-loop, but even experienced functional programmers are sometimes muddled about the time complexity of a piece of functional code. The confusion comes from the lack of knowledge of the compiler's inner-working, as stated by the scoff ``you can hide an elephant under a beta reduction''. Two examples of the mythical objects that compilers manage internally are closures and algebraic data values. When one does not know how closures are created, passed, and used, he or she will rightfully wonder the time cost of creating a lambda abstraction, passing a function, and applying a function (beta reduction). For values of algebraic data types such as list, the immutability of these values (one can only construct new values, not in-place modify them) often gives a C programmer the wrong impression that such values are copied and passed-by-value all the time and that passing a list as argument will take O(n) time because of the copying. The reality could not be further from this false impression, since immutability means almost everything can be safely implemented by pointers and references.

But requiring every programmer to fully understand the inner-working of functional-language compilers is infeasible and unreasonable. One should be able to reason about programs in the language in which she actually writes, not in the target language of the compiler. It is the compiler's responsibility to preserve all properties of interest from the source program to the target program. So to demythify the complexity of function programming, two pieces of work need to be done: (1) we need a way to let the programmers easily reason about the complexity of functional programs in the source language level, and (2) we need a compiler that preserves the complexity of the source program downto the target program. This paper deals with the first part, with some assumptions about the second part. 

One usually reasons about functional programs using its operational semantics. The closest thing related to time cost in operational semantics is the number of reduction steps (in some counting metric) in small-step operational semantics, with some evaluation strategy. The design choices are (1) which evaluation strategy to use and (2) in which metric to do the counting. For (1), we choose strict call-by-value evaluation strategy against call-by-name strategy, because we think call-by-value fits people's intuition of evaluation order better, and it makes our assumption about the compiler more plausible. Call-by-name evaluation strategy has the drawback that since it can postpone computations and executing them in a sudden burst, a reduction step in the evaluation strategy could take an arbitrary amount of time on a real machine.

For design choice (2), there are work~\cite{blelloch2013cache} that employ ingenious cost models in which the counting is done, and we do believe that we need well-designed cost models to account for various machine characteristics when we want to have a more precise analysis of the time cost. But for this work, we choose the simplest cost model - the number of reduction steps in a standard strict call-by-value evaluation strategy. The justification for this choice is that (a) our top-level theorem will give any well-typed program a big-O bound on the number of reduction steps, which allows for a constant factor to be the room of imprecision, and (b) we make the assumption that {\bf any reduction step in a standard strict call-by-value evaluation strategy can be implemented by the compiler as a constant-time computation (excluding garbage collection)}. When we later make clear our definition of ``constant'' and ``non-constant'', we will come back to this assumption and try to persuade the reader that this is a reasonable assumption. Combining (a) and (b), we will have a end-to-end theorem of the big-O bound on the actual running time (defined in a well-accepted machine model such as the Random Access Machine (RAM)), independent of our choice of functional cost model, which can be treated as a intermediate proof step.

Using such a cost model, programmers can do the counting in their heads and reason about their programs' complexity informally. However, we want to allow programmers to formally express their complexity constraints in a type system, which can formally guarantee the complexity via type-checking. Type systems are for preventing bugs, and we think that calling a function of a wrong complexity order, such as calling an O(n) function when an O(1) one is intended, is really a bug, not a ``performance issue''. Complexity, or at least complexity order, should be a constituent of functional correctness.

We designed such a type system for the calculus $\lambda^{\forall,\omega\,\mu}$ (that is, lambda calculus extended with impredicative (first-class) universal polymorphism, type-level operators, and (isomorphic) recursive types). Our main contribution is {\bf the first complexity type system with recursive types}. We believe this is an important milestone because the combiningg power of $\lambda^\forall$, $\lambda^\omega$, and $\lambda^\mu$ gives birth to generic algebraic data types such as lists and trees, which is the minimal requirement for a functional language to become practically ``useful''.

We imagine two groups of potential users for such a language and type system. The first group is everyday programmers, for whom this type system can be incorporated into the ML toolchain and provide additional benefits besides the already very useful ML type system. To be user-friendly, we must have a type checker (which means the type system needs to be proved decidable) and reasonable degree of type inference. We will discuss this in Section \ref{section-discussion} as future work.

The second group of users are computer theorists conducting complexity proofs of algorithms. As~\cite{harper2014proposal} points out, there is an unfornate crevice between ``combinatorial'' theoriests and programming language theoriests. Members of the former still believe that the only mathematically rigid way to do complexity analysis is on the assembly level - on the Turning Machine model or Random Access Machine model; while the latter community has decades of achievements raising the abstraction level and building large systems \emph{compositionally} out of smaller components. The combinatorial theorists actually often give up rigidity by using some ``pseudo-code'' in the papers and assume that the reader can straight-forwardly ``compile'' the pseudo-code into rigid RAM code in his or her head. We hope that our language can serve as the start point when computer theoriests can express their complexity proof in a rigid whilst high-level formal language, enjoying the benefits of modern functional languages such as compositionality and abstraction. Our language is only a start point because it still lacks such important features as mutable references (and mutable arrays), which is essential for implementing matrix-based numeric algorithms; and such advanced complexity analysis as probablistic and amortized analysis, which have become mainstream for modern complexity analysis. We also lack parallelism, though in our design we intentionlly maintain the generality (we have the notion of ``work'' and ``span'') to make later extension to parallelism easier.

Section \ref{section-example} will give an illustrating example, the merge-sort, to show what the type system can express. It will also give the top-level type soundness theorem. Section \ref{section-lang} defines the language and the typing rules. Section \ref{section-proof} gives the proof of the type soundness, using Logical Step-indexed Logical Relation~\cite{dreyer2009logical}. Section \ref{section-related} discusses related work. Section \ref{section-discussion} discusses this work's strength and limitations, future work and conclusion.

\section{\label{section-example}Merge-Sort Example and Main Theorem}

The merge-sort code and the types of relevant functions are shown in Figure \ref{msort}. Function $\symmsort$ is generic for the element type of the input list and generic for the comparison function. The type of $\symmsort$ requires that the comparison function $cmp$ should compare a pair of elements within 1 unit time (the meaning of ``1 unit'' will be explained later), and guarantees that $\symmsort$ will finish within $|s|*\log(|s|)$ units of time. As seen from the example, complexities are annotated on the arrows in the type, where above the arrow is the time complexity bound of the function, while below the arrow is the size bound of the this function's result value. In order to refer to the input size, each type on the left side of an arrow will introduce a \emph{size variable}, whose scope consists of the complexity expressions above and below the arrow, and the type on the right side of the arrow. Together with currying, the complexity expressions can refer to multiple arguments' sizes for n-arity functions, as shown in the type of the $\symmerge$ function. This notation may give a superficially dependent-typed flavor, but the type system is not dependent-typed because types can only depend on size variables, not values, and size variables will only be used on the arrows.

\begin{figure}
\begin{align*}
\symmsort &\defeq \lambda A. \lambda cmp.\;\symfix\;f(xs). \\
& \hspace{.1in} \symmatch\;xs\symwith \\
& \hspace{.2in} |\; \symnil\Rightarrow xs \\
& \hspace{.2in} |\; \_::xs' \Rightarrow \symmatch\;xs'\symwith \\
& \hspace{.3in} |\; \symnil\Rightarrow xs \\
& \hspace{.3in} |\; \_ \Rightarrow \symmatch\; \symsplit\;xs \symwith \\
& \hspace{.4in} |\; (ys, zs) \Rightarrow \symmerge\;cmp\;(f\;ys)\;(f\;zs) \\
& \hspace{-0.2in} : \forall A.\;\arrow{(\arrow{A}{}{}{\arrow{A}{1}{}{\symbool}})}{}{}{\arrow{\intro{s}{\symlist\;A}}{|s|*\log(|s|)}{\Sstats{s}}{\symlist\;A}} \\
\mathsf{where} & \\
\symsplit &: \forall A.\;\arrow{\intro{s}{\symlist\;A}}{|s|/2}{\Sstats{s}/2}{\symlist\;A\times\symlist\;A} \\
\symmerge &: \forall A.\;\arrow{(\arrow{A}{}{}{\arrow{A}{1}{}{\symbool}})}{}{}{\arrow{\intro{x}{\symlist\;A}}{}{}{\arrow{\intro{y}{\symlist\;A}}{|x|+|y|}{\Sstats{x}+\Sstats{y}}{\symlist\;A}}}
\end{align*}
\caption{\label{msort}Merge-sort}
\end{figure}

The ``size'' of an input value is not merely a natural number, as one might expect, but a tree-like structure, since we aim to deal with general algebraic data types such as trees. In order to use such a size in a time complexity expression, which is merely a number, we defined some measures on a size structure. Here the $|s|$ measure corresponds to the length of a list if $s$ is the size of a list value. The definition of $|-|$ will be given in Section \ref{section-lang}. In the size expression below the arrow, $\Sstats{s}$ stands for a size that has the same measures as size $s$, but not necessarily the same structure. In this example it can be understood as ``the size of a list whose length is the same as $s$'', which is a proper description of merge-sort's result.  The notations $\Sstats{s}/2$ and $\Sstats{x}+\Sstats{y}$ means ``a list of half length'' and ``a list of combined length'' respectively, whose precise definitions are given in Section \ref{section-lang}.

A unit of time in the complexity expression is defined as one unfold-fold reduction, and the complexity expression states the number of unfold-fold reductions. All other reductions are ignored by complexity expressions. The reason for this seemingly bizarre choice is that (1) the actual number of all reductions depends on some details of the input value, which are not accounted for by its size structure in our definition; and our type soundness theorem (see below) will state that (2) the actual number of all reduction steps will be bounded by the number of unfold-fold reductions, with a constant factor. Claim (2) is true crucially because we do not have built-in fixpoint or recursive-let in our language, so the only source of unbounded execution is via some recursive-type encoding of some combinators, which all have the property that each recursive call will involve at least one unfold-fold reduction (for the encoding we use to define fixpoint, the number is exactly one). The unfold-fold count thus covers the number of recursive calls, which is essentially the only thing that changes with input size. For functional programmers who are more familiar with algebraic data types than recursive types, they can think of the count as ``the number of recursive calls plus the number of pattern-matchings on recursive data structures''. Counting this is actually easier than counting all reduction steps, since not all programmers are familiar with the whole set of evaluation rules. 

Now we give the top-level type soundness theorem of our type system. It has two parts. Theorem \ref{thm-safety} states the standard type safety, in terms of nonstuckness. Theorem \ref{thm-boundedness}, distinguishing for our type system, states the time-boundedness of any well-typed program in our language.

\begin{thm}[\label{thm-safety}Soundness w.r.t. Safety]
$$
\begin{array}{l}
\forall e\;\tau, \\
\hspace{.1in} \vdash e:\tau \Rightarrow \\
\hspace{.2in} \forall e',\; e \leadsto^* e' \Rightarrow e'\in\mathsf{Val} \;\vee\; \exists e'',\;e' \leadsto e''.
\end{array}
$$
\end{thm}

\begin{thm}[\label{thm-boundedness}Soundness w.r.t. Boundedness]
$$
\begin{array}{l}
\forall f\;\tau_1\;\tau_2\;c\;s, \\
\hspace{0.1in} \vdash f:\arrow{(x:\tau_1)}{c}{s}{\tau_2} \wedge f\in\mathsf{Val}\Rightarrow \\
\hspace{0.2in} \exists B, \;\forall v,\; \vdash v:\tau_1 \Rightarrow \\
\hspace{0.3in} \forall e'\;n,\; f\;v\leadsto^n e' \Rightarrow n\leq B\times c[\vtos{v}/x]
\end{array}
$$
\end{thm}

Theorem \ref{thm-safety}, the standard nonstuckness property, says that for any well-typed program (in an empty typing context), wherever the program runs to ($e'$), it is either successfully done (be a value) or can take one more step. That is, it won't get stuck in some erronous state where there is no legal step to progress.

Theorem \ref{thm-boundedness} says that for any well-typed value $f$ of the arrow type $\arrow{(x:\tau_1)}{c}{s}{\tau_2}$, there exists some constant factor $B$, so that for any input value of the right type, the composed expression $f\;v$ won't run for more than $B\times c[\vtos{v}/x]$ steps ($\vtos{v}$ means the size of value $v$), where $c$ is the complexity guarantee given by the type. Combining it with Theorem \ref{thm-safety}, we can conclude that $f\;v$ will always successfully terminate within $B\times c[\vtos{v}/x]$ steps. Here a step $\leadsto$ means any reduction step in a standard strict call-by-value evaluation strategy (defined in Section \ref{section-lang}), not just the unfold-fold count used for complexity expressions in types. Note that the existentially quantified $B$ is outside hence independent of the input value, which meets the standard definition of asymptotic complexity. 

Theorem \ref{thm-boundedness} is only one corollary from the strenghened result in the main proof, for unary functions. We can have theorems such as Theorem \ref{thm-boundedness2} for binary and n-arity functions, and the existentially quantified constant factor will be independent of any input.

\begin{thm}[\label{thm-boundedness2}Soundness w.r.t. Boundedness (2-arity)]
$$
\begin{array}{l}
\forall f\;\tau_1\;\tau_2\;\tau_3\;c\;s, \\
\hspace{0.1in} \vdash f:\arrow{(x_1:\tau_1)}{0}{0}{\arrow{(x_2:\tau_2)}{c}{s}{\tau_3}} \wedge f\in\mathsf{Val}\Rightarrow \\
\hspace{0.2in} \exists B, \;\forall v_1\;v_2,\; (\forall i,\;\vdash v_i:\tau_i) \Rightarrow \\
\hspace{0.3in} \forall e'\;n,\; f\;v_1\;v_2\leadsto^n e' \Rightarrow n\leq B\times c[\vtos{v_1}/x_1][\vtos{v_2}/x_2]
\end{array}
$$
\end{thm}

Technically, Theorem \ref{thm-boundedness} is only true for first-order cases, where type $\tau_1$ does not have any arrow. For higher-order cases, the theorem takes a slightly more complicated form, as will be explained in Section \ref{section-proof}.

\section{\label{section-lang}Language and Type System}

\subsection{Syntax}

The syntax of the language is given in Figure \ref{syntax}. Excluding the complexity and size part, and the ``hide'' type $\thide{\tau}$, all the syntax and evaluation context are standard. We put type annotations in lambda, $\syminl$, $\syminr$ and fold to make type unique (modulo complexity annotations), and elide them when irrelevant. We also borrow the return clause of pattern-matching from Coq to guide the type checker to find the common type and size of all branches. All type variables are restricted to kind $*$, so we do not need kind annotations in type-level binders. The call-by-value evaluation rules, the kinding rules and the type equivalence rules are all standard and give in Appendix \ref{append1}

\begin{figure}
$$\begin{array}{rrcl}
  \textrm{Size Subpart Idx} & i &::=& \symf \textrm{(fst)} \mid \syms \textrm{(snd)} \mid \syml \textrm{(left)} \mid \symr \textrm{(right)} \mid \symuf \textrm{(unfold)} \\
  & & & \mid \symuh \textrm{(unhide)} \\
  \textrm{Complexity Expr} & c &::=& x.\vec{i}.\symwork \textrm{(work)} \mid x.\vec{i}.\symspan \textrm{(span)} \mid 0 \mid 1 \mid c+c \mid \cdots \\
  \textrm{Sizes} & s &::=& x.\vec{i} \mid \Sstats{c_\symwork,c_\symspan} \mid \symSlr\;s\;s \\
  & & & \mid \symSpair\;s\;s \mid \symSfold\;s \mid \symShide\;s \\
  \textrm{Types} & \tau &::=& \symunit \mid \tau\times\tau \mid \tau+\tau \mid \alpha \mid \arrow{\intro{x}{\tau}}{c}{s}{\tau} \\
  & & & \mid \forall^c_s \alpha.\tau \mid \Lambda \alpha.\tau \mid \tau\;\tau \mid \mu \alpha.\tau \mid \thide\tau \\
  \textrm{Expressions} & e &::=& \symtt \mid \sympair\;e\;e \mid \syminl_\tau\;e \mid \syminr_\tau\;e \mid x \mid e\;e \mid \lambda x:\tau.\;e \\
  & & & \mid \symlet x := e \symin e \mid e\;\tau \mid \lambda \alpha.e \mid \symfold\tau\;e \\
  & & & \mid \symunfold\;e \mid \symhide\;e \mid \symunhide\;e \mid \symfst\;e \mid \symsnd\;e \\
  & & & \mid (\symmatch\;e \symreturn \tau \symand s \symwith \\
  & & & \hspace{.1in} \syminl\;x\Rightarrow e\;|\;\syminr\;x\Rightarrow e) \\
  \textrm{Values} & v &::=& x \mid \lambda x:\tau.e \mid \lambda \alpha.e \mid \symfold\tau\;v \mid \symtt \mid (v,v) \\
  & & & \mid \syminl\;v \mid \syminr\;v \mid \symhide v \\
  \textrm{CBV Eval Ctx} & E &::=& \Box \mid E\;e \mid v\;E \mid \symlet x := E\symin e \mid E\;\tau \\
  & & & \mid \symfold\tau\;E \mid \symunfold\tau\;E \mid \symhide\;E \mid \symunhide\;E \\
  & & & \mid (E, e) \mid (v, E) \mid \syminl_\tau\;E \mid \syminr_\tau\;E \mid \symfst\;E \mid \symsnd\;E \\
  & & & \mid (\symmatch\;E\symreturn \tau \symand s \symwith \\
  & & & \hspace{.1in} \syminl\;x\Rightarrow e\;|\;\syminr\;x\Rightarrow e) \\
  \textrm{Kinds} & k &::=& * \mid *\to k \\
  \textrm{Typing Ctx} & \Gamma &::=& \cdot \mid \Gamma,\alpha \mid \Gamma, x:\tau
\end{array}$$
\caption{\label{syntax}Syntax}
\end{figure}

Now let's turn to the complexity and size part. Firstly, complexity and size annotations only appear on arrows and in $\forall$ binders (the latter can be seen as a degenerated case of the former). Each arrow type $\arrow{\intro{x}{\tau_1}}{c}{s}{\tau_2}$ introduces a size variable standing for the input size, which can be referred to in $c$, $s$ and $\tau_2$ (but not $\tau_1$). The $\forall$ binder does not introduce new size variables. The reason for $\forall$ to also have complexity annotation is that, just like $\lambda x.e$, $\lambda \alpha.e$ is a ``delayed'' computation, meaning that it is itself a value but when been applied, its inner part may take some steps to reach another value. But in most cases, the $c$ and $s$ in $\forall^c_s$ are just 0. When $c$ or $s$ is 0, we elide it in this paper.

We first explain the definition of sizes. As mention before, the size of a value is a tree-like structure, closely mirroring the structure of a value. Comparing ``Sizes'' and ``Values'' shows that they are almost isomorphic except that sizes do not have $\lambda$-abstractions but have constructors $x.\vec{i}$ and $\Sstats{c_\symwork,c_\symspan}$. $x.\vec{i}$ is used to refer to a subpart of a size variable. One can go as deep as she like into a size variable (ToDo: explain). The query indices for subparts are the corresponding eliminators of size constructors. $\Sstats{c_\symwork,c_\symspan}$ is used to construct a size that has no structural information, only the measures. There are two measures, ``work'' and ``span'', borrowed from the parallel computation literature. ``Work'' means the number of ``$\symSfold$'' nodes in the size; ``span'' means the height of the size seen as a tree, only considering ``$\symSfold$'' nodes. ``Span'' seems useless in a non-parallel setting, but remember we can define trees, where ``span'' is meaningful even in a non-parallel setting. 

The intuition behind the definition of size is that the $\symSfold$-skeleton is the real ``variable'' thing in a size, and when we say ``how computation grows with the input size'' we actually mean ``how computation grows with the input size's $\symSfold$-skeleton''.

We need the $\Sstats{c_\symwork,c_\symspan}$ constructor because $s$ is also our specification language for a function's result size (below the arrow), and usually measures are the only thing we can specify about the result size. For brevity we write ``$\Sstats{s}$'' for $\Sstats{s.\symwork,s.\symspan}$, ``0'' for $\Sstats{0,0}$, ``$\Sstats{c_1,c_2}/2$'' for $\Sstats{c_1/2,c_2/2}$ and similarly lift other operations on measures to sizes. We also overload the notation ``$(a,b)$'' to stands for $(\sympair\;a\;b)$, $(\symSpair\;a\;b)$ and other pair-like constructions.

Now we explain the time complexity expression. Since variables can appear in a complexity expression, we define complexity expression as an abstract algebra instead of just natural numbers. Which algebra to use (hence the expressiveness of complexity expressions) is almost an orthogonal issue of the type system design. We will elaborate the axioms we used in the type soundness proof regarding this algebra, thus any algebra satisfying these axioms will give us a sound type system. Do note that the choice of this algebra greatly influence the complexity precision and type checking/inference difficulty on concrete programs, so we will discuss this orthogonal but important issue in Section \ref{section-discussion}. Besides the algebraic operations, a complexity expression can also refer to a measure of (a subpart of) a size variable. For intuitiveness we write ``$|x|$'' for ``$x.\symwork$''

We introduced a new ``hide'' type constructor denoted as $\thide{\tau}$, its corresponding terms $(\symhide e)$ and $(\symunhide e)$, and its corresponding size constructor $(\symShide\;s)$. The motivation for this new type constructor is illustrated by this example: $f:\forall A.\;\arrow{\intro{x}{\symlist\;A}}{|x|}{}{\symlist\;A}$. When one first looks at this function one may think its complexity is linear with the input list's length. But when one instantiates it with a type, such as $(\symlist\;\symbool)$, $f$ suddenly becomes $f\;(\symlist\;\symbool):\arrow{\intro{x}{\symlist\;(\symlist\;\symbool)}}{|x|}{}{\symlist\;(\symlist\;\symbool)}$. Its complexity is no longer linear with the outer list's length! The problem is that there is not a way for a polymorphic function to express the notion of ``ignoring values of the parameter type'' in its complexity specification. Now that we have ``hide'' types and define list as $\symlist \defeq \Lambda A.\;\mu \alpha.\;\symunit + \thide{A} \times \alpha$, and the measure calculation ignores everything under a $\symShide$ node, both $f$'s and $(f\;(\symlist\;\symbool))$'s complexity specification mean the length of the input list.

\subsection{Typing Rules}

Figure \ref{typing} gives representative typing rules. Typing judgments have the form $\Gamma\typing{e}{\tau}{c}{s}$, where $c$ stands for the time bound of $e$ executing to a value. $s$ stands for the size bound of the final value. The typing rules formalize our intuition of how to count the reduction steps. In the typing context $\Gamma$ (defined in Figure \ref{syntax}), we reuse term variables $x$ to mean both a value and its size (conflating term variable and size variable), so the typing context becomes dependent (entries in it can depend on entries before them), and type variables become interleaved with term variables. But still our type system is not dependent-typed.

As is usually the case for functional languages, the most essential rules are {\sc App} and {\sc Abs}. They directly correspond to our intuition of how to count steps, and they demonstrate the meaning of the annotations on the arrow. Rule {\sc App} postulates that the time to evaluate $e_1\;e_2$ is the time to evaluate $e_1$ (i.e. $c_1$) plus that for $e_2$ (i.e. $c_2$), plus the time to evaluate the function body (i.e. $c[s_2/x]$). The final size is given by $s[s_2/x]$. Rule $Abs$ introduces arrows and its annotations. Since $\lambda$-abstract is already a value, and its size is 0 (ToDo: explain), we have $\turnstile{s}{s}{0}{0}{n}$.

Rule {\sc Tapp} and {\sc Tabs} are similar to {\sc App} and {\sc Abs} respectively, since $\forall$ is just another kind of ``arrow''. Rule {\sc Unfold} is the only place where time complexity is actually increased, corresponding to a unfold-fold reduction. The partial functions $\mathsf{IsFold}$, $\mathsf{IsHide}$, $\mathsf{IsPair}$ and $\mathsf{IsSum}$ check that the size has the right structure, and destruct it into subparts. Especially, they know how to properly destruct a size variable. As an example, $\mathsf{IsPair}$ is defined as

$$
\mathsf{IsPair}(s)\defeq\begin{cases}
(s_1, s_2) & \mbox{if } s = \symSpair\;s_1\;s_2 \\
(x.\syml, x.\symr) & \mbox{if } s = x \\
\bot & \mbox{otherwise}\\
\end{cases}
$$

Rule {\sc LE} relaxes and time and size bounds. The definitions of $c\leq c'$ and $s\leq s'$, given in Figure \ref{cle} and Figure \ref{sle} respectively, are a crucial part of design (ToDo: elaborate).

\begin{figure}
\fbox{$c_1\leq c_2$}
\begin{mathpar}
\end{mathpar}
(ToDo: fill in)
\caption{\label{cle}Complexity Order}
\end{figure}

\begin{figure}
\fbox{$s_1\leq s_2$}
\begin{mathpar}
\end{mathpar}
(ToDo: fill in)
\caption{\label{sle}Size Order}
\end{figure}


\begin{figure}
\fbox{$\Gamma\typing{e}{\tau}{c}{s}$}
{\small
\begin{mathpar}

\inferrule* [Right=Var] 
{\Gamma(x)=\tau} 
{\Gamma\typing{x}{\tau}{0}{x}} 

\inferrule* [Right=App] 
{\Gamma\typing{e_1}{\arrow{\intro{x}{\tau_1}}{c}{s}{\tau_2}}{c_1}{s_1} \\ \Gamma\typing{e_2}{\tau_1}{c_2}{s_2} }
{\Gamma\typing{e_1\;e_2}{\tau_2}{c_1+c_2+c[s_2/x]}{s[s_2/x]}} 

\inferrule* [Right=Abs] 
{\Gamma\vdash\tau_1 : * \\ \Gamma, x:\tau_1\typing{e}{\tau_2}{c}{s} }
{\Gamma\typing{\lambda x : \tau_1.\;e}{\arrow{\intro{x}{\tau_1}}{c}{s}{\tau_2}}{0}{0}} 

\\

\inferrule* [Right=Tapp]
{\Gamma\typing{e}{\forall^c_s \alpha.\tau}{c'}{1}}
{\Gamma\typing{e\;\tau_2}{\tau[\tau_2/\alpha]}{c'+c}{s}}

\inferrule* [Right=Tabs]
{\Gamma, \alpha\typing{e}{\tau}{c}{s}}
{\Gamma\typing{\lambda \alpha.\;e}{\forall \alpha^c_s.\tau}{0}{0}} 

\\

\inferrule* [Right=Fold]
{\tau \equiv \mu \alpha. \tau_1 \\ \Gamma\typing{e}{\tau_1[\tau/\alpha]}{c}{s}}
{\Gamma\typing{\symfold \tau \; e}{\tau}{c}{\symSfold s}}

\inferrule* [Right=Unfold]
{\Gamma\typing{e}{t}{c}{s} \\ \mathsf{IsFold}(s)=s_1 \\ \tau \equiv \mu \alpha.\tau_1}
{\Gamma\typing{\symunfold\;e}{\tau_1[\tau/\alpha]}{1+c}{s_1}}

\inferrule* [Right=Hide]
{\Gamma\typing{e}{\tau}{c}{s}}
{\Gamma\typing{\symhide e}{\thide{\tau}}{c}{\symShide s}} 

\inferrule* [Right=Unhide]
{\Gamma\typing{e}{\thide{\tau}}{c}{s} \\ \mathsf{IsHide}(s)=s_1}
{\Gamma\typing{\symunhide e}{\tau}{c}{s_1}}

\inferrule* [Right=Le]
{\Gamma\typing{e}{\tau}{c}{s} \\ c \leq c' \\ s \leq s'}
{\Gamma\typing{e}{\tau}{c'}{s'}} \\

\inferrule* [Right=Pair]
{\Gamma\typing{e_1}{\tau_1}{c_1}{s_1} \\ \Gamma\typing{e_2}{\tau_2}{c_2}{s_2}}
{\Gamma\typing{(e_1,e_2)}{\tau_1\times\tau_2}{c_1+c_2}{(s_1,s_2)}} 

\\

\inferrule* [Right=Fst]
{\Gamma\typing{e}{\tau_1\times\tau_2}{c}{s} \\ \mathsf{IsPair}(s)=(s_1, s_2)}
{\Gamma\typing{\symfst\;e}{\tau_1}{c}{s_1}} 

\inferrule* [Right=Snd]
{\Gamma\typing{e}{\tau_1\times\tau_2}{c}{s} \\ \mathsf{IsPair}(s)=(s_1, s_2)}
{\Gamma\typing{\symsnd\;e}{\tau_2}{c}{s_2}} 

\\

\inferrule* [Right=Inl]
{\Gamma\typing{e}{\tau}{c}{s}}
{\Gamma\typing{\syminl_{\tau'}\;e}{\tau+\tau'}{c}{\symSlr\;s\;0}} 

\inferrule* [Right=Inr]
{\Gamma\typing{e}{\tau}{c}{s}}
{\Gamma\typing{\syminr_{\tau'}\;e}{\tau'+\tau}{c}{\symSlr\;0\;s}} 

\inferrule* [Right=Match]
{\Gamma\typing{e}{\tau_1+\tau_2}{c}{s'} \\ \mathsf{IsSum}(s')=(s_1,s_2) \\ \forall i,\;\Gamma, x:(\tau_i,s_i)\typing{e_i}{\tau}{c_i}{s} \\ x\not\in\FV(\tau)\cup\FV(s)}
{\Gamma\typing{\symmatch\; e \symreturn \tau \symand s \symwith \syminl\;x \Rightarrow e_1 \;|\; \syminr\;x \Rightarrow e_2}{\tau}{c+\symmax(c_1[s_1/x],c_2[s_2/x])}{s}} 

\\

\end{mathpar}
}
\caption{\label{typing}Typing rules}
\end{figure}

\section{\label{section-proof}Type Soundness Proof}

Since this system is strictly more restrictive than System F, Thoerem \ref{thm-safety} is straight-forward to prove, and we will focus on the proof of Theorem \ref{thm-boundedness} (in its more general form). The proof is done by using Logical Step-indexed Logical Relation~\cite{dreyer2009logical}.

Logical relations (or logical predicates for unary relations as in this case) are essentially interpretations of types. By interpreting a type into the set of ``good expressions'' that have the right type and property, logical relations can be used to prove that any well-typed expression has the wanted property. Following the common practice, we define our logical relations in terms of $\relV{-}$ and $\relE{-}$. The logical relation definitions are given in Figure \ref{lrel}.

\begin{figure}
\fbox{$\relV{\tau}$} and \fbox{$\relE{\tau}$}
{\small
%% $$
%% \begin{array}{ll}
\begin{align*}
  \relV{\alpha}_\rho &\defeq \rho(\alpha) \\
  \relV{\symunit}_\rho &\defeq \lambda(v,w).\; v \downarrow \rho(\tau) \wedge w \downarrow \rho(\tau) \\
  \relV{\tau_1\times\tau_2}_\rho &\defeq \lambda(v,w).\; v \downarrow \rho(\tau) \wedge w \downarrow \rho(\tau) \wedge \exists v_1\;v_2\;w_1\;w_2, \\
  & \hspace{.1in} v=(v_1,v_2) \wedge w=(w_1,w_2) \wedge (v_1,w_1) \in \relV{\tau_1}_\rho \wedge \\
  & \hspace{.1in} (v_2,w_2) \in \relV{\tau_2}_\rho \\
  \relV{\tau_1+\tau_2}_\rho &\defeq \lambda(v,w).\; v \downarrow \rho(\tau) \wedge w \downarrow \rho(\tau) \wedge \exists v'\;w', \\
  & \hspace{.1in} v=\syminl\;v' \wedge w=\syminl\;w' \wedge (v',w') \in \relV{\tau_1}_\rho \vee \\
  & \hspace{.1in} v=\syminr\;v' \wedge w=\syminr\;w' \wedge (v',w') \in \relV{\tau_2}_\rho \\
  \relV{\arrow{\tau_1}{c}{s}{\tau_2}}_\rho &\defeq \lambda (v,w).\;v \downarrow \rho(\tau) \wedge w \downarrow \rho(\tau) \wedge \exists e,\;v=\lambda x.e \wedge \\
  & \hspace{.1in} w=\lambda x.(B,w_2) \wedge \forall v_1\;w_1,\;(v_1,w_1) \in \relV{\tau_1}_\rho \Rightarrow \\
  & \hspace{.1in} (e[v_1/x], w_2[w_1/x])\in \relE{\tau_2}^{\rho(c[\vtos{v_1}/x]),\rho(s[\vtos{v_1}/x])}_{\rho[x\mapsto(v_1,w_1)],B[w_1/x]} \\
  \relV{\forall^c_s\tau_1}_\rho &\defeq \lambda (v,w).\;v \downarrow \rho(\tau) \wedge w \downarrow \rho(\tau) \wedge \exists e,\;v=\lambda\alpha.e \wedge \\
  & \hspace{.1in} w=\lambda.(B,w_2) \wedge \forall \tau'\;r,\;r \in \mathsf{VRel}(\tau') \Rightarrow \\
  & \hspace{.1in} (e[\tau'/\alpha], w_2)\in \relE{\tau_1}^{\rho(c),\rho(s)}_{\rho[\alpha\mapsto(\tau',r)],B} \\
  \relV{\mu\alpha.\tau'}_\rho &\defeq \mu r.\;\lambda (v,w).\; v \downarrow \rho(\tau) \wedge w \downarrow \rho(\tau) \wedge \exists v'\;w',\; \\
  & \hspace{.1in} v = \symfold v' \wedge w = \symfold w' \wedge \\
  & \hspace{.1in} \later (v',w') \in \relV{\tau'}_{\rho[\alpha\mapsto(\rho(\tau),r)]} \\
  \relE{\tau}^{c,s}_{\rho,B} &\defeq \lambda(e,w).\;\vdash e:\rho(\tau) \wedge \vdash w:\rho(\tau) \wedge \\
  & \hspace{.1in} (\exists m, B \Downarrow \mathsf{const}\;m \wedge \forall n\;e',\; e\leadsto^n_0 e' \Rightarrow n \leq m) \wedge \\
  & \hspace{.1in} (\forall v\;w',\; e\Downarrow v \wedge w\Downarrow w'\Rightarrow (v,w') \in \relV{\tau}_\rho) \wedge \\
  & \hspace{.1in} (\forall e',\; e\leadsto^*_1 e' \Rightarrow c > 0 \wedge \later (e',w)\in \relE{\tau}^{c-1,s}_{\rho,B})
\end{align*}
%% \end{array}
%% $$
}
\caption{\label{lrel}Logical Relations}
\end{figure}

\subsection {Logical Step-indexed Logical Relation}
Recursive types cause trouble for defining logical relations because in the recursive type case the unfolded type is not strictly smaller, which prevents $\relV{\tau}$ from being defined recursively on $\tau$ (it cannot be defined inductively as a proposition either since in the arrow type case there is a non-positive appearance of $\relV{-}$). One solution is to define $\relV{\tau}$ recursively on another decreasing parameter called ``step index'' \cite{ahmed2006step}. Here we employ another solution, by defining the logical relation in Figure \ref{lrel} in a special logic introduced in \cite{dreyer2009logical} called Logical Step-indexed Logical Relation (LSLR). The crucial novel connectives of this logic are the recursive binder $\mu$ and the later operator $\later$. A syntactic rule says that any appearance of variables introduced by $\mu$ must be under $\later$. With this syntactic constraint, this logic can be given a Kripke model that corresponds to step-indices in \cite{ahmed2006step}. 

The syntax of LSLR is summarized in Figure \ref{lslr}. It is a second-order logic, so we distinguish between first-order variable $x$ (which can range over any domain other than $R$) and second-order variable $r$ (which impredicatively ranges over $R$), and the $\forall$ and $\exists$ quantifiers on them. $\lceil P \rceil$ lifts any proposition $P$ in the meta-logic (Coq in our case) to LSLR. Each LSLR formula $R$ is also associated with an arity, elided here and made precise in our Coq formalization using dependent types (see Figure \ref{coq-lslr}). For example, $R$ must have arity 0 in $R \wedge R$ and $\later R$, and $\lambda x.R$ and $x\in R$ increases and decreases the arity respectively. $D$ is a parameter of LSLR which determines how LSLR formulas are interpreted. For example, if $D$ is chosen to be closed expressions, then LSLR formulas with arity 1 are interpreted as sets of closed expressions. In our usage, we instantiate $D$ to be pairs of closed-expression/closed-width (explained below).

A LSLR validity judgment takes the form
$$
\mathcal{X};\mathcal{R};\mathcal{P}\vdash R
$$
where $\mathcal{X}$ is the context (a list) of first-order variables, $\mathcal{R}$ is the context of second-order variables, and $\mathcal{P}$ is a list of LSLR propositions (arity 0) well-formed in context, serving as premises. $R$ must be a well-formed LSLR proposition of arity 0. Readers are referred to \cite{dreyer2009logical} for the rules for establishing validity.

\begin{figure}
  $$\begin{array}{rrcl}
    & R &::=& r \mid x \mid \lceil P \rceil \mid R \wedge R \mid R \vee R \mid R \Rightarrow R \mid \forall x,R \mid \exists x,R \\
    & & & \mid \forall r,R \mid \exists r,R \mid \lambda x:D.R \mid x\in R \mid \mu r. R \mid \later R \\
  \end{array}$$
  \caption{\label{lslr}Logical Step-indexed Logical Relation}
\end{figure}

\subsection {Logical Relation Design}

The most salient feature of our logical relation design is that the target of interpretation is no longer sets of expressions, but sets of pairs of expression and ``width'' (denoted by $w$).

``Width'' stands for the constant factor in asymptotic complexity analysis, such as the $B$ in Theorem \ref{thm-boundedness}. It is the maximal number of steps allowed bewteen two consecutive unfold-fold reduction. The reason it is a structural thing instead of just a natural number is that, for example in Theorem \ref{thm-boundedness}, when $\tau_1$ has arrows in it, the ``width'' of function $f$ (i.e. $B$) actually dependents on the width of the input argument $v$. For example, suppose $\tau_1=\arrow{\symunit}{1}{0}{\symunit}$, $v$ could be any of the following values:
$$
\begin{array}{l}
  \lambda x.\symtt \\
  \lambda x.\symlet y=\symtt\symin\symtt \\
  \lambda x.\symlet y=\symtt\symin\symlet y=\symtt\symin\symtt \\
  \lambda x.\symlet y=\symtt\symin\symlet y=\symtt\symin\symlet y=\symtt\symin\symtt \\
  \cdots
\end{array}
$$

So if $f=\lambda v.\; v\;\symtt$, the width of $f$ will depend on the width of $v$. Equivalently by skolemization, $B$ in Theorem \ref{thm-boundedness} should be a function from width to width, instead of just a natural number. Taking widths into consideration, Theorem \ref{thm-boundedness} becomes
\begin{thm}[\label{thm-boundedness'}Soundness w.r.t. Boundedness]
$$
\begin{array}{l}
\forall f\;\tau_1\;\tau_2\;c\;s, \\
\hspace{0.1in} \vdash f:\arrow{(x:\tau_1)}{c}{s}{\tau_2} \wedge f\in\mathsf{Val}\Rightarrow \\
\hspace{0.2in} \exists B, \;\forall v\;w_1,\; \vdash v:\tau_1 \wedge v\textrm{\ has\ width\ } w_1 \Rightarrow \\
\hspace{0.3in} \forall e'\;n,\; f\;v\leadsto^n e' \Rightarrow n\leq B(w_1)\times c[\vtos{v}/x]
\end{array}
$$
\end{thm}
where the ``$v$ has width $w_1$'' part needs to be made formal.

This line of thinking needs to be taken further, because arrows can appear anywhere inside a type, and the structure of widths becomes almost isomorphic to the structure of expressions, with all the $\lambda$-abstraction, pair and recursive structures. Thus widths become ``shadow expressions'', accompanying expressions everywhere in our logical relations. The definition of widths is give by Figure \ref{widths}. Since they are almost isomorphic to expressions, we use the same notations for both of them.

\begin{figure}
$$\begin{array}{rrcl}
  \textrm{Widths} & w &::=& \symtt \mid \sympair\;w\;w \mid \syminl\;w \mid \syminr\;w \mid x \mid w(w)_1 \mid \lambda x.(B,w) \\
  & & & \mid w\;\tau \mid \lambda.(B,w) \mid \symfold w \mid \symunfold\;w \mid \symfst\;w \mid \symsnd\;w \\
  & & & \mid (\symmatch\;w \symwith\syminl\;x\Rightarrow w\;|\;\syminr\;x\Rightarrow w) \\
  & B &::=& \mathsf{const}\;n \mid w(w)_2 \mid B\;+\;B \mid B\;*\;B \mid \symmax\;B\;B \\
\end{array}$$
\caption{\label{widths}Widths}
\end{figure}

An $\lambda$-abstraction of width packages two pieces of information: $B$, the width of the function body; and $w$, the width structure of the result value. Since the $\lambda$-abstraction has two pieces of information, we need two kinds of applications to get the information out, hence $w(w)_1$ and $w(w)_2$. Syntactic class $B$ is a symbolic representation of natural numbers, meaning that it has abstractions and applications in it, but it should evaluates to a natural number. We also have definitions of values and evaluation rules for widths, similar to those for expressions. Since widths are not terminating by definition, in the logical relations we have to explicitly require that they terminate.

The design in Figure \ref{lrel} largely follows \cite{dreyer2009logical}, with the addition of widths accompanying and mirroring expressions. As in \cite{dreyer2009logical}, in order to make types strictly shrink for recursive definition, substitution for type variables are postponed and the substitutes are collected in a substitution table $\rho$, defined in Figure \ref{lrel-aux}. For each type variable $\alpha$, we need both a syntactic substitute $\tau$ and a semantic substitute $r$. The former is needed to close an open type; the latter is needed in the $\relV{\alpha}_\rho$ case. Because our types can have size variables, we also need to collect substitutes for size variables. And Because $\rho$ is also used later to close open expressions and open widths, we directly put $(v,w)$ as the substitute for size variable (and term variable) $x$ (size can be calculated from a value).

\begin{figure}
$$\begin{array}{rrcl}
  & \rho &::=& [] \mid \rho[\alpha\mapsto(\tau,r)] \mid \rho[x\mapsto(v,w)]\\
  & r\in\mathsf{VRel}(\tau) &\defeq& \forall v\;w,\;(v,w)\in r \Rightarrow v\downarrow\tau \wedge w\downarrow\tau \\
  & v \downarrow \tau &\defeq& \vdash v:\tau \wedge v\in\mathsf{Val} \\
  & e \Downarrow v &\defeq& e\leadsto^*v \wedge v\in\mathsf{Val} \\
  & e \leadsto^n_m e' &\defeq& n\textrm{\ steps\ wherein\ }m\textrm{\ are\ unfold-folds} \\
  & w \downarrow \tau &\defeq& \textrm{similar to }e\downarrow\tau \\
  & w \Downarrow w' &\defeq& \textrm{similar to }e\Downarrow v \\
\end{array}$$
\caption{\label{lrel-aux}Auxiliary Definitions for Logical Relation}
\end{figure}

The clauses of $\relE{\tau}$ are so designed to enable the proof of Lemma \ref{lem-bind}. The $(\exists m,\cdots)$ clause states the meaning of ``width'' to be ``the maximal number of steps between two consecutive unfold-fold reductions''. $e\leadsto^n_l e'$ means $e$ evaluates to $e'$ with $n$ reduction steps (any kind) within which $l$ of them are unfold-fold reductions. Since $B$ is a symbolic representation of natural numbers which does not necessarily terminate, we must explicitly require that it terminate to a natural number $m$. The next clause is a usual requirement connecting $\relE{\tau}$ and $\relV{\tau}$. The last clause, as in \cite{dreyer2009logical}, is needed because $\relV{\mu\alpha.\tau}$ only gives us $\later (v',w') \in \relV{\tau'}$, and we need to establish $\relE{\tau'}$ from this later/weaker information.

The roadmap of using logical relations is to split the theorem we want to prove, schematically written as $\vdash e:\tau \Rightarrow P_\tau(e)$, into two lemmas: (1) the Foundamental Lemma, $\vdash e:\tau \Rightarrow e\in\relE{\tau}$; and (2) the Adequacy Lemma, $e\in\relE{\tau} \Rightarrow P_\tau(e)$. The Foundamental Lemma is proved by induction on the typing derivation. The logical relations defined in Figure \ref{lrel} are only for closed expressions, but for the proof-by-induction to work, we need to expand it to open terms in a typing context. As an example, suppose a typing context $\Gamma=\alpha_1,x_2:\tau_2,x_3:\tau_3,\alpha_4,\cdots,\alpha_{n-1},x_n:\tau_n$, we define:

$$
\begin{array}{rcl}
\mathcal{X}&\defeq&\alpha_1,x_2,w_2,x_3,w_3,\alpha_4,\cdots,\alpha_{n-1},x_n,w_n \\
\mathcal{R}&\defeq&r_1,r_4,\cdots,r_{n-1} \\
\rho&\defeq&\alpha_1\mapsto(\alpha_1,r_1),x_2\mapsto(x_2,w_2),x_3\mapsto(x_3,w_3),\\
& & \alpha_4\mapsto(\alpha_4,r_4),\cdots,\alpha_{n-1}\mapsto(\alpha_{n-1},r_{n-1}),x_n\mapsto(x_n,w_n) \\
\mathcal{P}&\defeq&r_1\in\mathsf{VRel}(\alpha_1),(x_2,w_2)\in\relV{\tau_2}_\rho,(x_3,w_3)\in\relV{\tau_3}_\rho,\\
& & r_4\in\mathsf{VRel}(\alpha_4),\cdots,r_{n-1}\in\mathsf{VRel}(\alpha_{n-1}),(x_n,w_n)\in\relV{\tau_n}_\rho
\end{array}
$$
and define the ``related'' relation as

$$
w;B;\Gamma\related{e}{\tau}{c}{s}\defeq\mathcal{X};\mathcal{R};\mathcal{P}\vdash(\rho(e),\rho(w))\in\relE{\tau}^{\rho(c),\rho(s)}_{\rho,\rho(B)}
$$

We can now precisely state the Foundamental Lemma and the Adequacy Lemma:

\begin{lem}[\label{lem-foundamental}Foundamental]
$$
\begin{array}{l}
\forall \Gamma\;c\;s\;e\;\tau,\\
\hspace{.4in} \Gamma\typing{e}{\tau}{c}{s} \Rightarrow \exists w\;B,\; w;B;\Gamma\related{e}{\tau}{c}{s}.\\
\end{array}
$$
\end{lem}

\begin{lem}[\label{lem-adequacy}Adequacy]
$$
\begin{array}{l}
\forall w\;B\;c\;s\;e\;\tau,\\
\hspace{.4in} w;B;\cdot\related{e}{\tau}{c}{s} \Rightarrow \\
\hspace{.5in} \exists m,\; B\Downarrow \mathsf{const}\;m \wedge \forall n\;e',\;e\leadsto^ne'\Rightarrow n\leq(m+1)(c+1). \\
\end{array}
$$
\end{lem}

Theorem \ref{thm-boundedness'} is given by these two lemmas, letting $\Gamma=\cdot,c=0,s=0,\tau=\arrow{\tau_1}{c'}{s'}{\tau_2},$ and $e$ to be a value, and unfolding the definition of $\relV{\arrow{\tau_1}{c'}{s'}{\tau_2}}_{[]}$. Theorem \ref{thm-boundedness'} needs to be slightly modified by replacing $\vdash v:\tau_1$ with $(v,w_1)\in\relV{\tau_1}_{[]}$:

\begin{thm}[\label{thm-boundedness''}Soundness w.r.t. Boundedness]
$$
\begin{array}{l}
\forall f\;\tau_1\;\tau_2\;c\;s, \\
\hspace{0.1in} \vdash f:\arrow{(x:\tau_1)}{c}{s}{\tau_2} \wedge f\in\mathsf{Val}\Rightarrow \\
\hspace{0.2in} \exists B, \;\forall v\;w_1,\; (v,w_1)\in\relV{\tau_1}_{[]} \Rightarrow \\
\hspace{0.3in} \exists m,\; B(w_1)_2\Downarrow \mathsf{const}\;m \wedge \\
\hspace{0.4in} \forall e'\;n,\; f\;v\leadsto^n e' \Rightarrow n\leq (m+1)\times (c[\vtos{v}/x]+1)
\end{array}
$$
\end{thm}

Note that when $\tau_1$ does not contain arrows in it, Theorem \ref{thm-boundedness''} degenerates to Theorem \ref{thm-boundedness} (ignoring the $+1$ part), the most familiar form in complexity analysis. A take-away of this finding is that the familiar form of asymptotic complexity definition in theoretical computer science, where a constant factor is existentially quantified in the outmost, only works with first-order functions (whose arguments are not functions), not higher-order functions. Since computer theoriests usually do not consider higher-order functions, this aspect has long been overlooked.

The Foundamental Lemma is proved by induction on the typing derivation. Most the cases can be discharged by appealing to the Bind Lemma:

\begin{lem}[\label{lem-bind}Bind]
\begin{mathpar}
\inferrule*
{\mathcal{C}\vdash(e,w_e)\in\relE{\tau}^{c_1,s_1}_{\rho,B_e} \\ \mathcal{C}\vdash(e,w_e)\in\relEC{\tau,\tau'}^{s_1,c_2,s_2}_{E,\rho,\rho',w_E,B_E}} 
{\mathcal{C}\vdash(E[e],w_E)\in\relE{\tau'}^{c_1+c_2,s_2}_{\rho',B_e+B_E}}
\end{mathpar}
\end{lem}
where
$$
\begin{array}{l}
\relEC{\tau,\tau'}^{s_1,c_2,s_2}_{E,\rho,\rho',w_E,B_E}\defeq \\
\hspace{.2in} \lambda(e,w_e).\;\forall v\;w_e',\;(v,w_e')\in\relV{\tau}_\rho \wedge e\leadsto*v \wedge w_e\leadsto*w_e' \wedge \\
\hspace{.3in} \vtos{v}\leq s_1 \Rightarrow (E[v],w_E)\in\relE{\tau'}^{c_2,s_2}_{\rho',B_E}
\end{array}
$$

The intuition of the usefulness of this lemma is that when considering a term $E[e]$ (terms such as $e_1\;e_2$, $\symfold e$, $\sympair\;e_1\;e_2$, etc. can all be decomposed into this form), we only need to consider the case where $e$ is a value. So, for example, proving $e_1\;e_2\in\relE{\tau}$ can be reduced to proving $v_1\;v_2\in\relV{\tau}$ where we can appeal to the definition of $\relV{\tau}$.

(ToDo: list other key lemmas for the proof)

\section{\label{section-coq}Coq Formalization}

The central design decision in the Coq formalization of our language and proofs is how to encode the language and the LSLR logic used in the proof. For the language, we choose deBruijn indices, which is a common and popular encoding scheme for binding structures. In our Coq formalization, we used both dependent and non-dependent deBruijn indices, and found them having complemental strength and weakness and both worthwhile.

Dependent deBruijn-index encoding is to use dependent types to parameterize terms over a variable context, which is a list of in-scope variables (possibly with their types or other information). The dependent deBruijin-index encoding of our expressions is given in Figure \ref{coq-depdeb}.

\begin{figure}
\begin{coq}
Inductive ty := Ttype | Texpr.

Definition var t ctx := {n | nth_error ctx n = Some t}.

Inductive expr (ctx : list ty) :=
| Evar : var Texpr ctx -> expr ctx
| Eapp : expr ctx -> expr ctx -> expr ctx
| Eabs : type ctx -> expr (Texpr :: ctx) -> expr ctx
| Elet : expr ctx -> expr (Texpr :: ctx) -> expr ctx
| Etapp : expr ctx -> type ctx -> expr ctx
| Etabs : expr (Ttype :: ctx) -> expr ctx
| Efold : type ctx -> expr ctx -> expr ctx
| Eunfold : expr ctx -> expr ctx
| Ehide : expr ctx -> expr ctx
| Eunhide : expr ctx -> expr ctx
| Ett : expr ctx
| Epair : expr ctx -> expr ctx -> expr ctx
| Einl : type ctx -> expr ctx -> expr ctx
| Einr : type ctx -> expr ctx -> expr ctx
| Efst : expr ctx -> expr ctx
| Esnd : expr ctx -> expr ctx
| Ematch : expr ctx -> expr (Texpr :: ctx) -> expr (Texpr :: ctx) -> expr ctx.
\end{coq}
\caption{\label{coq-depdeb}Dependent deBruijn-index Encoding of Expressions}
\end{figure}

Non-dependent deBruijn-index encoding is the above encoding without the context part, so variables are represented by natural numbers with the traditional deBruijn-index scheme, and there is no guarentee that the natural number makes sense in the current variable scope. Dependent deBruijn-index encoding builds in scoping constraints, thus has more information and is more convenient to use (eliminate); on the other hand, terms in non-dependent deBruijn-index encoding is easier to create (introduce) because one can freely pick natural numbers regardless of scoping constraints. So we choose non-dependent deBruijn-index encoding for writing example programs and type-checking them, while dependent deBruijn-index encoding is used in our type soundness proof. The two are related by the fact that any well-typed non-dependent deBruijn-index encoded expression is well-scoped hence has a corresponding dependent deBruijn-index encoding.

Dependent deBruijn-index encoding could have been used also to encode the LSLR logic, but since most parts of LSLR formulas are just lifted Coq propositions, we want to use Coq's native Gallina terms for these lifted parts. When such a part refers a variable, such as $\forall x,\lceil P(x)\rceil$, $x$ also needs to be a Gallina variable, therefore we need Higher-Order Abstract Syntax (HOAS) encoding. As the solution, we use HOAS encoding for first-order variables in LSLR (because they need to appear in lifted Gallina terms), and dependent deBruijn indices for second-order variables (since HOAS cannot handle impredicative variables). The Coq definition of LSLR formulas is given in Figure \ref{coq-lslr}.

\begin{figure}
\begin{coq}
Inductive usability := Usable | Unusable.

Definition ty := (nat * usability).

Inductive rel : nat (*arity*)-> list ty -> Type :=
| Rvar {m ctx} : var (m, Usable) ctx -> rel m ctx
| Rinj {ctx} : Prop -> rel 0 ctx
| Rand {ctx} (_ _ : rel 0 ctx) : rel 0 ctx
| Ror {ctx} (_ _ : rel 0 ctx) : rel 0 ctx
| Rimply {ctx} (_ _ : rel 0 ctx) : rel 0 ctx
| Rforall1 {ctx T} : (T -> rel 0 ctx) -> rel 0 ctx
| Rexists1 {ctx T} : (T -> rel 0 ctx) -> rel 0 ctx
| Rforall2 {ctx m} : rel 0 (m :: ctx) -> rel 0 ctx
| Rexists2 {ctx m} : rel 0 (m :: ctx) -> rel 0 ctx
| Rabs {ctx m} : (D -> rel m ctx) -> rel (S m) ctx
| Rapp {ctx m} : rel (S m) ctx -> D -> rel m ctx
| Rrecur {ctx m} : rel m ((m, Unusable) :: ctx) -> rel m ctx
| Rlater {ctx} chg : rel 0 (change_usab chg ctx) -> rel 0 ctx
.
\end{coq}
\caption{\label{coq-lslr}Dependent deBruijn-index + HOAS Encoding of LSLR}
\end{figure}

The variable context in dependent deBruijn-index encoding is also used to encode the syntactic ``guardedness'' constraint in LSLR that variables introduced by $\mu$ only become ``usable'' under $\later$.

We also tried the Parametric Higher-Order Abstract Syntax (PHOAS) encoding but found it hard to define the Kriple-model interpretation of LSLR formulas.

\section{\label{section-related}Related Work}

\section{\label{section-discussion}Discussion and Conclusion}

\subsection{Lessons Learned}

\appendix

\section{\label{append1}Evaluation, Kinding and Type Equivalence Rules}

\begin{figure}
\fbox{$e\leadsto e'$}
\begin{mathpar}

\inferrule*
{e_1\leadsto e_2}
{E[e_1]\leadsto E[e_2]}

\inferrule*
{\;}
{(\lambda x:\tau.e)\;v \leadsto e[v/x]}

\inferrule*
{\;}
{\symlet x:\tau:=v \symin e \leadsto e[v/x]}

\inferrule*
{\;}
{\symfst\;(v_1,v_2)\leadsto v_1}

\inferrule*
{\;}
{\symsnd\;(v_1,v_2)\leadsto v_2}

\inferrule*
{\;}
{\symmatch\;\syminl\;v\symwith\syminl\;x\Rightarrow e_1\;|\;\syminr\;x\Rightarrow e_2 \leadsto e_1[v/x]}

\inferrule*
{\;}
{\symmatch\;\syminr\;v\symwith\syminl\;x\Rightarrow e_1\;|\;\syminr\;x\Rightarrow e_2 \leadsto e_2[v/x]}

\inferrule*
{\;}
{(\lambda \alpha.e)\;\tau \leadsto e[\tau/\alpha]}

\inferrule*
{\;}
{\symunfold\;(\symfold\tau\;v) \leadsto v}

\inferrule*
{\;}
{\symunhide(\symhide v) \leadsto v}

\end{mathpar}
\caption{\label{eval}Call-by-value Evaluation Rules}
\end{figure}

\begin{figure}
\fbox{$\Gamma\vdash \tau:k$}
\begin{mathpar}

\inferrule*
{\alpha\in\Gamma}
{\Gamma\kinding{\alpha}{*}}

\inferrule*
{\Gamma\kinding{\tau_1}{*\to k} \\ \Gamma\kinding{\tau_2}{*}}
{\Gamma\kinding{\tau_1\;\tau_2}{k}}

\inferrule*
{\Gamma,\alpha\kinding{\tau}{k}}
{\Gamma\kinding{\Lambda \alpha.\tau}{*\to k}}

\inferrule*
{\Gamma\kinding{\tau_1}{*} \\ \Gamma,x:(\tau_1,\bot)\kinding{\tau_2}{*}}
{\Gamma\kinding{\arrow{(x:\tau_1)}{c}{s}{\tau_2}}{*}}

\inferrule*
{\Gamma,\alpha\kinding{\tau}{*}}
{\Gamma\kinding{\forall \alpha.\tau}{*}}

\inferrule*
{\Gamma,\alpha\kinding{\tau}{*}}
{\Gamma\kinding{\mu \alpha.\tau}{*}}

\inferrule*
{\Gamma\kinding{\tau}{*}}
{\Gamma\kinding{\thide{\tau}}{*}}

\inferrule*
{\;}
{\Gamma\kinding{\symunit}{*}}

\inferrule*
{\Gamma\kinding{\tau_1}{*} \\ \Gamma\kinding{\tau_2}{*}}
{\Gamma\kinding{\tau_1\times\tau_2}{*}}

\inferrule*
{\Gamma\kinding{\tau_1}{*} \\ \Gamma\kinding{\tau_2}{*}}
{\Gamma\kinding{\tau_1+\tau_2}{*}}

\end{mathpar}
\caption{\label{kinding}Kinding Rules}
\end{figure}

\begin{figure}
\fbox{$\teq{\tau_1}{\tau_2$}}
\begin{mathpar}

\inferrule*
{\;}
{\teq{\tau}{\tau}}

\inferrule*
{\teq{\tau_1}{\tau_2} \\ \teq{\tau_2}{\tau_3}}
{\teq{\tau_1}{\tau_3}}

\inferrule*
{\teq{\tau_2}{\tau_1}}
{\teq{\tau_1}{\tau_2}}

\inferrule*
{\teq{\tau_1}{\tau_2}}
{\teq{\Lambda \alpha.\tau_1}{\Lambda \alpha.\tau_2}}

\inferrule*
{\teq{\tau_1}{\tau_1'} \\ \teq{\tau_2}{\tau_2'}}
{\teq{\tau_1\;\tau_2}{\tau_1'\;\tau_2'}}

\inferrule*
{\;}
{\teq{(\Lambda \alpha.\tau_1)\;\tau_2}{\tau_1[\tau_2/\alpha]}}

\inferrule*
{\teq{\tau_1}{\tau_1'} \\ \teq{\tau_2}{\tau_2'}}
{\teq{\arrow{(x:\tau_1)}{c}{s}{\tau_2}}{\arrow{(x:\tau_1')}{c}{s}{\tau_2'}}}

\inferrule*
{\teq{\tau_1}{\tau_2}}
{\teq{\mu \alpha.\tau_1}{\mu \alpha.\tau_2}}

\end{mathpar}
\caption{\label{teq}Type Equivalence Rules}
\end{figure}

%% \acks

%% Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\bibliography{bib.bib}

%% \begin{thebibliography}{}
%% \softraggedright

%% \bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%% P. Q. Smith, and X. Y. Jones. ...reference text...

%% \end{thebibliography}


\end{document}

%                       Revision History
%                       -------- -------
%  Date         Person  Ver.    Change
%  ----         ------  ----    ------

%  2013.06.29   TU      0.1--4  comments on permission/copyright notices

