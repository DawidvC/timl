%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath,amsfonts,amscd,amssymb,proof,MnSymbol}
\usepackage{mathpartir}
\usepackage{turnstile}% http://ctan.org/pkg/turnstile
\usepackage{adjustbox}% http://ctan.org/pkg/adjustbox
\usepackage{color}

\renewcommand{\makehor}[4]
  {\ifthenelse{\equal{#1}{n}}{\hspace{#3}}{}
   \ifthenelse{\equal{#1}{s}}{\rule[-0.5#2]{#3}{#2}}{}
   \ifthenelse{\equal{#1}{d}}{\setlength{\lengthvar}{#2}
     \addtolength{\lengthvar}{0.5#4}
     \rule[-\lengthvar]{#3}{#2}
     \hspace{-#3}
     \rule[0.5#4]{#3}{#2}}{}
   \ifthenelse{\equal{#1}{t}}{\setlength{\lengthvar}{1.5#2}
     \addtolength{\lengthvar}{#4}
     \rule[-\lengthvar]{#3}{#2}
     \hspace{-#3}
     \rule[-0.5#2]{#3}{#2}
     \hspace{-#3}
     \setlength{\lengthvar}{0.5#2}
     \addtolength{\lengthvar}{#4}
     \rule[\lengthvar]{#3}{#2}}{}
   \ifthenelse{\equal{#1}{w}}{% New wavy $\sim$ definition
     \setbox0=\hbox{$\sim$}%
     \raisebox{-.6ex}{\hspace*{-.05ex}\adjustbox{width=#3,height=\height}{\clipbox{0.75 0 0 0}{\usebox0}}}}{}
  }

\newcommand{\thide}[1]{\left \langle #1 \right \rangle}
\newcommand{\typing}[4]{\turnstile{s}{s}{#4}{#3}{n}#1:#2}
\newcommand{\kinding}[2]{\turnstile{s}{s}{}{}{n}#1:#2}
\newcommand{\teq}[2]{#1\equiv#2}
\newcommand{\arrow}[4]{#1\xrightarrow[#3]{#2}#4}
\newcommand{\symlet}{\mathsf{let\;}}
\newcommand{\symin}{\mathsf{\;in\;}}
\newcommand{\symletrec}{\mathsf{letrec\;}}
\newcommand{\symand}{\mathsf{\;and\;}}
\newcommand{\symmatch}{\mathsf{match}}
\newcommand{\FV}{\mathsf{FV}}
\newcommand{\symwith}{\mathsf{\;with\;}}
\newcommand{\syminl}{\mathsf{inl}}
\newcommand{\syminr}{\mathsf{inr}}
\newcommand{\symmax}{\mathsf{max}}
\newcommand{\symSinl}{\mathsf{Sinl\;}}
\newcommand{\symSinr}{\mathsf{Sinr\;}}
\newcommand{\symfold}{\mathsf{fold\;}}
\newcommand{\symSfold}{\mathsf{Sfold\;}}
\newcommand{\symunfold}{\mathsf{unfold\;}}
\newcommand{\symSunfold}{\mathsf{Sunfold\;}}
\newcommand{\symhide}{\mathsf{hide\;}}
\newcommand{\symShide}{\mathsf{Shide\;}}
\newcommand{\symunhide}{\mathsf{unhide\;}}
\newcommand{\symSunhide}{\mathsf{Sunhide\;}}
\newcommand{\leO}{\preceq}
\newcommand{\sympair}{\mathsf{pair}}
\newcommand{\symtt}{\mathsf{tt}}
\newcommand{\symunit}{\mathsf{unit}}
\newcommand{\symlist}{\mathsf{list}}
\newcommand{\symnil}{\mathsf{nil}}
\newcommand{\symcons}{\mathsf{cons}}
\newcommand{\symfix}{\mathsf{fix}}
\newcommand{\symbool}{\mathsf{bool}}
\newcommand{\symtrue}{\mathsf{true}}
\newcommand{\symfalse}{\mathsf{false}}
\newcommand{\symmerge}{\mathsf{merge}}

%% \newcommand{\intro}[2]{#2^#1}
\newcommand{\intro}[2]{(#1 : #2)}
%% \newcommand{\intro}[2]{(#2 \mathsf{\;size\;} #1)}
%% \newcommand{\intro}[2]{\{#2 \mathsf{\;|\;} #1\}}

\newcommand{\symsum}{\mathsf{sum}}
\newcommand{\symfst}{\mathsf{fst}}
\newcommand{\symsnd}{\mathsf{snd}}
\newcommand{\symif}{\mathsf{if\;}}
\newcommand{\symthen}{\mathsf{\;then\;}}
\newcommand{\symelse}{\mathsf{\;else\;}}
\newcommand{\symSbool}{\mathsf{Sbool}}
\newcommand{\symuf}{\mathsf{uf}}
\newcommand{\symuh}{\mathsf{uh}}
\newcommand{\syml}{\mathsf{l}}
\newcommand{\symr}{\mathsf{r}}
\newcommand{\symf}{\mathsf{f}}
\newcommand{\syms}{\mathsf{s}}
\newcommand{\symmsort}{\mathsf{msort}}
\newcommand{\symSstat}{\mathsf{Sstat}}
\newcommand{\symsplit}{\mathsf{split}}
\newcommand{\symprod}{\mathsf{prod}}
\newcommand{\symStt}{\mathsf{Stt}}
\newcommand{\symSpair}{\mathsf{Spair}}
\newcommand{\symSlr}{\mathsf{Slr}}
\newcommand{\defeq}{\triangleq}
\newcommand{\symwork}{\mathsf{w}}
\newcommand{\symspan}{\mathsf{s}}

\newcommand{\logo}{\lambda^{\forall,\omega,\mu}_\mathrm{c}}
\newcommand{\Sstats}[1]{\left \langle #1 \right \rangle}
\newtheorem{thm}{Theorem}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

%% \titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{$\logo$ : Complexity Recursive Types}
%% \subtitle{Subtitle Text, if any}

\authorinfo{Peng Wang}
           {MIT CSAIL}
           {wangpeng@csail.mit.edu}

\maketitle

\begin{abstract}
Efficiency of functional programs is important.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore, 
% you may leave them out
%% \terms
%% term1, term2

\keywords
type system; complexity; recursive types

\section{Introduction}

One major obstacle of the adaptation of functional programming is the lack of a clear intuition of functional program's time complexity. A C programmer can easily read off the big-O complexity of a for-loop, but even experienced functional programmers are sometimes muddled about the time complexity of a piece of functional code. The confusion comes from the lack of knowledge of the compiler's inner-working, as stated by the scoff ``you can hide an elephant under a beta reduction''. Two examples of the mythical objects that compilers manage internally are closures and algebraic data values. When one does not know how closures are created, passed, and used, he or she will rightfully wonder the time cost of creating a lambda abstraction, passing a function, and applying a function (beta reduction). For values of algebraic data types such as list, the immutability of these values (one can only construct new values, not in-place modify them) often gives a C programmer the wrong impression that such values are copied and passed-by-value all the time and that passing a list as argument will take O(n) time because of the copying. The reality could not be further from this false impression, since immutability means almost everything can be safely implemented by pointers and references.

But requiring every programmer to fully understand the inner-working of functional-language compilers is infeasible and unreasonable. One should be able to reason about programs in the language in which she actually writes, not in the target language of the compiler. It is the compiler's responsibility to preserve all properties of interest from the source program to the target program. So to demythify the complexity of function programming, two pieces of work need to be done: (1) we need a way to let the programmers easily reason about the complexity of functional programs in the source language level, and (2) we need a compiler that preserves the complexity of the source program downto the target program. This paper deals with the first part, with some assumptions about the second part. 

One usually reason about functional programs using its operational semantics. The closest thing related to time cost in operational semantics is the number of reduction steps (in some counting metric) in small-step operational semantics, with some evaluation strategy. The design choices are (1) which evaluation strategy to use and (2) in which metric to do the counting. For (1), we choose strict call-by-value evaluation strategy against call-by-name strategy, because we think call-by-value fits people's intuition of evaluation order better, and it makes our assumption about the compiler more plausible. Call-by-name evaluation strategy has the drawback that since it can postpone computations and executing them in a sudden burst, a reduction step in the evaluation strategy could take an arbitrary amount of time on a real machine.

For design choice (2), there are work~\cite{blelloch2013cache} that employ ingenious cost models in which the counting is done, and we do believe that we need well-designed cost models to account for various machine characteristics when we want to have a more precise analysis of the time cost. But for this work, we choose the simplest cost model - the number of reduction steps in a standard strict call-by-value evaluation strategy. The justification for this choice is that (a) our top-level theorem will give any well-typed program a big-O bound on the number of reduction steps, which allows for a constant factor to be the room of imprecision, and (b) we make the assumption that {\bf any reduction step in a standard strict call-by-value evaluation strategy can be implemented by the compiler as a constant-time computation (excluding garbage collection)}. When we later make clear our definition of ``constant'' and ``non-constant'', we will come back to this assumption and try to persuade the reader that this is a reasonable assumption. Combining (a) and (b), we will have a end-to-end theorem of the big-O bound on the actual running time (defined in a well-accepted machine model such as the Random Access Machine (RAM)), independent of our choice of functional cost model, which can be treated as a intermediate proof step.

Using such a cost model, programmers can do the counting in their heads and reason about their programs' complexity informally. However, we want to allow programmers to formally express their complexity constraints in a type system, which can formally guarantee the complexity via type-checking. Type systems are for preventing bugs, and we think that calling a function of a wrong complexity order, such as calling an O(n) function when an O(1) one is intended, is really a bug, not a ``performance issue''. Complexity, or at least complexity order, should be a constituent of functional correctness.

We designed such a type system for the calculus $\lambda^{\forall,\omega\,\mu}$ (that is, lambda calculus extended with impredicative (first-class) universal polymorphism, type-level operators, and recursive types). Our main contribution is {\bf the first complexity type system with recursive types}. We believe this is an important milestone because the combiningg power of $\lambda^\forall$, $\lambda^\omega$, and $\lambda^\mu$ gives birth to generic algebraic data types such as lists and trees, which is the minimal requirement for a functional language to become practically ``useful''.

We imagine two groups of potential users for such a language and type system. The first group is everyday programmers, for whom this type system can be incorporated into the ML toolchain and provide additional benefits besides the already very useful ML type system. To be user-friendly, we must have a type checker (which means the type system needs to be proved decidable) and reasonable degree of type inference. We will discuss this in Section \ref{section-discussion} as future work.

The second group of users are computer theorists conducting complexity proofs of algorithms. As~\cite{harper2014proposal} points out, there is an unfornate crevice between ``combinatorial'' theoriests and programming language theoriests. Members of the former still believe that the only mathematically rigid way to do complexity analysis is on the assembly level - on the Turning Machine model or Random Access Machine model; while the latter community has decades of achievements raising the abstraction level and building large systems \emph{compositionally} out of smaller components. The combinatorial theorists actually often give up rigidity by using some ``pseudo-code'' in the papers and assume that the reader can straight-forwardly ``compile'' the pseudo-code into rigid RAM code in his or her head. We hope that our language can serve as the start point when computer theoriests can express their complexity proof in a rigid whilst high-level formal language, enjoying the benefits of modern functional languages such as compositionality and abstraction. Our language is only a start point because it still lacks such important features as mutable references (and mutable arrays), which is essential for implementing matrix-based numeric algorithms; and such advanced complexity analysis as probablistic and amortized analysis, which have become mainstream for modern complexity analysis. We also lack parallelism, though in our design we intentionlly maintain the generality (we have the notion of ``work'' and ``span'') to make later extension to parallelism easier.

Section \ref{section-example} will give an illustrating example, the merge-sort, to show what the type system can express. It will also give the top-level type soundness theorem. Section \ref{section-lang} defines the language and the typing rules. Section \ref{section-proof} gives the proof of the type soundness, using Logical Step-indexed Logical Relation~\cite{dreyer2009logical}. Section \ref{section-related} discusses related work. Section \ref{section-discussion} discusses this work's strength and limitations, future work and conclusion.

\section{\label{section-example}Merge-sort Example and Main Theorem}

The merge-sort code and the types of relevant functions are shown in Figure \ref{msort}. Function $\symmsort$ is a generic on the element type of the input list and generic on the comparison function. The type of $\symmsort$ expresses the requirement that the comparison function $cmp$ should compare a pair of elements within 1 unit time (the meaning of ``1 unit'' will be explained later), and the guarantee that $\symmsort$ will finish within $|s|*\log(|s|)$ units of time. As seen from the example, complexities are annotated on the arrows in the type, where above the arrow is the time complexity of the function represented by this arrow, while below the arrow is the size bound of the this function's result value. In order to refer to the input size, each type on the left side of an arrow will introduce a \emph{size variable}, whose scope consists of the complexity expressions above and below the arrow, and the type on the right size of the arrow. Together with currying, the complexity expressions can refer to multiple arguments' sizes for n-arity functions, as shown in the type of the $\symmerge$ function. This notation may give a superficially dependent-typed flavor, but the type system is not dependent-typed because types can only depend on size variables, not values, and size variables will only be used on the arrows.

\begin{figure}
\begin{align*}
\symmsort &\defeq \lambda A. \lambda cmp.\;\symfix\;f(xs). \\
& \hspace{.1in} \symmatch\;xs\symwith \\
& \hspace{.2in} |\; \symnil\Rightarrow xs \\
& \hspace{.2in} |\; \_::xs' \Rightarrow \symmatch\;xs'\symwith \\
& \hspace{.3in} |\; \symnil\Rightarrow xs \\
& \hspace{.3in} |\; \_ \Rightarrow \symmatch\; \symsplit\;xs \symwith \\
& \hspace{.4in} |\; (ys, zs) \Rightarrow \symmerge\;cmp\;(f\;ys)\;(f\;zs) \\
& \hspace{-0.2in} : \forall A.\;\arrow{(\arrow{A}{0}{1}{\arrow{A}{1}{1}{\symbool}})}{0}{1}{\arrow{\intro{s}{\symlist\;A}}{|s|*\log(|s|)}{\Sstats{s}}{\symlist\;A}} \\
\mathsf{where} & \\
\symsplit &: \forall A.\;\arrow{\intro{s}{\symlist\;A}}{|s|/2}{\Sstats{s}/2}{\symlist\;A\times\symlist\;A} \\
\symmerge &: \forall A.\arrow{(\arrow{A}{0}{1}{\arrow{A}{1}{1}{\symbool}})}{0}{1}{\arrow{\intro{x}{\symlist\;A}}{0}{1}{\arrow{\intro{y}{\symlist\;A}}{|x|+|y|}{\Sstats{x}+\Sstats{y}}{\symlist\;A}}}
\end{align*}
\caption{\label{msort}Merge-sort}
\end{figure}

The ``size'' of an input value is not merely a natural number, as one might expect, but a tree-like structure, since we aim to deal with general algebraic data types such as trees. In order to use such a size in a time complexity expression, which is merely a number, one defined some measures on a size structure. Here the $|s|$ measure corresponds to the length of a list if $s$ is the size of a list value. The definition of $|-|$ will be given in Section \ref{section-lang}. In the size expression below the arrow, $\Sstats{s}$ stands for a size that has the same measures as size $s$, but not necessarily the same structure. In this example it can be understood as ``the size of a list whose length is the same as $s$'', which is a proper description of merge-sort's result value.  The notations $\Sstats{s}/2$ and $\Sstats{x}+\Sstats{y}$ means ``a list of half length'' and ``a list of combined length'' respectively, whose precise definitions are given in Section \ref{section-lang}. All non-recursive data structures such as functions and booleans will have size ``1'', also defined in Section \ref{section-lang}.

A unit of time in the complexity expression is defined as one unfold-fold reduction, and the complexity expression states the number of unfold-fold reductions. All other reductions are ignored by complexity expressions. The reason for this seemingly bizarre choice is that (1) the actual number of all reductions depends on some details of the input value, which are not accounted for by its size structure in our definition, and (2) our type soundness theorem (see below) will state that the actual number of all reduction steps will be bounded by the number of unfold-fold reductions, with a constant factor. (2) is true crucially because we do not have built-in fixpoint or recursive-let in our language, but encode fixpoint with recursive-typed Y combinator, which has the property that each recursive call will involve exactly one unfold-fold reduction. For a functional programmer who is more familiar with algebraic data types than recursive types, she can think of the count as ``the number of recursive calls plus the number of pattern-matchings on recursive data structures''. Counting this is actually easier than counting all reduction steps, since not all programmers are familiar with the whole set of evaluation rules.

Now we give the top-level type soundness theorem of our type system. It has two parts. Theorem \ref{thm-safety} states the standard type safety, in terms of nonstuckness. Theorem \ref{thm-boundedness}, distinguishing for our type system, states the time-boundedness of any well-typed program in our language.

\begin{thm}[\label{thm-safety}Soundness w.r.t. Safety]
$$
\begin{array}{l}
\forall e,\tau. \\
\hspace{.1in} \vdash e:\tau \Rightarrow \\
\hspace{.2in} \forall e'.\; e \leadsto^* e' \Rightarrow e'\in\mathsf{Val} \;\vee\; \exists e''.\;e' \leadsto e''.
\end{array}
$$
\end{thm}

\begin{thm}[\label{thm-boundedness}Soundness w.r.t. Boundedness]
$$
\begin{array}{l}
\forall f,\tau_1,\tau_2,c,s. \\
\hspace{0.1in} \vdash f:\arrow{(x:\tau_1)}{c}{s}{\tau_2} \wedge f\in\mathsf{Val}\Rightarrow \\
\hspace{0.2in} \exists C,\xi_0. \;\forall v.\; \vdash v:\tau_1 \wedge |v| \geq \xi_0 \Rightarrow \\
\hspace{0.3in} \forall e',n.\; f\;v\leadsto^n e' \Rightarrow n\leq C\times c[|v|/x]
\end{array}
$$
\end{thm}

Theorem \ref{thm-safety}, the standard nonstuckness property, says that for any well-typed program (in an empty typing context), wherever the program runs to ($e'$), it is either successfully done (be a value) or can take one more step. That is, it won't get stuck in some erronous state where there is no legal step to progress. Theorem \ref{thm-boundedness} says that for any well-typed value $f$ of the arrow type $\arrow{(x:\tau_1)}{c}{s}{\tau_2}$, there exists some constant factor $C$ and a threshold $\xi_0$, so that for any input value of the right type and whose size is larger than or equal to the threshold $\xi_0$, the composed expression $f\;v$ won't run for more than $C\times c[|v|/x]$ steps, where $c$ is the complexity guarantee given by the type. Combining it with Theorem \ref{thm-safety}, we can conclude that $f\;v$ will always successfully terminate within $C\times c[|v|/x]$ steps. Here a step $\leadsto$ means any reduction step in a standard strict call-by-value evaluation strategy (defined in Section \ref{section-lang}), not just the unfold-fold count used for the complexity annotation in types. Note that the existentially quantified $C$ and $\xi_0$ are outside hence independent of the input value, which meets the standard definition of asymptotic complexity. The reason we need a threshold for input size and do not guarentee time bound for inputs of all sizes is that sometimes the complexity expressions do not make sense with too small sizes, as will be seen in Section \ref{section-lang}. Small inputs do not matter since in asymptotic analysis we only care about the performance behavior as the inputs grow ever larger. We do guarentee safety on inputs of all sizes, since Theorem \ref{thm-safety} does not involve thresholds.

Theorem \ref{thm-boundedness} is only one corollary from the strenghened result in the main proof, for unary functions. We can have theorems such as Theorem \ref{thm-boundedness2} for binary and n-arity functions, and all existentially quantified constant factors and thresholds will be independent of any input.

\begin{thm}[\label{thm-boundedness2}Soundness w.r.t. Boundedness (2-arity)]
$$
\begin{array}{l}
\forall f,\tau_1,\tau_2,\tau_3,c,s. \\
\hspace{0.1in} \vdash f:\arrow{(x_1:\tau_1)}{0}{1}{\arrow{(x_2:\tau_2)}{c}{s}{\tau_3}} \wedge f\in\mathsf{Val}\Rightarrow \\
\hspace{0.2in} \exists C,\xi_0^1,\xi_0^2. \;\forall v_1,v_2.\; (\forall i.\;\vdash v_i:\tau_i \wedge |v_i| \geq \xi_0^i) \Rightarrow \\
\hspace{0.3in} \forall e',n.\; f\;v_1\;v_2\leadsto^n e' \Rightarrow n\leq C\times c[|v_1|/x_1][|v_2|/x_2]
\end{array}
$$
\end{thm}

\section{\label{section-lang}Language and Type System}

\section{\label{section-proof}Type Soundness Proof}

\section{\label{section-related}Related Work}

\section{\label{section-discussion}Discussion and Conclusion}

\appendix

%% \acks

%% Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\bibliography{bib.bib}

%% \begin{thebibliography}{}
%% \softraggedright

%% \bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%% P. Q. Smith, and X. Y. Jones. ...reference text...

%% \end{thebibliography}


\end{document}

%                       Revision History
%                       -------- -------
%  Date         Person  Ver.    Change
%  ----         ------  ----    ------

%  2013.06.29   TU      0.1--4  comments on permission/copyright notices

