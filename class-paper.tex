%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath,amsfonts,amscd,amssymb,proof,MnSymbol}
\usepackage{mathpartir}
\usepackage{turnstile}% http://ctan.org/pkg/turnstile
\usepackage{adjustbox}% http://ctan.org/pkg/adjustbox
\usepackage{color}
\usepackage{listings}
\usepackage{lstcoq}

%% \renewcommand{\makehor}[4]
%%   {\ifthenelse{\equal{#1}{n}}{\hspace{#3}}{}
%%    \ifthenelse{\equal{#1}{s}}{\rule[-0.5#2]{#3}{#2}}{}
%%    \ifthenelse{\equal{#1}{d}}{\setlength{\lengthvar}{#2}
%%      \addtolength{\lengthvar}{0.5#4}
%%      \rule[-\lengthvar]{#3}{#2}
%%      \hspace{-#3}
%%      \rule[0.5#4]{#3}{#2}}{}
%%    \ifthenelse{\equal{#1}{t}}{\setlength{\lengthvar}{1.5#2}
%%      \addtolength{\lengthvar}{#4}
%%      \rule[-\lengthvar]{#3}{#2}
%%      \hspace{-#3}
%%      \rule[-0.5#2]{#3}{#2}
%%      \hspace{-#3}
%%      \setlength{\lengthvar}{0.5#2}
%%      \addtolength{\lengthvar}{#4}
%%      \rule[\lengthvar]{#3}{#2}}{}
%%    \ifthenelse{\equal{#1}{w}}{% New wavy $\sim$ definition
%%      \setbox0=\hbox{$\sim$}%
%%      \raisebox{-.6ex}{\hspace*{-.05ex}\adjustbox{width=#3,height=\height}{\clipbox{0.75 0 0 0}{\usebox0}}}}{}
%%   }

\newcommand{\thide}[1]{\left \{ #1 \right \}}
%% \newcommand{\typing}[4]{\turnstile{s}{s}{#4}{#3}{n}#1:#2}
\newcommand{\typing}[4]{\sststile{#4}{#3}#1:#2}
\newcommand{\related}[4]{\sdtstile{#4}{#3}#1:#2}
\newcommand{\kinding}[2]{\turnstile{s}{s}{}{}{n}#1:#2}
\newcommand{\teq}[2]{#1\equiv#2}
\newcommand{\arrow}[4]{#1\xrightarrow[#3]{#2}#4}
\newcommand{\symlet}{\mathsf{let\;}}
\newcommand{\symin}{\mathsf{\;in\;}}
\newcommand{\symletrec}{\mathsf{letrec\;}}
\newcommand{\symand}{\mathsf{\;and\;}}
\newcommand{\symmatch}{\mathsf{match}}
\newcommand{\FV}{\mathsf{FV}}
\newcommand{\symwith}{\mathsf{\;with\;}}
\newcommand{\symreturn}{\mathsf{\;return\;}}
\newcommand{\syminl}{\mathsf{inl}}
\newcommand{\syminr}{\mathsf{inr}}
\newcommand{\symmax}{\mathsf{max}}
\newcommand{\symSinl}{\mathsf{Sinl\;}}
\newcommand{\symSinr}{\mathsf{Sinr\;}}
\newcommand{\symfold}{\mathsf{fold}}
\newcommand{\symSfold}{\mathsf{Sfold}}
\newcommand{\symunfold}{\mathsf{unfold}}
\newcommand{\symSunfold}{\mathsf{Sunfold\;}}
\newcommand{\symhide}{\mathsf{hide}}
\newcommand{\symShide}{\mathsf{Shide}}
\newcommand{\symunhide}{\mathsf{unhide}}
\newcommand{\symSunhide}{\mathsf{Sunhide\;}}
\newcommand{\leO}{\preceq}
\newcommand{\sympair}{\mathsf{pair}}
\newcommand{\symtt}{\mathsf{tt}}
\newcommand{\symunit}{\mathsf{unit}}
\newcommand{\symlist}{\mathsf{list}}
\newcommand{\symnil}{\mathsf{nil}}
\newcommand{\symcons}{\mathsf{cons}}
\newcommand{\symfix}{\mathsf{fix}}
\newcommand{\symbool}{\mathsf{bool}}
\newcommand{\symtrue}{\mathsf{true}}
\newcommand{\symfalse}{\mathsf{false}}
\newcommand{\symmerge}{\mathsf{merge}}
\newcommand{\relV}[1]{\mathcal{V}\lsem#1\rsem}
\newcommand{\relE}[1]{\mathcal{E}\lsem#1\rsem}
\newcommand{\relEC}[1]{\mathcal{EC}\lsem#1\rsem}
\newcommand{\later}{\triangleright}
\newcommand{\vtos}[1]{\lfloor #1 \rfloor}
\newcommand{\getcsize}{\mathsf{get\_csize}}
\newcommand{\symCStt}{\mathsf{CStt}}
\newcommand{\symCSpair}{\mathsf{CSpair}}
\newcommand{\symCSinl}{\mathsf{CSinl}}
\newcommand{\symCSinr}{\mathsf{CSinr}}
\newcommand{\symCSfold}{\mathsf{CSfold}}
\newcommand{\symCShide}{\mathsf{CShide}}
\newcommand{\symapp}{\mathsf{app}}
\newcommand{\symclp}{\mathsf{clp}}
\newcommand{\symnormal}{\mathsf{normal}}

%% \newcommand{\intro}[2]{#2^#1}
\newcommand{\intro}[2]{(#1 : #2)}
%% \newcommand{\intro}[2]{(#2 \mathsf{\;size\;} #1)}
%% \newcommand{\intro}[2]{\{#2 \mathsf{\;|\;} #1\}}

\newcommand{\symsum}{\mathsf{sum}}
\newcommand{\symfst}{\mathsf{fst}}
\newcommand{\symsnd}{\mathsf{snd}}
\newcommand{\symif}{\mathsf{if\;}}
\newcommand{\symthen}{\mathsf{\;then\;}}
\newcommand{\symelse}{\mathsf{\;else\;}}
\newcommand{\symSbool}{\mathsf{Sbool}}
\newcommand{\symuf}{\mathsf{uf}}
\newcommand{\symuh}{\mathsf{uh}}
\newcommand{\syml}{\mathsf{l}}
\newcommand{\symr}{\mathsf{r}}
\newcommand{\symf}{\mathsf{f}}
\newcommand{\syms}{\mathsf{s}}
\newcommand{\symmsort}{\mathsf{msort}}
\newcommand{\symSstat}{\mathsf{Sstat}}
\newcommand{\symsplit}{\mathsf{split}}
\newcommand{\symprod}{\mathsf{prod}}
\newcommand{\symStt}{\mathsf{Stt}}
\newcommand{\symSpair}{\mathsf{Spair}}
\newcommand{\symSlr}{\mathsf{Slr}}
\newcommand{\symwork}{\mathsf{w}}
\newcommand{\symspan}{\mathsf{s}}

%% \newcommand{\defeq}{\triangleq}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

%% \newcommand{\logo}{\lambda^{\forall,\omega,\mu}_\mathrm{c}}
\newcommand{\logo}{\lambda_\mathrm{c}}
\newcommand{\Sstats}[1]{\left \langle #1 \right \rangle}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newcommand{\optional}[1]{\lfloor #1 \rfloor}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

%% \titlebanner{banner above paper title}        % These are ignored unless
%% \preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Complexity Recursive Types}
%% \subtitle{Subtitle Text, if any}

\authorinfo{Peng Wang}
           {MIT CSAIL}
           {wangpeng@csail.mit.edu}

\maketitle

\begin{abstract}

%% This paper contributes the first time-complexity type system with recursive types. The type system lets functional programmers specify time-complexity bounds of functions, and the type system checks and guarantees such bounds. This language is a milestone towards the ``great synthesis'' of complexity analysis and programming languages, for it supports universal polymorphism, type-level operators and isomorphic recursive types, the combined power of which enables generic algebraic data types, the watershed of practical usefulness. Potential beneficiaries of our language includes everyday functional programmers and computer theoriests, using it both as a practical bug-preventing tool and as a formal logic to prove algorithm complexities. 

%% The soundness of the type system is proved using the Logical Step-indexed Logical Relation. We formalize our design and proof in the Coq proof assistant (in progress). Key technical contributions include:

%% \begin{itemize}
%% \item Recognizing the number of unfold-fold reductions as the only relevant metric in asymptotic complexity analysis.
%% \item Introduction of structured ``sizes'' of values in a functional language with algebraic data types.
%% \item Introduction of ``hide type''.
%% \item Introduction of structured ``widths'' as a generalization of the ``constant factor'' in asymptotic complexity analysis especially for higher-order functions.
%% \item Strengths and weaknesses of different language encoding schemes in Coq.
%% \end{itemize}


This paper introduces $\logo$, the first time-complexity type system with recursive types. $\logo$ lets functional programmers specify time-complexity bounds of functions, and the type system checks and guarantees such bounds. $\logo$ is a milestone towards the ``great synthesis'' of complexity analysis and programming languages, for it supports recursive types, universal polymorphism and type-level operators, the combined power of which enables generic algebraic data types, the watershed of practical usefulness. Potential beneficiaries of $\logo$ includes everyday functional programmers and computer theorists, using it both as a practical bug-preventing tool and as a formal logic to prove algorithm complexities.

Soundness of $\logo$ is proved using the Logical Step-indexed Logical Relation. We formalize our design and proof in the Coq proof assistant (in progress). Key technical contributions include:
\begin{itemize}
\item Realization that the number of unfold-fold reductions is the only metric needed in counting a program's asymptotic complexity.
\item Introduction of structured ``size'', a generalization of the ``input size'' in complexity analysis.
\item Introduction of ``hide type''.
\item Introduction of structured ``width'', a generalization of the ``constant factor'' in asymptotic complexity, especially for higher-order functions.
\item Strengths and weaknesses of different language encoding schemes in Coq.
\end{itemize}

\end{abstract}

%\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore, 
% you may leave them out
%% \terms
%% term1, term2

\keywords
type system; complexity; recursive types

\section{\label{intro}Introduction}

One major obstacle of the adaptation of functional programming is the lack of a clear intuition of functional program's time complexity. A C programmer can easily read off the big-O complexity of a for-loop, but even experienced functional programmers are sometimes muddled about the time complexity of a piece of functional code. The confusion comes from the lack of knowledge of the compiler's inner-working, as stated by the scoff ``you can hide an elephant under a beta reduction''. Two examples of the mythical objects that compilers manage internally are closures and algebraic data values. When one does not know how closures are created, passed, and used, he or she will rightfully wonder the time cost of creating a lambda abstraction, passing a function, and applying a function (beta reduction). For values of algebraic data types such as list, the immutability of these values (one can only construct new values, not in-place modify them) often gives a C programmer the wrong impression that such values are copied and passed-by-value all the time and that passing a list as argument will take O(n) time because of the copying. The reality could not be further from this false impression, since immutability means almost everything can be safely implemented by pointers and references.

But requiring every programmer to fully understand the inner-working of functional-language compilers is infeasible and unreasonable. One should be able to reason about programs in the language in which she actually writes, not in the target language of the compiler. It is the compiler's responsibility to preserve all properties of interest from the source program to the target program. Therefore, to demythify the complexity of function programming, two pieces of work need to be done: (1) we need a way to let the programmers easily reason about the complexity of functional programs in the source language level, and (2) we need a compiler that preserves the complexity of the source program downto the target program. This paper deals with the first part, with some assumptions about the second part. 

One usually reasons about functional programs using its operational semantics. The closest thing related to time cost in operational semantics is the number of reduction steps (in some counting metric) in small-step operational semantics, with some evaluation strategy. The design choices are (1) which evaluation strategy to use and (2) in which metric to do the counting. For (1), we choose strict call-by-value evaluation strategy against call-by-name strategy, because we think call-by-value fits people's intuition of evaluation order better, and it makes our assumption about the compiler more plausible. Call-by-name evaluation strategy has the drawback that since it can postpone computations and executing them in a sudden burst, a reduction step in the evaluation strategy could take an arbitrary amount of time on a real machine.

For design choice (2), we choose a very simple cost model - the number of reduction steps in a standard strict call-by-value evaluation strategy. There are work (e.g. \cite{blelloch2013cache}, \cite{blelloch1994}) that employ ingenious cost models in which the counting is done, and we do believe that well-designed cost models is needed to account for various machine characteristics when we want to have a more precise analysis of the time cost. The justification for the choice in this work is that (a) our top-level theorem will only give a big-O bound on the number of reduction steps, which has a constant factor as the room of imprecision, so more precise cost models do not help; and (b) we make the assumption that {\bf any reduction step in a standard strict call-by-value evaluation strategy can be implemented by the compiler as a constant-time computation (excluding garbage collection)}. When we later make clear our definition of ``constant'' and ``non-constant'', we will come back to this assumption and try to persuade the reader that this is a reasonable assumption. Combining (a) and (b), we will have a end-to-end theorem of the big-O bound on the actual running time (defined in a well-accepted machine model such as the Random Access Machine (RAM)), independent of our choice of cost model, which can be treated as a intermediate proof artifact.

Using such a cost model, programmers can do the counting in their heads and reason about their programs' complexity informally. However, we want to allow programmers to formally express their complexity constraints in a type system, which can formally guarantee the complexity via type-checking. Type systems are for preventing bugs, and we think that calling a function of a wrong complexity order, such as calling an O(n) function when an O(1) one is intended, is really a bug, not a ``performance problem''. Complexity, or at least complexity order, should be a constituent of functional correctness.

We designed such a type system, called $\logo$, which adds complexity annotations to a lambda calculus that includes recursive types ($\lambda^\mu$), (first-class) universal polymorphism ($\lambda^\forall$) and type-level operators ($\lambda^\omega$). This paper's main contribution is {\bf the first complexity type system with recursive types}. We believe this is an important milestone because the combined power of recursive types, universal polymorphism and type-level operators allows generic algebraic data types such as lists and trees, which is a minimal requirement for a functional language to become practically ``useful''.

This paper's technical contributions include:
\begin{itemize}
\item Recognizing the number of unfold-fold reductions as the only relevant metric in asymptotic complexity counting: when writing programs and giving complexity annotations, the programmer only needs to count the number of unfold-fold reductions it takes for the program to terminate, ignoring all other reductions. The type soundness theorem guarantees that the total number of all reductions is bounded by this unfold-fold count modulo a constant factor. 
\item Introduction of structured ``sizes'' of values in a functional language with algebraic data types: the input size of an algorithm, which was just a number in conventional algorithm complexity analysis, is generalized to a structured construct to accommodate for algorithms (especially when expressed as functional programs) that work on structured inputs.
\item Introduction of structured ``widths'' as a generalization of the constant factor in asymptotic complexity analysis especially for higher-order functions: during our proof of type soundness, we realized that the conventional definition of big-O complexity, saying that ``there exists a constant factor (independent of input size), so that the running time is bounded by the complexity times this factor'', does not work with higher-order functions. For a higher-order function, the ``constant factor'' of its function body actually depends on the ``constant factor'' of the particular input function. We generalized the ``constant factor'' into a structured construct we call ``width''.
\item Strengths and weaknesses of different language encoding schemes in Coq: in our attempt to formalize the design and proof of $\logo$, we tries several available encoding schemes, and summarized the strengths and weakness of them.
\end{itemize}

We imagine two groups of potential users for such a language and type system. The first group is everyday programmers, for whom this type system can be incorporated into the ML toolchain and provide additional benefits besides the already useful ML type system. To be user-friendly, we must have a type checker (which means the type system needs to be proved decidable) and reasonable degree of type inference. We will discuss this in Section \ref{section-discussion} as future work.

The second group of users are computer theorists conducting complexity proofs of algorithms. As the author in \cite{harper2014proposal} points out, there is an unfornate crevice between ``combinatorial'' theoriests and programming language theoriests. Members of the former still believe that the only mathematically rigid way to do complexity analysis is on the assembly level - on the Turning Machine model or Random Access Machine model; while the latter community has decades of achievements raising the abstraction level and building large systems \emph{compositionally} out of smaller components. The combinatorial theorists actually often give up rigidity by using some ``pseudo-code'' in the papers and assume that the readers can straight-forwardly ``compile'' the pseudo-code into rigid RAM code in their heads. We hope that our language can serve as the start point when computer theoriests can express their complexity proof in a rigid whilst high-level formal language, enjoying the benefits of modern functional languages such as compositionality and abstraction. 

Our language is only a start point because it still lacks such important features as mutable references (and mutable arrays), which is essential for implementing matrix-based numeric algorithms; and such advanced complexity analysis as probablistic and amortized analysis, which have become mainstream for modern complexity analysis. We also lack parallelism, though in our design we intentionlly maintain the generality (we have the notion of ``work'' and ``span'') to make later extension to parallelism easier. Another potentially useful feature is ``complexity polymorphism'', where the complexity of a higher-order function can depend on the complexity of the input function, which our language does not allow.

Section \ref{section-example} will give an illustrating example, the merge-sort, to show what the type system can express. It will also give the top-level type soundness theorem. Section \ref{section-lang} defines the language and the typing rules. Section \ref{section-proof} gives the proof of the type soundness, using Logical Step-indexed Logical Relation~\cite{dreyer2009logical}. Section \ref{section-related} discusses related work. Section \ref{section-discussion} discusses this work's strength and limitations, future work and conclusion.

\section{\label{section-example}Merge-Sort Example and Main Theorem}

As an example program of $\logo$, consider the merge-sort code and the types of relevant functions shown in Figure \ref{msort}. Function $\symmsort$ is generic for the element type of the input list and generic for the comparison function (but not its complexity). The type of $\symmsort$ requires that the comparison function $cmp$ should compare a pair of elements within 1 unit time (the meaning of ``1 unit'' will be explained later), and guarantees that $\symmsort$ will finish within $|s|*\log(|s|)$ units of time. As seen from the example, complexities are annotated on the arrows in the type, where above the arrow is the time complexity bound of the function, and below the arrow is the size bound of the this function's result value. In order to refer to the input size, each type on the left side of an arrow will introduce a \emph{size variable}, whose scope consists of the complexity expressions above and below the arrow, and the type on the right side of the arrow. Together with currying, the complexity expressions can refer to multiple arguments' sizes for n-arity functions, as shown in the type of the $\symmerge$ function. This notation may give a dependent-typed flavor, but the type system is not fully dependent-typed because return types can only depend on the sizes of arguments, not their values, and size variables will only be used on the arrows.

\begin{figure}
\begin{align*}
\symmsort &\defeq \lambda A. \lambda cmp.\;\symfix\;f(xs). \\
& \hspace{.1in} \symmatch\;xs\symwith \\
& \hspace{.2in} |\; \symnil\Rightarrow xs \\
& \hspace{.2in} |\; \_::xs' \Rightarrow \symmatch\;xs'\symwith \\
& \hspace{.3in} |\; \symnil\Rightarrow xs \\
& \hspace{.3in} |\; \_ \Rightarrow \symmatch\; \symsplit\;xs \symwith \\
& \hspace{.4in} |\; (ys, zs) \Rightarrow \symmerge\;cmp\;(f\;ys)\;(f\;zs) \\
& \hspace{-0.2in} : \forall A.\;\arrow{(\arrow{A}{}{}{\arrow{A}{1}{}{\symbool}})}{}{}{\arrow{\intro{s}{\symlist\;A}}{|s|*\log(|s|)}{\Sstats{s}}{\symlist\;A}} \\
\mathsf{where} & \\
\symsplit &: \forall A.\;\arrow{\intro{s}{\symlist\;A}}{|s|/2}{\Sstats{s}/2}{\symlist\;A\times\symlist\;A} \\
\symmerge &: \forall A.\;\arrow{(\arrow{A}{}{}{\arrow{A}{1}{}{\symbool}})}{}{}{\arrow{\intro{x}{\symlist\;A}}{}{}{\arrow{\intro{y}{\symlist\;A}}{|x|+|y|}{\Sstats{x}+\Sstats{y}}{\symlist\;A}}}
\end{align*}
\caption{\label{msort}Merge-sort}
\end{figure}

The ``size'' of an input value is not merely a natural number, as one might expect, but a tree-like structure, since we aim to deal with general algebraic data types such as trees. In order to use such a size in a time complexity expression, which is merely a (syntactic) number, we defined some measures on a size structure. Here the $|s|$ measure corresponds to the length of a list if $s$ is the size of a list value. The definition of $|-|$ will be given in Section \ref{section-lang}. In the size expression below the arrow, $\Sstats{s}$ stands for a size that has the same measures as size $s$, but not necessarily the same structure. In this example it can be understood as ``the size of a list whose length is the same as $s$'', which is a proper description of merge-sort's result.  The notations $\Sstats{s}/2$ and $\Sstats{x}+\Sstats{y}$ means ``a list of half length'' and ``a list of combined length'' respectively, whose precise definitions are given in Section \ref{section-lang}.

A unit of time in the complexity expression is defined as one unfold-fold reduction, and the complexity expression states the number of unfold-fold reductions. All other reductions are ignored by complexity expressions. The reason to ignore other reduction kinds is that (1) the actual number of all reductions depends on some details of the input value, which are not accounted for by its size structure in our definition; and our type soundness theorem (see below) will state that (2) the actual number of all reduction steps will be bounded by the number of unfold-fold reductions, with a constant factor. Claim (2) is true crucially because we do not have built-in fixpoint or recursive-let in our language, so the only source of unbounded execution is via some recursive-type encoding of the Y-combinator and alike, which all have the property that each recursive call will involve at least one unfold-fold reduction (for the encoding we use to define fixpoint, the number is exactly one). The unfold-fold count thus covers the number of recursive calls, which is essentially the only metric that changes with input size. For functional programmers who are more familiar with algebraic data types than recursive types, they can think of the count as ``the number of recursive calls plus the number of pattern-matchings on recursive data structures''. Counting this is actually easier than counting all reduction steps, since not all programmers are familiar with the whole set of evaluation rules. 

Now we give the top-level type soundness theorem of our type system. It has two parts: Theorem \ref{thm-safety} states the standard type safety, in terms of nonstuckness. Theorem \ref{thm-boundedness}, distinguishing for our type system, states the time-boundedness of any well-typed program in our language.

\begin{thm}[\label{thm-safety}Soundness w.r.t. Safety]
$$
\begin{array}{l}
\forall e\;\tau, \\
\hspace{.1in} \vdash e:\tau \Rightarrow \\
\hspace{.2in} \forall e',\; e \leadsto^* e' \Rightarrow e'\in\mathsf{Val} \;\vee\; \exists e'',\;e' \leadsto e''.
\end{array}
$$
\end{thm}

\begin{thm}[\label{thm-boundedness}Soundness w.r.t. Boundedness (for unary functions)]
$$
\begin{array}{l}
\forall f\;\tau_1\;\tau_2\;c\;s, \\
\hspace{0.1in} \vdash f:\arrow{(x:\tau_1)}{c}{s}{\tau_2} \wedge f\in\mathsf{Val}\Rightarrow \\
\hspace{0.2in} \exists B, \;\forall v,\; \vdash v:\tau_1 \Rightarrow \\
\hspace{0.3in} \forall e'\;n,\; f\;v\leadsto^n e' \Rightarrow n\leq B\times c[\vtos{v}/x]
\end{array}
$$
\end{thm}

Theorem \ref{thm-safety}, the standard nonstuckness, says that for any well-typed program (in an empty typing context), wherever the program runs to ($e'$), it is either successfully done (be a value) or can take one more step. That is, it won't get stuck in some erronous state where there is no legal step to progress.

Theorem \ref{thm-boundedness}, the time-boundedness, says that for any well-typed value $f$ of the arrow type $\arrow{(x:\tau_1)}{c}{s}{\tau_2}$, there exists some constant factor $B$, so that for any input value of the right type, the composed expression $f\;v$ won't run for more than $B\times c[\vtos{v}/x]$ steps ($\vtos{v}$ means the size of value $v$), where $c$ is the complexity guarantee given by the type. Combining it with Theorem \ref{thm-safety}, we can conclude that $f\;v$ will always successfully terminate within $B\times c[\vtos{v}/x]$ steps. Here a step $\leadsto$ means any reduction step in a standard strict call-by-value evaluation strategy (defined in Section \ref{section-lang}), not just the unfold-fold count used for complexity expressions in types. 

Note that the existentially quantified $B$ is outside hence independent of the input value, which meets the standard definition of asymptotic complexity. But this is only true for first-order functions, where type $\tau_1$ does not have any arrow. For higher-order functions, the theorem takes a slightly more complicated form, as will be explained in Section \ref{section-proof}.

Theorem \ref{thm-boundedness} is only a corollary from a more general result in the main proof. We can have similar corollaries for binary functions and beyond, such as Theorem \ref{thm-boundedness2}:

\begin{thm}[\label{thm-boundedness2}Soundness w.r.t. Boundedness (for binary functions)]
$$
\begin{array}{l}
\forall f\;\tau_1\;\tau_2\;\tau_3\;c\;s, \\
\hspace{0.1in} \vdash f:\arrow{(x_1:\tau_1)}{0}{0}{\arrow{(x_2:\tau_2)}{c}{s}{\tau_3}} \wedge f\in\mathsf{Val}\Rightarrow \\
\hspace{0.2in} \exists B, \;\forall v_1\;v_2,\; (\forall i,\;\vdash v_i:\tau_i) \Rightarrow \\
\hspace{0.3in} \forall e'\;n,\; f\;v_1\;v_2\leadsto^n e' \Rightarrow n\leq B\times c[\vtos{v_1}/x_1][\vtos{v_2}/x_2]
\end{array}
$$
\end{thm}

\section{\label{section-lang}Language and Type System}

\subsection{Syntax}

The syntax of $\logo$ is given in Figure \ref{syntax}. The call-by-value evaluation rules is give in Appendix \ref{append1}. 

\begin{figure}
$$\begin{array}{rrcl}
  \multicolumn{4}{l}{\textrm{Unnormalized Types}} \\
  & \pi &::=& \symunit \mid \pi\times\pi \mid \pi+\pi \mid \alpha \mid \arrow{\intro{x}{\pi}}{c}{s}{\pi} \\
  & & & \mid \forall^c_s \alpha.\pi \mid \mu \alpha.\pi \mid \thide\pi \mid \Lambda \alpha.\pi \mid \pi\;\pi \\
  \multicolumn{4}{l}{\textrm{(Normalized) Types}} \\
  & \tau &::=& \symunit \mid \tau\times\tau \mid \tau+\tau \mid \alpha \mid \arrow{\intro{x}{\tau}}{c}{s}{\tau} \\
  & & & \mid \forall^c_s \alpha.\tau \mid \mu \alpha.\tau \mid \thide\tau \\
  \multicolumn{4}{l}{\textrm{Expressions}} \\
  & e &::=& \symtt \mid \sympair\;e\;e \mid \syminl_\pi\;e \mid \syminr_\pi\;e \mid x \mid e\;e \mid \lambda x:\pi.\;e \\
  & & & \mid \symlet x := e \symin e \mid e\;\pi \mid \lambda \alpha.e \mid \symfold_\pi\;e \\
  & & & \mid \symunfold\;\;e \mid \symhide\;\;e \mid \symunhide\;\;e \mid \symfst\;e \mid \symsnd\;e \\
  & & & \mid (\symmatch\;e \symreturn \pi \symand s \symwith \syminl\;x\Rightarrow e\;|\;\syminr\;x\Rightarrow e) \\
  \multicolumn{4}{l}{\textrm{Concrete Sizes}} \\
  & \xi &::=& \symCStt \mid \symCSpair\;\xi\;\xi \mid \symCSinl\;\xi \mid \symCSinl\;\xi \\
  & & & \mid \symCSfold\;\xi \mid \symCShide\;\xi \\
  \multicolumn{4}{l}{\textrm{Size Specs}} \\
  & s &::=& \Sstats{c_\symwork,c_\symspan} \mid x.\vec{i} \mid \symSpair\;s\;s \\
  & & & \mid \symSlr\;s\;s \mid \symSfold\;s \mid \symShide\;s \\
  & & & \mid \symSpair\;s\;s \mid \symSfold\;s \mid \symShide\;s \\
  \multicolumn{4}{l}{\textrm{Size Subpart Idx}} \\
  & i &::=& \symf \textrm{(fst)} \mid \syms \textrm{(snd)} \mid \syml \textrm{(left)} \mid \symr \textrm{(right)} \mid \symuf \textrm{(unfold)} \\
  & & & \mid \symuh \textrm{(unhide)} \\
  \multicolumn{4}{l}{\textrm{Complexity Expr}} \\
  & c &::=& x.\vec{i}.\symwork \textrm{(work)} \mid x.\vec{i}.\symspan \textrm{(span)} \mid 0 \mid 1 \mid c+c \mid \cdots \\
\end{array}$$
\caption{\label{syntax}Syntax}
\end{figure}

In order to support generic algebraic data types, $\logo$ has type-level operations, enabled by type-level abstraction $\Lambda\alpha.\pi$ and type-level application $\pi\;\pi$. We employ a 2-stage system to support type-level operations. Types that can contain $\Lambda\alpha.\pi$ and $\pi\;\pi$ are called ``unnormalized types'' (denoted by $\pi$). They have a simple kinding system, shown in Figure \ref{kinding} (in Appendix). Because of the normalizability of simply typed lambda calculus and the fact that we are using iso-recursive types (rather than equi-recursive types), all well-kinded types can be normalized to ``normalized types'' (denoted by $\tau$), where there is no $\Lambda\alpha.\tau$ or $\tau\;\tau$. We assume that all the types appearing in a programs will be kind-checked and normalized before typing checking. So from now on we ignore unnormalized types and assume that all types are normalized.

Most of the syntax are standard, except the complexities, sizes and the ``hide'' type $\thide{\tau}$. We put type annotations in lambda, $\syminl$, $\syminr$ and fold to make type unique (modulo complexity annotations), and elide them when irrelevant. We also borrow the return clause of pattern-matching from Coq to guide the type checker to find the common type and size of all branches. 

Now let us turn to the complexity and size part. Firstly, complexity and size annotations appear only on arrows and in $\forall$ binders (they are the same in a dependent type system). Each arrow type $\arrow{\intro{x}{\tau_1}}{c}{s}{\tau_2}$ introduces a size variable standing for the input size, which can be referred to in $c$, $s$ and $\tau_2$ (but not $\tau_1$). The $\forall$ binder does not introduce new size variables. The reason for $\forall$ to also have complexity annotation is that, just like $\lambda x.e$, $\lambda \alpha.e$ is a ``delayed'' computation, meaning that it is itself a value but when been applied, its inner part may take some steps to reach another value. But in most cases, the $c$ and $s$ in $\forall^c_s$ are just 0. When $c$ or $s$ is 0, we elide it in this paper.

We first explain the definition of sizes. There are two notions of ``size'' in this paper: ``concrete size'' and ``size spec''. A ``concrete size'' is the shape ``skeleton'' of a value; a ``size spec'' is a syntactic construct and a closed size spec can be interpretted into a set of concrete sizes (the interpretation is defined in Figure \ref{cexpr-size-aux}). Concrete sizes only appear in the type soundness theorem and proof, so it is not a part of program syntax (we list it here only to make a closer comparison with size specs). The size annotations underneath the arrow are size specs. We use ``size'' for either of them when context makes it clear.

The concrete size of a value is a tree-like structure, closely mirroring the structure of a value. Comparing ``Concrete Sizes'' and ``Values'' in Figure \ref{syntax} shows that they are almost isomorphic except that concrete sizes do not have variables (because they will only be used for closed values) and $\lambda$-abstractions. There is a function $\getcsize$ defined in Figure \ref{cexpr-size-aux} for getting the concrete size of a value, which is straight-forward.

Size specs are predicates on concrete types. In principle they can be as simple as numbers, specifying the maximal number of nodes in the concrete types. In our design of size specs, we want to make them expressive enough to (1) constrain some measure of the concrete size (such as the number of nodes as above); (2) constrain the shape of the concrete size (such as ``the concrete size must be a pair whose first and second part satisfy these two constraints respectively''); (3) be able to mention size variables in scope.

For (1), we have the constructor $\Sstats{c_\symwork,c_\symspan}$. It constructs a size spec that constrains two measures of concrete sizes. The two measures are called ``work'' and ``span'', borrowed from the parallel computation literature \cite{parallelbook}. ``Work'' means the number of ``$\symSfold$'' nodes in the size; ``span'' means the height of the size seen as a tree, only considering ``$\symSfold$'' nodes. ``Span'' seems useless in a non-parallel setting, but remember we can define trees, about which ``span'' is meaningful even in a non-parallel setting, for algorithms whose complexities depend on the height of the input tree. 

The intuition for only counting the  ``$\symSfold$'' nodes in concrete sizes is similar to the intuition of only counting unfold-fold reductions when analysing time complexity. The $\symSfold$-skeleton is what is really ``variable'' in a concrete size, and when we say ``how computation grows with the input size'' we actually mean ``how computation grows with the input size's $\symSfold$-skeleton''.

For (2), we have structural constructors similar to concrete sizes. We have $\symSlr$ instead of $\symSinl$ and $\symSinr$. A requirement of ``only $\syminl$'' will be written as $\symSlr\;s\;\Sstats{0,0}$.

For (3), we have the constructor $x.\vec{i}$. $x.\vec{i}$ not only mentions a variable $x$, but also a ``subpart query path'' $\vec{i}$, which is a list of ``subpart query indices''. The subpart query path is used to refer to a subpart of a size variable. One can go as deep as s/he likes into a size variable. This seems alarming since their is no guarantee that the future substitute for the variable have that desired subpart. The definition of substitution (in Figure \ref{subst}) dictates that when the wanted subpart is not present, the substitution result is $\Sstats{0,0}$. This substitution policy is OK because our type soundness proof only uses three axioms about size specs, shown in Figure \ref{axioms}, so any design of size spec substitution and size spec ordering satisfying these axioms will give us a sound type system. Our definition of size spec substitution and ordering satisfies them. 

For brevity we write ``$\Sstats{s}$'' for $\Sstats{s.\symwork,s.\symspan}$, ``0'' for $\Sstats{0,0}$, ``$\Sstats{c_1,c_2}/2$'' for $\Sstats{c_1/2,c_2/2}$ and similarly lift other operations on measures to sizes. We also overload the notation ``$(a,b)$'' to stands for $(\sympair\;a\;b)$, $(\symSpair\;a\;b)$ and other pair-like constructions.

Now we explain the time complexity expression. Ideally time complexities are just numbers. But since they should be able to mention size variables in scope, we define complexities as syntactic expressions. A closed complexity expression can be interpreted into a natural number (the interpretation is defined in Figure \ref{interpret-cexpr-size}). When referring to a variable, complexity expressions must specify which measure (work or span) they want, since complexities are numbers not trees. We write ``$|x|$'' for ``$x.\symwork$'' for intuitiveness. Like size specs, a complexity expression can refer to a subpart of a size variable. When doing substitution and the wanted subpart is not present, the result of $x.\vec{i}.\symwork$ (or $x.\vec{i}.\symspan$) will be 0 (a consequence of $x.\vec{i}$ being $\Sstats{0,0}$). The axioms about complexity expressions used by our type soundness proof are listed in Figure \ref{axioms}.

The axiomization of size spec and complexity expression in our type soundness proof makes the design choices of size specs and complexity expression an orthogonal issue of the type system design, and they do not affect type soundness when the axioms are fulfilled. However, these choices greatly influence expressiveness, precision and type checking/inference difficulty, so we will discuss this orthogonal but important issue in Section \ref{section-discussion}. 

We introduced a new ``hide'' type constructor denoted as $\thide{\tau}$, its corresponding terms $(\symhide\; e)$ and $(\symunhide\; e)$, and its corresponding size constructors $(\symShide\;s)$ and $(\symCShide\;s)$. The motivation for this new type constructor is illustrated by this example: 
$$f:\forall A.\;\arrow{\intro{x}{\symlist\;A}}{|x|}{}{\symlist\;A}.$$
When one first looks at this function one may think its complexity is linear with the input list's length. But when one instantiates it with a type, such as $(\symlist\;\symbool)$, $f$ suddenly becomes 
$$f\;(\symlist\;\symbool):\arrow{\intro{x}{\symlist\;(\symlist\;\symbool)}}{|x|}{}{\symlist\;(\symlist\;\symbool)}.$$
Its complexity is no longer linear with the outer list's length! The problem is that there is not a way for a polymorphic function to express the notion of ``ignoring values of the parameter type'' in its complexity specification. Now that we have ``hide'' types and define list as 
$$\symlist \defeq \Lambda A.\;\mu \alpha.\;\symunit + \thide{A} \times \alpha,$$ 
and the measure calculation ignores everything under a $\symShide$ node, both $f$'s and $(f\;(\symlist\;\symbool))$'s complexity specification mean the length of the input list.

In languages like ML, $\mu\alpha.\tau$, $\symfold$ and $\symunfold$ are hidden in algebraic data type definitions and pattern-matchings. The same can be done for $\thide{\tau}$, $\symhide$ and $\symunhide$.

\subsection{Typing Rules}

Figure \ref{typing} gives representative typing rules. Typing judgments have the form $\Gamma\typing{e}{\tau}{c}{s}$, where $c$ stands for the maximal time units (number of unfold-fold reductions) $e$ takes to execute to a value. $s$ stands for the size bound of the final value. The typing rules formalize our intuition of how to count the unfold-fold reductions. In the typing context $\Gamma$ (defined in Figure \ref{syntax}), we reuse term variables $x$ to mean both a value and its size (conflating term variable and size variable), so the typing context becomes dependent (entries in it can depend on entries before them), and type variables become interleaved with term variables. 

\begin{figure}
\fbox{$\Gamma\typing{e}{\tau}{c}{s}$}
\quad$\Gamma ::= \cdot \mid \Gamma,\alpha \mid \Gamma, x:\tau$ (Typing Context)
{\small
\begin{mathpar}

\inferrule* [Right=Var] 
{\Gamma(x)=\tau} 
{\Gamma\typing{x}{\tau}{0}{x}} 

\inferrule* [Right=App] 
{\Gamma\typing{e_1}{\arrow{\intro{x}{\tau_1}}{c}{s}{\tau_2}}{c_1}{s_1} \\ \Gamma\typing{e_2}{\tau_1}{c_2}{s_2} }
{\Gamma\typing{e_1\;e_2}{\tau_2}{c_1+c_2+c[s_2/x]}{s[s_2/x]}} 

\inferrule* [Right=Abs] 
{\Gamma, x:\tau_1\typing{e}{\tau_2}{c}{s} }
{\Gamma\typing{\lambda x : \tau_1.\;e}{\arrow{\intro{x}{\tau_1}}{c}{s}{\tau_2}}{0}{0}} 

\\

\inferrule* [Right=Tapp]
{\Gamma\typing{e}{\forall^c_s \alpha.\tau}{c'}{1}}
{\Gamma\typing{e\;\tau_2}{\tau[\tau_2/\alpha]}{c'+c}{s}}

\inferrule* [Right=Tabs]
{\Gamma, \alpha\typing{e}{\tau}{c}{s}}
{\Gamma\typing{\lambda \alpha.\;e}{\forall \alpha^c_s.\tau}{0}{0}} 

\\

\inferrule* [Right=Fold]
{\tau \equiv \mu \alpha. \tau_1 \\ \Gamma\typing{e}{\tau_1[\tau/\alpha]}{c}{s}}
{\Gamma\typing{\symfold\; \tau \; e}{\tau}{c}{\symSfold s}}

\inferrule* [Right=Unfold]
{\Gamma\typing{e}{t}{c}{s} \\ \mathsf{IsFold}(s)=s_1 \\ \tau \equiv \mu \alpha.\tau_1}
{\Gamma\typing{\symunfold\;\;e}{\tau_1[\tau/\alpha]}{1+c}{s_1}}

\inferrule* [Right=Hide]
{\Gamma\typing{e}{\tau}{c}{s}}
{\Gamma\typing{\symhide\; e}{\thide{\tau}}{c}{\symShide s}} 

\inferrule* [Right=Unhide]
{\Gamma\typing{e}{\thide{\tau}}{c}{s} \\ \mathsf{IsHide}(s)=s_1}
{\Gamma\typing{\symunhide\; e}{\tau}{c}{s_1}}

\inferrule* [Right=Le]
{\Gamma\typing{e}{\tau}{c}{s} \\ c \leq c' \\ s \leq s'}
{\Gamma\typing{e}{\tau}{c'}{s'}} \\

\inferrule* [Right=Pair]
{\Gamma\typing{e_1}{\tau_1}{c_1}{s_1} \\ \Gamma\typing{e_2}{\tau_2}{c_2}{s_2}}
{\Gamma\typing{(e_1,e_2)}{\tau_1\times\tau_2}{c_1+c_2}{(s_1,s_2)}} 

\\

\inferrule* [Right=Fst]
{\Gamma\typing{e}{\tau_1\times\tau_2}{c}{s} \\ \mathsf{IsPair}(s)=(s_1, s_2)}
{\Gamma\typing{\symfst\;e}{\tau_1}{c}{s_1}} 

\inferrule* [Right=Snd]
{\Gamma\typing{e}{\tau_1\times\tau_2}{c}{s} \\ \mathsf{IsPair}(s)=(s_1, s_2)}
{\Gamma\typing{\symsnd\;e}{\tau_2}{c}{s_2}} 

\\

\inferrule* [Right=Inl]
{\Gamma\typing{e}{\tau}{c}{s}}
{\Gamma\typing{\syminl_{\tau'}\;e}{\tau+\tau'}{c}{\symSlr\;s\;0}} 

\inferrule* [Right=Match]
{\Gamma\typing{e}{\tau_1+\tau_2}{c}{s'} \\ \mathsf{IsSum}(s')=(s_1,s_2) \\ \forall i,\;\Gamma, x:(\tau_i,s_i)\typing{e_i}{\tau}{c_i}{s} \\ x\not\in\FV(\tau)\cup\FV(s)}
{\Gamma\typing{\symmatch\; e \symreturn \tau \symand s \symwith \syminl\;x \Rightarrow e_1 \;|\; \syminr\;x \Rightarrow e_2}{\tau}{c+\symmax(c_1[s_1/x],c_2[s_2/x])}{s}} 

\\

\end{mathpar}
}
\caption{\label{typing}Typing rules}
\end{figure}

The most essential rules are {\sc App} and {\sc Abs}, which give the meaning of the annotations on the arrow. Rule {\sc App} postulates that the time to evaluate $e_1\;e_2$ is the time to evaluate $e_1$ (i.e. $c_1$) plus that for $e_2$ (i.e. $c_2$), plus the time to evaluate the function body (i.e. $c[s_2/x]$). The final size is given by $s[s_2/x]$. Rule $Abs$ introduces arrows and its annotations. Since $\lambda$-abstract is already a value, and its size is 0 (see definition of $\getcsize$ in Figure \ref{cexpr-size-aux}), we have $\turnstile{s}{s}{0}{0}{n}$.

Rule {\sc Tapp} and {\sc Tabs} are similar to {\sc App} and {\sc Abs} respectively, since $\forall$ is just another kind of ``arrow''. Rule {\sc Unfold} is the only place where time complexity is actually increased, corresponding to a unfold-fold reduction. The partial functions $\mathsf{IsFold}$, $\mathsf{IsHide}$, $\mathsf{IsPair}$ and $\mathsf{IsSum}$ check that the size has the right structure, and destruct it into subparts. As an example, $\mathsf{IsPair}$ is defined as

$$
\mathsf{IsPair}(s)\defeq\begin{cases}
(s_1, s_2) & \mbox{if } s = \symSpair\;s_1\;s_2 \\
(x.\vec{i}.\symf, x.\vec{i}.\syms) & \mbox{if } s = x.\vec{i} \\
\bot & \mbox{otherwise}\\
\end{cases}
$$

As this example shows, $\mathsf{IsPair}(s)=(s_1,s_2)$ is almost the same as the equition $s = \symSpair\;s_1\;s_2$. The purpose for $\mathsf{IsPair}$ is that when $s$ is a variable (with subpart query path), $\mathsf{IsPair}$ knows that the destruction of it is two new variables with the proper subpart query indices appended.

Rule {\sc LE} relaxes and time and size bounds. The definitions of $c\leq c'$ and $s\leq s'$, given in Figure \ref{cle} and Figure \ref{sle} respectively, satisfy the axioms listed in Figure \ref{axioms}. They are one example among many legitimate choices. (ToDo: talk about exp, log and other real-number expressions).

\begin{figure}
(ToDo: fill in)
\caption{\label{axioms}Axioms about complexity and size}
\end{figure}

\begin{figure}
(ToDo: fill in)
\caption{\label{cexpr-size-aux}Auxiliary definitions for complexity and size}
\end{figure}

\begin{figure}
\fbox{$c_1\leq c_2$}
\begin{mathpar}
\end{mathpar}
(ToDo: fill in)
\caption{\label{cle}Complexity Order}
\end{figure}

\begin{figure}
\fbox{$s_1\leq s_2$}
\begin{mathpar}
\end{mathpar}
(ToDo: fill in)
\caption{\label{sle}Size Order}
\end{figure}


\section{\label{section-proof}Type Soundness Proof}

Since $\logo$ is strictly more restrictive than System F, Thoerem \ref{thm-safety} is straight-forward to prove by showing that any term well-typed in $\logo$ is well-typed in System F (with the complexity annotations stripped), so we will focus on the proof of Theorem \ref{thm-boundedness} (in its more general form). The proof is done by using Logical Step-indexed Logical Relation~\cite{dreyer2009logical}.

\subsection {Logical Step-indexed Logical Relation}

Logical relations (or logical predicates for unary relations as in this case) are essentially interpretations of types. By interpreting a type into the set of ``good expressions'' that have the right type and property, logical relations can be used to prove that any well-typed expression has the wanted property. Logical relations are usually defined as two mutually recursive functions: $\relV{\tau}$ and $\relE{\tau}$. The former can be thought of as the set of ``good values'' of type $\tau$, while the later the set of ``good expressions''. $\relV{\tau}$ is usually defined by recursion on $\tau$.

Recursive types cause trouble for defining logical relations because in the $\mu\alpha.\tau'$ case the unfolded type $\tau'[\mu\alpha.\tau'/\alpha]$ is not strictly smaller than $\mu\alpha.\tau'$, which prevents $\relV{\tau}$ from being defined recursively on $\tau$ (it cannot be defined inductively as a proposition either since in the arrow type case there is a non-positive appearance of $\relV{-}$). One solution is to define $\relV{\tau}$ recursively on another decreasing parameter called ``step index'' \cite{ahmed2006step}. The drawback of that solution is that the all the definitions will be polluted by this step index and there will be many detailed natural number operations ($+1$, $-1$, $<$) that are not relevant of the main definition structure. 

Here we employ another solution: instead of defining the logical relation in a classic logic, we define it in a special logic introduced in \cite{dreyer2009logical} called Logical Step-indexed Logical Relation (LSLR). The crucial novel logical connectives of this logic are the recursive binder $\mu$ and the later operator $\later$. A syntactic rule says that any appearance of variables introduced by $\mu$ must be under $\later$. With this syntactic constraint, this logic can be given a Kripke model that corresponds to step-indices in \cite{ahmed2006step}. 

\begin{figure}
  $$\begin{array}{rrcl}
    & R^m &::=& r \mid x \mid \lceil P \rceil \mid R^0 \wedge R^0 \mid R^0 \vee R^0 \mid R^0 \Rightarrow R^0 \mid \forall x,R^0 \\ 
    & & & \mid \exists x,R^0 \mid \forall r^0,R^0 \mid \exists r^0,R^0 \mid \lambda x:D.R^n \mid x\in R^{n+1} \\
    & & & \mid \mu r^n. R^n \mid \later R^0
  \end{array}$$
  \caption{\label{lslr}Logical Step-indexed Logical Relation}
\end{figure}

The syntax of LSLR is summarized in Figure \ref{lslr}. It is a second-order logic, so we distinguish between first-order variable $x$ (which can range over any domain other than $R$) and second-order variable $r$ (which impredicatively ranges over $R$). The universal and existential quantifiers on first-order and second-order variables are different, but we use the $\forall$ and $\exists$ notations for both of them when context can tell which is intended. $\lceil P \rceil$ lifts any proposition $P$ in the ambient meta-logic (CiC of Coq in our case) to LSLR. Each LSLR formula $R$ is also associated with an arity, shown as superscripts. For example, $R$ must have arity 0 in $R \wedge R$ and $\later R$, and $\lambda x.R$ and $x\in R$ increases and decreases the arity respectively. $D$ is a parameter of LSLR which determines how LSLR formulas are interpreted. For example, if $D$ is chosen to be expressions, then LSLR formulas with arity 1 are interpreted as sets of expressions. 

A LSLR validity judgment takes the form
$$\mathcal{X};\mathcal{R};\mathcal{P}\vdash R^0$$
where $\mathcal{X}$ is the context (a list) of first-order variables, $\mathcal{R}$ is the context of second-order variables, and $\mathcal{P}$ is a list of LSLR propositions (arity 0) well-formed in context, serving as premises. $R$ must be a well-formed LSLR proposition of arity 0. Readers are referred to \cite{dreyer2009logical} for the rules for establishing validity.

\subsection {Width}

The logical relation we designed for proving $\logo$'s type soundness relies on a concept called ``width''. ``Width'' is a generalization of the ``constant factor'' in big-O asymptotic complexity, such as the $B$ in Theorem \ref{thm-boundedness}. In our setting it is equivalent to the maximal number of steps allowed bewteen two consecutive unfold-fold reductions. The definition of big-O notation in conventional complexity analysis postulates that this number be existential quantified at outermost and is independent of the program's input size. But this intuition breaks down when the program is a higher-order function that takes another function as input. 

For example, in Theorem \ref{thm-boundedness}, when $\tau_1$ has arrows in it, the ``width'' of function $f$ (i.e. $B$) actually dependents on the width of the input argument $v$. Suppose $\tau_1=\arrow{\symunit}{1}{0}{\symunit}$, $v$ could be any of the following values:
$$
\begin{array}{l}
  \lambda x.\symtt \\
  \lambda x.\symlet y=\symtt\symin\symtt \\
  \lambda x.\symlet y=\symtt\symin\symlet y=\symtt\symin\symtt \\
  \lambda x.\symlet y=\symtt\symin\symlet y=\symtt\symin\symlet y=\symtt\symin\symtt \\
  \cdots
\end{array}
$$
Suppose $f=\lambda v.\; v\;\symtt$, then the width of $f$ depends on the width of $v$. If we insist that this ``constant factor'' (width) be existential quantified at outermost, then by skolemization, $B$ in Theorem \ref{thm-boundedness} should be a function from width to width, instead of just a number. 

Not only could $B$ be a function from width to width. If $\tau_1$ is more complex with arrows buried deeper inside, $B$ could be ``function from width to function from with to width'', ``pair of function from width to width'', etc. In the end, the structure of widths becomes almost isomorphic to the structure of expressions, and has constructors such as lambda, pair and fold. Intuitively a width is still a number, but it is a syntactic number which must be evaluated first to produce a number.

The definition of widths is give by Figure \ref{widths}. Since they are almost isomorphic to expressions, we use the same constructor notations ($\sympair$, $\syminl$, etc.) for both of them.

\begin{figure}
$$\begin{array}{rrcl}
  \textrm{Widths} & B &::=& \mathsf{const}\;n \mid \symapp_1\;w\;w \mid B\;+\;B \mid B\;*\;B \mid \symmax\;B\;B \\
  & w &::=& x \mid \symtt \mid \sympair\;w\;w \mid \syminl\;w \mid \syminr\;w \mid x \mid \symapp_2\;w\;w \\
  & & & \mid \lambda x.(B,w) \mid w\;\tau \mid \lambda.(B,w) \mid \symfold\; w \\
\end{array}$$
\caption{\label{widths}Widths}
\end{figure}

There are two syntactic classes for widths. Syntactic class $B$ is a symbolic representation of numbers, meaning that it can have abstractions and applications in it, but it should evaluates to a number. Syntactic class $w$ stands for widths that do not evaulate to numbers, but to functions on widths, pairs of widths, etc. 

The most interesting width constructor is $\lambda$-abstraction. An $\lambda$-abstraction of width packages two pieces of information: $B$, the width of the function body (the maximal number of reductions allowed between two consecutive unfold-fold reductions when evaluating the function body); and $w$, the width structure of the result value. Since the $\lambda$-abstraction has two pieces of information, we need two kinds of applications to get the information out: $(\symapp_1\;w\;w)$ and $(\symapp_2\;w\;w)$. We will write $w(w)_1$ and $w(w)_2$ for $(\symapp_1\;w\;w)$ and $(\symapp_2\;w\;w)$. The variable-less abstraction $\lambda.(B,w)$ corresponds to $\lambda\alpha.e$. 

Widths do not have eliminators (such as $\symfst$, $\symunfold$ and $\symmatch$) other than applications, because in our type soundness proof we ended up not needing those eliminators (one way to think about this is that all widths are values modulo beta-reduction). Definitions of values and evaluation rules for widths (in Appendix) are similar to those for expressions. 

Taking widths into consideration, Theorem \ref{thm-boundedness} becomes
\begin{thm}[\label{thm-boundedness'}Soundness w.r.t. Boundedness (for unary functions)]
$$
\begin{array}{l}
\forall f\;\tau_1\;\tau_2\;c\;s, \\
\hspace{0.1in} \vdash f:\arrow{(x:\tau_1)}{c}{s}{\tau_2} \wedge f\in\mathsf{Val}\Rightarrow \\
\hspace{0.2in} \exists B, \;\forall v\;w_1,\; \vdash v:\tau_1 \wedge v\textrm{\ has\ width\ } w_1 \textrm{\ of\ type\ }\tau_1 \Rightarrow \\
\hspace{0.3in} \forall e'\;n,\; f\;v\leadsto^n e' \Rightarrow n\leq B(w_1)\times c[\vtos{v}/x]
\end{array}
$$
\end{thm}
where the ``$v$ has width $w_1$ of type $\tau_1$'' part needs to be made formal (in Section \ref{subsection-proof}). 

One design goal is that when $\tau_1$ does not contain arrows, Theorem \ref{thm-boundedness'} should degenerates into Theorem \ref{thm-boundedness} - the more familiar form in conventional complexity analysis literature. That means when $\tau_1$ does not contain arrows, a width of type $\tau_1$ should have zero information (such as always be $\symtt$). This is achieved by the design of the typing rules for widths, shown in Figure \ref{wtyping}.

\begin{figure}
\fbox{$\Gamma\vdash B$ and $\Gamma\vdash w:\tau$}
{\small
\begin{mathpar}

\inferrule* [Right=WVar] 
{\Gamma(x)=\tau} 
{\Gamma\vdash x:\tau} 

\inferrule* [Right=WApp1] 
{\Gamma\vdash w_1:\tau_1\to\tau_2 \\ \Gamma\vdash w_2:\tau_1 }
{\Gamma\vdash \symapp_1\;w_1\;w_2 } 

\inferrule* [Right=WApp2] 
{\Gamma\vdash w_1:\tau_1\to\tau_2 \\ \Gamma\vdash w_2:\tau_1 }
{\Gamma\vdash \symapp_2\;w_1\;w_2 : \tau_2} 

\inferrule* [Right=WAbs] 
{\Gamma, x:\tau_1 \vdash B \\ \Gamma, x:\tau_1 \vdash w:\tau_2 }
{\Gamma\vdash \lambda x.(B,w) : \tau_1\to\tau_2} 

\inferrule* [Right=WPair]
{\Gamma\vdash w_1 : \tau_1 \\ \Gamma\vdash w_2 : \tau_2 \\ \symclp(\tau_1\times\tau_2)\equal\symfalse}
{\Gamma\vdash (w_1,w_2) : \tau_1\times\tau_2} 

\inferrule* [Right=WPair']
{\symclp(\tau_1\times\tau_2)\equal\symtrue}
{\Gamma\vdash \symtt : \tau_1\times\tau_2} 

%% \inferrule* [Right=WFold]
%% {\Gamma\vdash w : \tau[\mu\alpha.\tau/\alpha] \\ \symclp(\tau_1\times\tau_2)\equal\symfalse}
%% {\Gamma\vdash \symfold\;w : \mu\alpha.\tau} 

\end{mathpar}
}
\caption{\label{wtyping}Typing rules for widths (seleted)}
\end{figure}

The most interesting rules in Figure \ref{wtyping} are {\sc WPair} and {\sc WPair'} (sum and recursive types have similar two-case rules). They split the typing for product type into two cases, depending on the test result of $\symclp$. This $\symclp$ test, meaning ``collapses'', is true if the type contains no arrow (and hence can ``collapse'' into $\symunit$). Its definition is given in Figure \ref{width-aux}. 

\begin{figure}
$$\begin{array}{rcl}
\symclp(\tau_1\times\tau_2) &=& \symclp(\tau_1)\wedge\symclp(\tau_2) \\
\symclp(\tau_1+\tau_2) &=& \symclp(\tau_1)\wedge\symclp(\tau_2) \\
\symclp(\mu\alpha.\tau) &=& \symclp(\tau[\symunit/\alpha]) \\
\symclp(\{\tau\}) &=& \symclp(\tau) \\
\symclp(\symunit) &=& \symtrue \\
\symclp(\tau) &=& \symfalse \\
\end{array}$$
\caption{\label{width-aux}Auxiliary definitions for widths}
\end{figure}

\subsection {\label{subsection-proof}Logical Relation for $\logo$}

The logical relation definitions for the proof of $\logo$'s type soundness are given in Figure \ref{lrel}. All the formulas on the right hand side are LRSR formulas. The most salient feature of our logical relation design is that the target domain of interpretation ($D$ in Figure \ref{lslr}) is not sets of expressions, but sets of pairs of expression and ``width'' (denoted by $w$). 

\begin{figure}
\fbox{$\relV{\tau}$} and \fbox{$\relE{\tau}$}
{\small
%% $$
%% \begin{array}{ll}
\begin{align*}
  \relV{\alpha}_\rho &\defeq \rho(\alpha) \\
  \relV{\symunit}_\rho &\defeq \lambda(v,w).\; v \downarrow \rho(\tau) \wedge w \downarrow \rho(\tau) \\
  \relV{\tau_1\times\tau_2}_\rho &\defeq \lambda(v,w).\; v \downarrow \rho(\tau) \wedge w \downarrow \rho(\tau) \wedge \exists v_1\;v_2\;w_1\;w_2, \\
  & \hspace{.1in} v=(v_1,v_2) \wedge w=(w_1,w_2) \wedge (v_1,w_1) \in \relV{\tau_1}_\rho \wedge \\
  & \hspace{.1in} (v_2,w_2) \in \relV{\tau_2}_\rho \\
  \relV{\tau_1+\tau_2}_\rho &\defeq \lambda(v,w).\; v \downarrow \rho(\tau) \wedge w \downarrow \rho(\tau) \wedge \exists v'\;w', \\
  & \hspace{.1in} v=\syminl\;v' \wedge w=\syminl\;w' \wedge (v',w') \in \relV{\tau_1}_\rho \vee \\
  & \hspace{.1in} v=\syminr\;v' \wedge w=\syminr\;w' \wedge (v',w') \in \relV{\tau_2}_\rho \\
  \relV{\arrow{\tau_1}{c}{s}{\tau_2}}_\rho &\defeq \lambda (v,w).\;v \downarrow \rho(\tau) \wedge w \downarrow \rho(\tau) \wedge \exists e,\;v=\lambda x.e \wedge \\
  & \hspace{.1in} w=\lambda x.(B,w_2) \wedge \forall v_1\;w_1,\;(v_1,w_1) \in \relV{\tau_1}_\rho \Rightarrow \\
  & \hspace{.1in} (e[v_1/x], w_2[w_1/x])\in \relE{\tau_2}^{\rho(c[\vtos{v_1}/x]),\rho(s[\vtos{v_1}/x])}_{\rho[x\mapsto(v_1,w_1)],B[w_1/x]} \\
  \relV{\forall^c_s\tau_1}_\rho &\defeq \lambda (v,w).\;v \downarrow \rho(\tau) \wedge w \downarrow \rho(\tau) \wedge \exists e,\;v=\lambda\alpha.e \wedge \\
  & \hspace{.1in} w=\lambda.(B,w_2) \wedge \forall \tau'\;r,\;r \in \mathsf{VRel}(\tau') \Rightarrow \\
  & \hspace{.1in} (e[\tau'/\alpha], w_2)\in \relE{\tau_1}^{\rho(c),\rho(s)}_{\rho[\alpha\mapsto(\tau',r)],B} \\
  \relV{\mu\alpha.\tau'}_\rho &\defeq \mu r.\;\lambda (v,w).\; v \downarrow \rho(\tau) \wedge w \downarrow \rho(\tau) \wedge \exists v'\;w',\; \\
  & \hspace{.1in} v = \symfold\; v' \wedge w = \symfold\; w' \wedge \\
  & \hspace{.1in} \later (v',w') \in \relV{\tau'}_{\rho[\alpha\mapsto(\rho(\tau),r)]} \\
  \relE{\tau}^{c,s}_{\rho,B} &\defeq \lambda(e,w).\;\vdash e:\rho(\tau) \wedge \vdash w:\rho(\tau) \wedge \\
  & \hspace{.1in} (\exists m, B \Downarrow \mathsf{const}\;m \wedge \forall n\;e',\; e\leadsto^n_0 e' \Rightarrow n \leq m) \wedge \\
  & \hspace{.1in} (\forall v\;w',\; e\Downarrow v \wedge w\Downarrow w'\Rightarrow (v,w') \in \relV{\tau}_\rho) \wedge \\
  & \hspace{.1in} (\forall e',\; e\leadsto^*_1 e' \Rightarrow c > 0 \wedge \later (e',w)\in \relE{\tau}^{c-1,s}_{\rho,B})
\end{align*}
%% \end{array}
%% $$
}
\caption{\label{lrel}Logical Relations}
\end{figure}

Since widths are not terminating by definition, in the logical relations we have to explicitly require that they terminate.
Thus widths become ``shadow expressions'', accompanying expressions everywhere in our logical relations. 
The design in Figure \ref{lrel} largely follows \cite{dreyer2009logical}, with the addition of widths accompanying and mirroring expressions. As in \cite{dreyer2009logical}, in order to make types strictly shrink for recursive definition, substitution for type variables are postponed and the substitutes are collected in a substitution table $\rho$, defined in Figure \ref{lrel-aux}. For each type variable $\alpha$, we need both a syntactic substitute $\tau$ and a semantic substitute $r$. The former is needed to close an open type; the latter is needed in the $\relV{\alpha}_\rho$ case. Because our types can have size variables, we also need to collect substitutes for size variables. And Because $\rho$ is also used later to close open expressions and open widths, we directly put $(v,w)$ as the substitute for size variable (and term variable) $x$ (size can be calculated from a value).

\begin{figure}
$$\begin{array}{rrcl}
  & \rho &::=& [] \mid \rho[\alpha\mapsto(\tau,r)] \mid \rho[x\mapsto(v,w)]\\
  & r\in\mathsf{VRel}(\tau) &\defeq& \forall v\;w,\;(v,w)\in r \Rightarrow v\downarrow\tau \wedge w\downarrow\tau \\
  & v \downarrow \tau &\defeq& \vdash v:\tau \wedge v\in\mathsf{Val} \\
  & e \Downarrow v &\defeq& e\leadsto^*v \wedge v\in\mathsf{Val} \\
  & e \leadsto^n_m e' &\defeq& n\textrm{\ steps\ wherein\ }m\textrm{\ are\ unfold-folds} \\
  & w \downarrow \tau &\defeq& \textrm{similar to }e\downarrow\tau \\
  & w \Downarrow w' &\defeq& \textrm{similar to }e\Downarrow v \\
\end{array}$$
\caption{\label{lrel-aux}Auxiliary Definitions for Logical Relation}
\end{figure}

The clauses of $\relE{\tau}$ are so designed to enable the proof of Lemma \ref{lem-bind}. The $(\exists m,\cdots)$ clause states the meaning of ``width'' to be ``the maximal number of steps between two consecutive unfold-fold reductions''. $e\leadsto^n_l e'$ means $e$ evaluates to $e'$ with $n$ reduction steps (any kind) within which $l$ of them are unfold-fold reductions. Since $B$ is a symbolic representation of natural numbers which does not necessarily terminate, we must explicitly require that it terminate to a natural number $m$. The next clause is a usual requirement connecting $\relE{\tau}$ and $\relV{\tau}$. The last clause, as in \cite{dreyer2009logical}, is needed because $\relV{\mu\alpha.\tau}$ only gives us $\later (v',w') \in \relV{\tau'}$, and we need to establish $\relE{\tau'}$ from this later/weaker information.

The roadmap of using logical relations is to split the theorem we want to prove, schematically written as $\vdash e:\tau \Rightarrow P_\tau(e)$, into two lemmas: (1) the Fundamental Lemma, $\vdash e:\tau \Rightarrow e\in\relE{\tau}$; and (2) the Adequacy Lemma, $e\in\relE{\tau} \Rightarrow P_\tau(e)$. The Fundamental Lemma is proved by induction on the typing derivation. The logical relations defined in Figure \ref{lrel} are only for closed expressions, but for the proof-by-induction to work, we need to expand it to open terms in a typing context. As an example, suppose a typing context $\Gamma=\alpha_1,x_2:\tau_2,x_3:\tau_3,\alpha_4,\cdots,\alpha_{n-1},x_n:\tau_n$, we define:

$$
\begin{array}{rcl}
\mathcal{X}&\defeq&\alpha_1,x_2,w_2,x_3,w_3,\alpha_4,\cdots,\alpha_{n-1},x_n,w_n \\
\mathcal{R}&\defeq&r_1,r_4,\cdots,r_{n-1} \\
\rho&\defeq&\alpha_1\mapsto(\alpha_1,r_1),x_2\mapsto(x_2,w_2),x_3\mapsto(x_3,w_3),\\
& & \alpha_4\mapsto(\alpha_4,r_4),\cdots,\alpha_{n-1}\mapsto(\alpha_{n-1},r_{n-1}),x_n\mapsto(x_n,w_n) \\
\mathcal{P}&\defeq&r_1\in\mathsf{VRel}(\alpha_1),(x_2,w_2)\in\relV{\tau_2}_\rho,(x_3,w_3)\in\relV{\tau_3}_\rho,\\
& & r_4\in\mathsf{VRel}(\alpha_4),\cdots,r_{n-1}\in\mathsf{VRel}(\alpha_{n-1}),(x_n,w_n)\in\relV{\tau_n}_\rho
\end{array}
$$
and define the ``related'' relation as

$$
w;B;\Gamma\related{e}{\tau}{c}{s}\defeq\mathcal{X};\mathcal{R};\mathcal{P}\vdash(\rho(e),\rho(w))\in\relE{\tau}^{\rho(c),\rho(s)}_{\rho,\rho(B)}
$$

We can now precisely state the Fundamental Lemma and the Adequacy Lemma:

\begin{lem}[\label{lem-fundamental}Fundamental]
$$
\begin{array}{l}
\forall \Gamma\;c\;s\;e\;\tau,\\
\hspace{.4in} \Gamma\typing{e}{\tau}{c}{s} \Rightarrow \exists w\;B,\; w;B;\Gamma\related{e}{\tau}{c}{s}.\\
\end{array}
$$
\end{lem}

\begin{lem}[\label{lem-adequacy}Adequacy]
$$
\begin{array}{l}
\forall w\;B\;c\;s\;e\;\tau,\\
\hspace{.4in} w;B;\cdot\related{e}{\tau}{c}{s} \Rightarrow \\
\hspace{.5in} \exists m,\; B\Downarrow \mathsf{const}\;m \wedge \forall n\;e',\;e\leadsto^ne'\Rightarrow n\leq(m+1)(c+1). \\
\end{array}
$$
\end{lem}

Theorem \ref{thm-boundedness'} is given by these two lemmas, letting $\Gamma=\cdot,c=0,s=0,\tau=\arrow{\tau_1}{c'}{s'}{\tau_2},$ and $e$ to be a value, and unfolding the definition of $\relV{\arrow{\tau_1}{c'}{s'}{\tau_2}}_{[]}$. Theorem \ref{thm-boundedness'} needs to be slightly modified by replacing $\vdash v:\tau_1$ with $(v,w_1)\in\relV{\tau_1}_{[]}$:

\begin{thm}[\label{thm-boundedness''}Soundness w.r.t. Boundedness]
$$
\begin{array}{l}
\forall f\;\tau_1\;\tau_2\;c\;s, \\
\hspace{0.1in} \vdash f:\arrow{(x:\tau_1)}{c}{s}{\tau_2} \wedge f\in\mathsf{Val}\Rightarrow \\
\hspace{0.2in} \exists B, \;\forall v\;w_1,\; (v,w_1)\in\relV{\tau_1}_{[]} \Rightarrow \\
\hspace{0.3in} \exists m,\; B(w_1)_2\Downarrow \mathsf{const}\;m \wedge \\
\hspace{0.4in} \forall e'\;n,\; f\;v\leadsto^n e' \Rightarrow n\leq (m+1)\times (c[\vtos{v}/x]+1)
\end{array}
$$
\end{thm}

Note that when $\tau_1$ does not contain arrows in it, Theorem \ref{thm-boundedness''} degenerates to Theorem \ref{thm-boundedness} (ignoring the $+1$ part), the most familiar form in complexity analysis. A take-away of this finding is that the familiar form of asymptotic complexity definition in theoretical computer science, where a constant factor is existentially quantified in the outermost, only works with first-order functions (whose arguments are not functions), not higher-order functions. Since computer theoriests usually do not consider higher-order functions, this aspect has long been overlooked.

The Fundamental Lemma is proved by induction on the typing derivation. Most the cases can be discharged by appealing to the Bind Lemma:

\begin{lem}[\label{lem-bind}Bind]
\begin{mathpar}
\inferrule*
{\mathcal{C}\vdash(e,w_e)\in\relE{\tau}^{c_1,s_1}_{\rho,B_e} \\ \mathcal{C}\vdash(e,w_e)\in\relEC{\tau,\tau'}^{s_1,c_2,s_2}_{E,\rho,\rho',w_E,B_E}} 
{\mathcal{C}\vdash(E[e],w_E)\in\relE{\tau'}^{c_1+c_2,s_2}_{\rho',B_e+B_E}}
\end{mathpar}
\end{lem}
where
$$
\begin{array}{l}
\relEC{\tau,\tau'}^{s_1,c_2,s_2}_{E,\rho,\rho',w_E,B_E}\defeq \\
\hspace{.2in} \lambda(e,w_e).\;\forall v\;w_e',\;(v,w_e')\in\relV{\tau}_\rho \wedge e\leadsto*v \wedge w_e\leadsto*w_e' \wedge \\
\hspace{.3in} \vtos{v}\leq s_1 \Rightarrow (E[v],w_E)\in\relE{\tau'}^{c_2,s_2}_{\rho',B_E}
\end{array}
$$

The intuition of the usefulness of this lemma is that when considering a term $E[e]$ (terms such as $e_1\;e_2$, $\symfold\; e$, $\sympair\;e_1\;e_2$, etc. can all be decomposed into this form), we only need to consider the case where $e$ is a value. So, for example, proving $e_1\;e_2\in\relE{\tau}$ can be reduced to proving $v_1\;v_2\in\relV{\tau}$ where we can appeal to the definition of $\relV{\tau}$.

(ToDo: list other key lemmas for the proof)

\section{\label{section-coq}Coq Formalization}

The central design decision in the Coq formalization of our language and proofs is how to encode the language and the LSLR logic used in the proof. For the language, we choose deBruijn indices, which is a common and popular encoding scheme for binding structures. In our Coq formalization, we used both dependent and non-dependent deBruijn indices, and found them having complemental strength and weakness and both worthwhile.

Dependent deBruijn-index encoding is to use dependent types to parameterize terms over a variable context, which is a list of in-scope variables (possibly with their types or other information). The dependent deBruijin-index encoding of our expressions is given in Figure \ref{coq-depdeb}.

\begin{figure}
\begin{coq}
Inductive ty := Ttype | Texpr.

Definition var t ctx := {n | nth_error ctx n = Some t}.

Inductive expr (ctx : list ty) :=
| Evar : var Texpr ctx -> expr ctx
| Eapp : expr ctx -> expr ctx -> expr ctx
| Eabs : type ctx -> expr (Texpr :: ctx) -> expr ctx
| Elet : expr ctx -> expr (Texpr :: ctx) -> expr ctx
| Etapp : expr ctx -> type ctx -> expr ctx
| Etabs : expr (Ttype :: ctx) -> expr ctx
| Efold : type ctx -> expr ctx -> expr ctx
| Eunfold : expr ctx -> expr ctx
| Ehide : expr ctx -> expr ctx
| Eunhide : expr ctx -> expr ctx
| Ett : expr ctx
| Epair : expr ctx -> expr ctx -> expr ctx
| Einl : type ctx -> expr ctx -> expr ctx
| Einr : type ctx -> expr ctx -> expr ctx
| Efst : expr ctx -> expr ctx
| Esnd : expr ctx -> expr ctx
| Ematch : expr ctx -> expr (Texpr :: ctx) -> expr (Texpr :: ctx) -> expr ctx.
\end{coq}
\caption{\label{coq-depdeb}Dependent deBruijn-index Encoding of Expressions}
\end{figure}

Non-dependent deBruijn-index encoding is the above encoding without the context part, so variables are represented by natural numbers with the traditional deBruijn-index scheme, and there is no guarentee that the natural number makes sense in the current variable scope. Dependent deBruijn-index encoding builds in scoping constraints, thus has more information and is more convenient to use (eliminate); on the other hand, terms in non-dependent deBruijn-index encoding is easier to create (introduce) because one can freely pick natural numbers regardless of scoping constraints. So we choose non-dependent deBruijn-index encoding for writing example programs and type-checking them, while dependent deBruijn-index encoding is used in our type soundness proof. The two are related by the fact that any well-typed non-dependent deBruijn-index encoded expression is well-scoped hence has a corresponding dependent deBruijn-index encoding.

Dependent deBruijn-index encoding could have been used also to encode the LSLR logic, but since most parts of LSLR formulas are just lifted Coq propositions, we want to use Coq's native Gallina terms for these lifted parts. When such a part refers a variable, such as $\forall x,\lceil P(x)\rceil$, $x$ also needs to be a Gallina variable, therefore we need Higher-Order Abstract Syntax (HOAS) encoding. As the solution, we use HOAS encoding for first-order variables in LSLR (because they need to appear in lifted Gallina terms), and dependent deBruijn indices for second-order variables (since HOAS cannot handle impredicative variables). The Coq definition of LSLR formulas is given in Figure \ref{coq-lslr}.

\begin{figure}
\begin{coq}
Inductive usability := Usable | Unusable.

Definition ty := (nat * usability).

Inductive rel : nat (*arity*)-> list ty -> Type :=
| Rvar {m ctx} : var (m, Usable) ctx -> rel m ctx
| Rinj {ctx} : Prop -> rel 0 ctx
| Rand {ctx} (_ _ : rel 0 ctx) : rel 0 ctx
| Ror {ctx} (_ _ : rel 0 ctx) : rel 0 ctx
| Rimply {ctx} (_ _ : rel 0 ctx) : rel 0 ctx
| Rforall1 {ctx T} : (T -> rel 0 ctx) -> rel 0 ctx
| Rexists1 {ctx T} : (T -> rel 0 ctx) -> rel 0 ctx
| Rforall2 {ctx m} : rel 0 (m :: ctx) -> rel 0 ctx
| Rexists2 {ctx m} : rel 0 (m :: ctx) -> rel 0 ctx
| Rabs {ctx m} : (D -> rel m ctx) -> rel (S m) ctx
| Rapp {ctx m} : rel (S m) ctx -> D -> rel m ctx
| Rrecur {ctx m} : rel m ((m, Unusable) :: ctx) -> rel m ctx
| Rlater {ctx} chg : rel 0 (change_usab chg ctx) -> rel 0 ctx
.
\end{coq}
\caption{\label{coq-lslr}Dependent deBruijn-index + HOAS Encoding of LSLR}
\end{figure}

The variable context in dependent deBruijn-index encoding is also used to encode the syntactic ``guardedness'' constraint in LSLR that variables introduced by $\mu$ only become ``usable'' under $\later$.

We also tried the Parametric Higher-Order Abstract Syntax (PHOAS) encoding but found it hard to define the Kriple-model interpretation of LSLR formulas.

\section{\label{section-related}Related Work}

When comparing our work to related work, we investigate the related work on these aspects: (1) program expressiveness: e.g. do they support only first-order functions or higher-order functions? (2) datatype expressiveness: e.g. do they have only list or general algebraic datatypes? (3) complexity expressiveness: e.g. do they support only polynomial complexity or more complexity expressions? Do they only support absolute bound or the complexity can refer to input sizes? (5) cost model: what do they count as time cost? (6) automation: can they calculate the complexity fully automatically, or the programmers have to figure out the complexity by themselves? 

\subsection{LXres}

The type system LXres described in \cite{crary2000} is the most similar to our design. They both choose to let programmers specify and prove time complexity, not to infer it. Programmers must first figure out the complexity result by themselves. Unlike $\logo$ which extends from a ML-like lambda calculus with a trivial kind system, LXres employs a complex kind system (which has sum and inductive kinds) to simulate dependent-types. $\logo$-functions look like ML-functions except for the complexity annotations on arrows; while LXres-functions are more like functions in a dependent-typed language. In dependent-type terminology, a function in LXres takes an input argument, an input size, and a ``proof'' that the argument has that size. Both the structures of the size and the ``proof'' can be defined by the user, unlike in $\logo$, where the structure of size is predefined by the language (in Figure \ref{syntax}), and the structure of the ``proof'' is fixed in typing rules. By internalizing sizes into the language and letting the programmers choose their own notion of ``size'', LXres offers more flexibility than $\logo$. But this design entails two issues. (1) Over-representing: because of syntactic constraints on ``enforcement types'' in LXres, sizes in LXres always specify the full shape of input values, so they do not have $\logo$-size's ability to abstract size spec (e.g. only specify work or span). In other words, sizes in LXres mirror too closely to values, while we made some delicate design trade-offs to make size specs look like values but not too much.  (2) Because different functions can choose different definitions of sizes and ``proofs'', they may conflict. In $\logo$ sizes are defined by the language and are the same across functions. LXres also incurs nonpractical burdens on programmers, since the programmer must specify the finishing time for each function call. As their paper indicated, LXres is more suited as a language manipulated by machines than a language to write programs in by hand. Their cost model only counts function calls, which leaves open the question whether the running time is bounded by the number of function calls. The bulk of our proof is for showing that the number of reductions is indeed bounded by our unfold-fold count.

\subsection{Automatic Program Analysis}

A series of work by Steffen Jost, Jan Hoffmann and et al aim at calculating time (and other resource) complexity fully automatically for first-order functions. They all use linear programming to solve complexity constraints derived by analysing programs. They all only support lists. The system in \cite{jost2010} supports linear complexity, which can depend on input size. The system in \cite{hoffmann2010} supports polynomial complexity and can conduct amortized complexity analysis using potentials. The RAML language in \cite{hoffmann2012} extends that in \cite{hoffmann2010} with multivariate complexity support. The system in \cite{hoffmann2015} adds support for parallel programs.

The type and effect system in \cite{portillo} is another system that can conduct fully automated time complexity inference. The language is a simple lambda calculus which has higher-order functions but no recursions. The only supported datatype is list.

\subsection{Size Types and Refinement Types}

The type system in \cite{cicek} is designed to specify and prove the complexity for incremental computation where changes of input are propogated to calculate the new output, instead of computing the output from scratch. Refinement types are used to represent an input argument of a given size. It only supports lists, and ``sizes'' are just numbers. The language has a built-in fixpoint and only recursive calls are counted as time cost. The authors also use step-indexed logical relation (but not LSLR) to prove type soundness.

The authors in \cite{vazou2015} add bounded quantification to refinement types, and get a type system whose type-checking and type inference can be done by a SMT solver. The type and effect system in \cite{vasconcelos} uses size types and can automatically infer cost equations. Some of these equations are shown to have a complete solving algorithm. Only polynomial complexity can be given by the solver. The language has higher-order functions and only lists as datatypes. Only beta-reductions are counted as cost. The system in \cite{simoes} has similar features to \cite{vasconcelos} but uses intersection types. The work in \cite{lago} reduces complexity analysis of higher-order functions into simpler verification problems by using linear dependent types. But only natural numbers are supported as datatypes.

\subsection{Program Transformation}

Instead of having a type system, the authors in \cite{danner2015} proposes a procedure to transform a program (without complexity-annotated types) into a certain form, and read off the complexity recurrence equations from there. The proposed pipeline is to (1) transform the source program into a program in a complexity language; (2) apply programmer-specified size functions to map each datatype value to a ``size'' (natural number or some other preorder), and at this point the program becomes complexity recurrence; (3) solve the recurrence (not addressed by the paper). The ``complexity language'' actually corresponds to typing judgements in $\logo$, so the first step is essentially a type-checking against a complexity type system. They let the programmers choose their own notion of ``sizes'' and size functions, but the paper only uses natural numbers as sizes (their soundness proof allows any preorder satisfying certain axioms to be used as sizes, similar to our design). The cost model is defined in a big-step operational semantics and only counts function applications and recursions. They also uses logical relation to prove soundness. Work in \cite{danner2013} is also based on program transformations but only supports lists and list recursions. Authors in \cite{avanzini2015} demonstrated several transformation steps which convert higher-order functions to first-order functions, as a way to automatically analyse the complexity of higher-order functions. These transformations can serve as inspirations for our future work on a complexity-preserving compiler.

\subsection{Other}

The author of \cite{danielsson} implemented a monad in Haskell which stands for a computation with a given bound of computation steps. Since it is hosted in Haskell, it is in a lazy evaluation setting, hence has the drawback of lazy evaluation for doing complexity analysis as discussed in Section \ref{intro}. They time bounds are absolute bounds (natural numbers).

Program logic is an alternative to type system as a vehicle to conduct complexity analysis, especially for imperative and low-level languages. The program logic in \cite{aspinall} has a cost model in it and is designed for proving time complexity of (a fragment of) JVM codes. For our future work to design a complexity-preserving compiler from $\logo$ to assembly language, we conjecture that the target language needs to be equiped with an alike program logic.

The red-black tree implementation \cite{appel2011} in Coq and the various highly efficient functional data structures described in \cite{okasaki} stand as witness that functional languages are suitable for implementing efficient data structures and these data structures deserve a rigid complexity analysis.

\section{\label{section-discussion}Discussion and Conclusion}

\subsection{Lessons Learned}

\appendix

\section{\label{append1}Evaluation, Kinding and Type Equivalence Rules}

\begin{figure}
$$\begin{array}{rrcl}
  \textrm{Values} & v &::=& x \mid \lambda x:\tau.e \mid \lambda \alpha.e \mid \symfold_\tau\;v \mid \symtt \mid (v,v) \\
  & & & \mid \syminl_\tau\;v \mid \syminr_\tau\;v \mid \symhide\; v \\
  \textrm{CBV Eval Ctx} & E &::=& \Box \mid E\;e \mid v\;E \mid \symlet x := E\symin e \mid E\;\tau \\
  & & & \mid \symfold_\tau\;E \mid \symunfold\;E \mid \symhide\;\;E \mid \symunhide\;\;E \\
  & & & \mid (E, e) \mid (v, E) \mid \syminl_\tau\;E \mid \syminr_\tau\;E \mid \symfst\;E \mid \symsnd\;E \\
  & & & \mid (\symmatch\;E\symreturn \tau \symand s \symwith \\
  & & & \hspace{.1in} \syminl\;x\Rightarrow e\;|\;\syminr\;x\Rightarrow e)
\end{array}$$
\caption{\label{eval-aux}Call-by-value values and evaluation contexts}
\end{figure}

\begin{figure}
\fbox{$e\leadsto e'$}
\begin{mathpar}

\inferrule*
{e_1\leadsto e_2}
{E[e_1]\leadsto E[e_2]}

\inferrule*
{\;}
{(\lambda x:\tau.e)\;v \leadsto e[v/x]}

\inferrule*
{\;}
{\symlet x:\tau:=v \symin e \leadsto e[v/x]}

\inferrule*
{\;}
{\symfst\;(v_1,v_2)\leadsto v_1}

\inferrule*
{\;}
{\symsnd\;(v_1,v_2)\leadsto v_2}

\inferrule*
{\;}
{\symmatch\;\syminl\;v\symwith\syminl\;x\Rightarrow e_1\;|\;\syminr\;x\Rightarrow e_2 \leadsto e_1[v/x]}

\inferrule*
{\;}
{\symmatch\;\syminr\;v\symwith\syminl\;x\Rightarrow e_1\;|\;\syminr\;x\Rightarrow e_2 \leadsto e_2[v/x]}

\inferrule*
{\;}
{(\lambda \alpha.e)\;\tau \leadsto e[\tau/\alpha]}

\inferrule*
{\;}
{\symunfold\;\;(\symfold_\tau\;v) \leadsto v}

\inferrule*
{\;}
{\symunhide\;(\symhide\; v) \leadsto v}

\end{mathpar}
\caption{\label{eval}Call-by-value evaluation rules}
\end{figure}

\begin{figure}
\fbox{$\Gamma\vdash \pi:k$}
\quad$k ::= * \mid *\to k$
\begin{mathpar}

\inferrule*
{\alpha\in\Gamma}
{\Gamma\kinding{\alpha}{*}}

\inferrule*
{\Gamma\kinding{\pi_1}{*\to k} \\ \Gamma\kinding{\pi_2}{*}}
{\Gamma\kinding{\pi_1\;\pi_2}{k}}

\inferrule*
{\Gamma,\alpha\kinding{\pi}{k}}
{\Gamma\kinding{\Lambda \alpha.\pi}{*\to k}}

\inferrule*
{\Gamma\kinding{\pi_1}{*} \\ \Gamma,x:(\pi_1,\bot)\kinding{\pi_2}{*}}
{\Gamma\kinding{\arrow{(x:\pi_1)}{c}{s}{\pi_2}}{*}}

\inferrule*
{\Gamma,\alpha\kinding{\pi}{*}}
{\Gamma\kinding{\forall \alpha.\pi}{*}}

\inferrule*
{\Gamma,\alpha\kinding{\pi}{*}}
{\Gamma\kinding{\mu \alpha.\pi}{*}}

\inferrule*
{\Gamma\kinding{\pi}{*}}
{\Gamma\kinding{\thide{\pi}}{*}}

\inferrule*
{\;}
{\Gamma\kinding{\symunit}{*}}

\inferrule*
{\Gamma\kinding{\pi_1}{*} \\ \Gamma\kinding{\pi_2}{*}}
{\Gamma\kinding{\pi_1\times\pi_2}{*}}

\inferrule*
{\Gamma\kinding{\pi_1}{*} \\ \Gamma\kinding{\pi_2}{*}}
{\Gamma\kinding{\pi_1+\pi_2}{*}}

\end{mathpar}
\caption{\label{kinding}Kinding Rules}
\end{figure}

%% \acks

%% Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\bibliography{bib.bib}

%% \begin{thebibliography}{}
%% \softraggedright

%% \bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%% P. Q. Smith, and X. Y. Jones. ...reference text...

%% \end{thebibliography}


\end{document}

%                       Revision History
%                       -------- -------
%  Date         Person  Ver.    Change
%  ----         ------  ----    ------

%  2013.06.29   TU      0.1--4  comments on permission/copyright notices

